AWSTemplateFormatVersion: '2010-09-09'
Description: Lifecycle Script Stack

Parameters: 

  ResourceNamePrefix:
    Description: Prefix to be used for all resources created by this template.
    Type: String
    Default: sagemaker-hyperpod-eks

  S3BucketName: 
    Description: The name of the S3 bucket used to store the cluster lifecycle scripts.
    Type: String
    Default: sagemaker-hyperpod-eks-bucket

Resources: 

  S3CustomResourceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 
                 - s3:PutObject
                 - s3:DeleteObject
                 - s3:ListBucket
                Resource:
                  - !Sub 'arn:aws:s3:::${S3BucketName}'
                  - !Sub 'arn:aws:s3:::${S3BucketName}/*'

  S3CustomResourceFunction: 
    Type: AWS::Lambda::Function
    Properties: 
      FunctionName: !Sub '${ResourceNamePrefix}-lifecycle-script-loader'
      Environment: 
        Variables:
          BUCKET_NAME: !Ref S3BucketName
          GITHUB_FOLDER_URL: 'https://github.com/awslabs/awsome-distributed-training/tree/main/1.architectures/7.sagemaker-hyperpod-eks/LifecycleScripts/base-config'
      Handler: index.lambda_handler
      Role: !GetAtt S3CustomResourceRole.Arn
      Runtime: python3.12
      Timeout: 600
      Code: 
        ZipFile: |
          import boto3
          import cfnresponse
          import os
          import json
          import urllib.request
          import urllib.error

          def determine_content_type(file_name):
            """Determine content type based on file extension."""
            if file_name.endswith('.sh'):
              return 'text/x-sh'
            elif file_name.endswith('.py'):
              return 'text/x-python'
            elif file_name.endswith('.json'):
              return 'application/json'
            elif file_name.endswith('.yaml') or file_name.endswith('.yml'):
              return 'application/x-yaml'
            else:
              return 'text/plain'

          def upload_file_to_s3(s3_client, bucket, file_content, s3_key, content_type):
            """Upload a file to S3."""
            s3_client.put_object(
              Bucket=bucket,
              Key=s3_key,
              Body=file_content,
              ContentType=content_type
            )

          def download_github_file(download_url):
            """Download file content from GitHub."""
            file_req = urllib.request.Request(download_url)
            file_req.add_header('User-Agent', 'AWS-Lambda-LifecycleScriptLoader')
            
            with urllib.request.urlopen(file_req) as file_response:
              return file_response.read()

          def process_directory(s3_client, bucket, repo_url, branch, path, prefix=''):
            """Process a directory recursively and upload all files to S3."""
            api_url = f"https://api.github.com/repos/{repo_url.split('github.com/')[1]}/contents/{path}?ref={branch}"
            
            req = urllib.request.Request(api_url)
            req.add_header('Accept', 'application/vnd.github.v3+json')
            req.add_header('User-Agent', 'AWS-Lambda-LifecycleScriptLoader')
            
            with urllib.request.urlopen(req) as response:
              contents = json.loads(response.read().decode('utf-8'))
            
            for item in contents:
              if item['type'] == 'file':
                file_name = item['name']
                download_url = item['download_url']
                s3_key = f"{prefix}/{file_name}" if prefix else file_name
                
                file_content = download_github_file(download_url)
                content_type = determine_content_type(file_name)
                upload_file_to_s3(s3_client, bucket, file_content, s3_key, content_type)
                
              elif item['type'] == 'dir':
                dir_name = item['name']
                new_path = f"{path}/{dir_name}"
                new_prefix = f"{prefix}/{dir_name}" if prefix else dir_name
                process_directory(s3_client, bucket, repo_url, branch, new_path, new_prefix)

          def delete_s3_objects_recursively(s3_client, bucket):
            """Delete all objects in the bucket."""
            paginator = s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=bucket)
            
            objects_to_delete = []
            for page in pages:
              if 'Contents' in page:
                for obj in page['Contents']:
                  objects_to_delete.append({'Key': obj['Key']})
            
            if objects_to_delete:
              for i in range(0, len(objects_to_delete), 1000):
                batch = objects_to_delete[i:i+1000]
                s3_client.delete_objects(
                  Bucket=bucket,
                  Delete={'Objects': batch}
                )

          def parse_github_url(url):
            """Parse GitHub URL to extract repo and path info."""
            if '/tree/' in url:
              parts = url.split('/tree/')
              repo_url = parts[0]
              branch_and_path = parts[1].split('/', 1)
              branch = branch_and_path[0]
              path = branch_and_path[1] if len(branch_and_path) > 1 else ''
              return repo_url, branch, path
            return None, None, None

          def lambda_handler(event, context):
            try:
              if event['RequestType'] in ['Create', 'Update']:
                s3 = boto3.client('s3')
                bucket = os.environ['BUCKET_NAME']
                github_folder_url = os.environ['GITHUB_FOLDER_URL']
                
                try:
                  s3.head_bucket(Bucket=bucket)
                  
                  repo_url, branch, path = parse_github_url(github_folder_url)
                  if not repo_url:
                    cfnresponse.send(event, context, cfnresponse.FAILED, {
                      'Message': 'Invalid GitHub URL format'
                    })
                    return
                  
                  process_directory(s3, bucket, repo_url, branch, path)
                  
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                    'Message': 'Files uploaded successfully'
                  })
                  
                except s3.exceptions.NoSuchBucket:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {
                    'Message': 'Bucket does not exist'
                  })
                except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {
                    'Message': str(e)
                  })
                
              elif event['RequestType'] == 'Delete':
                s3 = boto3.client('s3')
                bucket = os.environ['BUCKET_NAME']
                
                try:
                  delete_s3_objects_recursively(s3, bucket)
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                    'Message': 'Files deleted successfully'
                  })
                except:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                    'Message': 'Cleanup completed'
                  })
                  
            except Exception as e:
              print(e)
              cfnresponse.send(event, context, cfnresponse.FAILED, {
                'Error': str(e)
              })

  S3CustomResource: 
    Type: AWS::CloudFormation::CustomResource
    Properties: 
      ServiceToken: !GetAtt S3CustomResourceFunction.Arn