# SPDX-FileCopyrightText: Copyright (C) SchedMD LLC.
# SPDX-License-Identifier: Apache-2.0
# Inter-pod anti-affinity and node affinity for non-compute components
commonAffinity: &commonAffinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "node.kubernetes.io/instance-type"
            operator: In
            values:
              - "ml.m5.2xlarge"
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: "app.kubernetes.io/name"
            operator: In
            values: ["slurmdbd", "slurmctld", "slurm-exporter", "login", "mariadb", "slurmrestd"]
        topologyKey: "kubernetes.io/hostname"

#
# Debug configuration.
# @ignored
debug:
  #
  # -- (bool)
  # Enables debug configuration.
  enabled: false
  #
  # -- (bool)
  # Allow a locally running operator to communicate with slurm cluster via port-forward.
  # NOTE: use when running the operator in a local debugger.
  localOperator: true

#
# -- (string)
# Overrides the name of the release.
nameOverride: ""

#
# -- (string)
# Overrides the full name of the release.
fullnameOverride: ""

#
# -- (string)
# Overrides the namespace of the release.
namespaceOverride: ""

#
# -- (list)
# Set the secrets for image pull.
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
  # - name: regcred

#
# -- (string)
# Set the image pull policy.
imagePullPolicy: IfNotPresent

#
# -- (string)
# Set the priority class to use.
# Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
priorityClassName: ""

#
# Slurm JWT authentication.
jwt:
  #
  # JWT hs256 configurations.
  hs256:
    #
    # -- (string)
    # The existing secret containing the jwt_hs256.key, otherwise one will be generated.
    secretName: ""

#
# Slurm configurations.
slurm:
  #
  # Slurm authentication configurations.
  auth:
    #
    # -- (string)
    # The existing secret containing the slurm.key, otherwise one will be generated.
    secretName: ""
  #
  # -- (map[string]string | map[string][]string)
  # Extra slurmdbd configuration lines to append to `slurmdbd.conf`.
  # WARNING: Values can override existing ones.
  # Ref: https://slurm.schedmd.com/slurmdbd.conf.html
  extraSlurmdbdConf: {}
    # CommitDelay: 1
    ### LOGGING ###
    # DebugLevel: debug2
    # DebugFlags: []
    ### PURGE ###
    # PurgeEventAfter: 12month
    # PurgeJobAfter: 12month
    # PurgeResvAfter: 2month
    # PurgeStepAfter: 2month
    # PurgeSuspendAfter: 1month
    # PurgeTXNAfter: 12month
    # PurgeUsageAfter: 12month
  #
  # -- (map[string]string | map[string][]string)
  # Extra slurm configuration lines to append to `slurm.conf`, represetned as a string or a map.
  # WARNING: Values can override existing ones.
  # Ref: https://slurm.schedmd.com/slurm.conf.html
  extraSlurmConf:
    SlurmctldHost: slurm-controller
    # MinJobAge: 2
    # MaxNodeCount: 1024
    ### LOGGING ###
    # SlurmctldDebug: debug2
    # SlurmSchedLogLevel: 1
    # SlurmdDebug: debug2
    # DebugFlags: []
    ### PLUGINS & PARAMETERS ###
    # SchedulerParameters:
    #   - defer_batch
  #
  # -- (map[string]string)
  # Optional raw Slurm configuration files, as a map.
  # The map key represents the config file by name; the map value represents config file contents as a string.
  # Ref: https://slurm.schedmd.com/man_index.html#configuration_files
  configFiles:
    # acct_gather.conf: |
    #   # Ref: https://slurm.schedmd.com/acct_gather.conf.html
    # burst_buffer.conf: |
    #   # Ref: https://slurm.schedmd.com/burst_buffer.conf.html
    # gres.conf: |
    #   # Ref: https://slurm.schedmd.com/gres.conf.html
    # helpers.conf: |
    #   # Ref: https://slurm.schedmd.com/helpers.conf.html
    # job_container.conf: |
    #   # Ref: https://slurm.schedmd.com/job_container.conf.html
    # mpi.conf: |
    #   # Ref: https://slurm.schedmd.com/mpi.conf.html
    # oci.conf: |
    #   # Ref: https://slurm.schedmd.com/oci.conf.html
    # plugstack.conf: |
    #   # Ref: https://slurm.schedmd.com/plugstack.conf.html
    # topology.conf: |
    #   # Ref: https://slurm.schedmd.com/topology.conf.html
  #
  # -- (map[string]string)
  # The Prolog scripts for compute nodesets, as a map.
  # The map key represents the filename; the map value represents the script contents.
  # WARNING: The script must include a shebang (!) so it can be executed correctly by Slurm.
  # Ref: https://slurm.schedmd.com/slurm.conf.html#OPT_Prolog
  # Ref: https://slurm.schedmd.com/prolog_epilog.html
  # Ref: https://en.wikipedia.org/wiki/Shebang_(Unix)
  prologScripts: {}
    # 00-empty.sh: |
    #   #!/usr/bin/env bash
    #   set -euo pipefail
    #   exit 0
  #
  # -- (map[string]string)
  # The Epilog scripts for compute nodesets, as a map.
  # The map key represents the filename; the map value represents the script contents.
  # WARNING: The script must include a shebang (!) so it can be executed correctly by Slurm.
  # Ref: https://slurm.schedmd.com/slurm.conf.html#OPT_Epilog
  # Ref: https://slurm.schedmd.com/prolog_epilog.html
  # Ref: https://en.wikipedia.org/wiki/Shebang_(Unix)
  epilogScripts: {}
    # 00-empty.sh: |
    #   #!/usr/bin/env bash
    #   set -euo pipefail
    #   exit 0

# Slurm authcred (sackd) configurations.
authcred:
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Set the image to use.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/sackd
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # -- (object)
  # Set container resource requests and limits for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources: {}
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 4Gi

#
# Slurm controller (slurmctld) configurations.
controller:
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Set the image to use.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/slurmctld
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # -- (object)
  # The controller service configuration.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/service/
  service: {}
    # type: LoadBalancer
    # externalIPs: []
    # externalName: my.slurmctld.example.com
  #
  # -- (integer)
  # The external service port number.
  servicePort: 6817
  #
  # -- (integer)
  # The external service node port number.
  # Ignored unless `service.type == NodePort`.
  serviceNodePort: 36817
  #
  # -- (string)
  # Set the priority class to use.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  #
  # -- (map)
  # Selector which must match a node's labels for the pod to be scheduled on that node.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  #
  # -- (object)
  # Set affinity for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: *commonAffinity
  #
  # -- (list)
  # Configure pod tolerations.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  #
  # -- (object)
  # Set container resource requests and limits for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources: {}
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 4Gi
  #
  # Define a persistent volume for the slurm controller to store its save-state.
  # Used to recover from system failures or from pod upgrades.
  persistence:
    #
    # -- (bool)
    # Enables save-state persistence.
    enabled: false
    #
    # -- (string)
    # Name of an existing `PersistentVolumeClaim` to use instead of creating one from definition.
    # NOTE: When not empty, the other persistence fields will be ignored.
    existingClaim: ""
    #
    # -- (object)
    # Create a `PersistentVolumeClaim` with these annotations.
    annotations: {}
    #
    # -- (object)
    # Create a `PersistentVolumeClaim` with these labels.
    labels: {}
    #
    # -- (string)
    # Create a `PersistentVolumeClaim` with this storage class.
    storageClass: standard
    #
    # -- (list)
    # Create a `PersistentVolumeClaim` with these access modes.
    accessModes:
      - ReadWriteOnce
    #
    # -- (string)
    # Create a `PersistentVolumeClaim` with this storage size.
    size: 4Gi
    #
    # -- (object)
    # Selector to match an existing `PersistentVolume`.
    selector: {}
      # matchLabels:
      #   app: foo

#
# Login node configurations.
login:
  #
  # -- (bool)
  # Enables login nodes.
  enabled: true
  #
  # -- (integer)
  # Set the number of replicas to deploy.
  replicas: 1
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Set the image to use.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/login
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # -- (object)
  # The login service configuration.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    type: LoadBalancer
    # externalIPs: []
    # externalName: my.login.example.com
  #
  # -- (integer)
  # The external service port number.
  servicePort: 22
  #
  # -- (integer)
  # The external service node port number.
  # Ignored unless `service.type == NodePort`.
  serviceNodePort: 32222
  #
  # -- (list)
  # The `/root/.ssh/authorized_keys` file to write, represented as a list.
  rootSshAuthorizedKeys:
    - "<your-public-ssh-key-here>"
  #
  # -- (map)
  # The `/etc/ssh/sshd_config` file to use, represented as a map.
  # Ref: https://man.openbsd.org/sshd_config
  sshdConfig:
    # LogLevel: DEBUG3
    # Include: "/etc/ssh/sshd_config.d/*.conf"
    # X11Forwarding: "yes"
    # UsePAM: "yes"
    # Subsystem: sftp /usr/libexec/openssh/sftp-server
    AcceptEnv: "LANG LC_*"
    AuthorizedKeysFile: "/root/.ssh/authorized_keys"
    ChallengeResponseAuthentication: "no"
    ClientAliveCountMax: "3"
    ClientAliveInterval: "60"
    LogLevel: "INFO"
    PasswordAuthentication: "no"
    PermitRootLogin: "yes"
    Port: "22"
    PrintMotd: "no"
    Protocol: "2"
    PubkeyAuthentication: "yes"
    Subsystem: "sftp internal-sftp"
    TCPKeepAlive: "yes"
    UseDNS: "no"
    UsePAM: "no"
    X11Forwarding: "no"
  #
  # The `/etc/sssd/sssd.conf` represented by as a map.
  sssdConf:
    #
    # -- (map)
    # The `/etc/sssd/sssd.conf` [sssd] section, represented as a map.
    # Ref: https://man.archlinux.org/man/sssd.conf.5#The_%5Bsssd%5D_section
    sssd:
      # debug_level: 9
      config_file_version: 2
      services: nss, pam
      domains: DEFAULT
    #
    # -- (map[map])
    # The `/etc/sssd/sssd.conf` [domain/$DOMAIN] sections, represented as a map of map.
    # Ref: https://man.archlinux.org/man/sssd.conf.5#DOMAIN_SECTIONS
    domains:
      DEFAULT:
        # debug_level: 9
        auth_provider: ldap
        id_provider: ldap
        ldap_uri: ldap://ldap.example.com
        ldap_search_base: dc=example,dc=com
        ldap_user_search_base: ou=Users,dc=example,dc=com
        ldap_group_search_base: ou=Groups,dc=example,dc=com
    #
    # -- (map)
    # The `/etc/sssd/sssd.conf` [nss] section, represented as a map.
    # Ref: https://man.archlinux.org/man/sssd.conf.5#NSS_configuration_options
    nss:
      # debug_level: 9
      filter_groups: root,slurm
      filter_users: root,slurm
    #
    # -- (map)
    # The `/etc/sssd/sssd.conf` [pam] section, represented as a map.
    # Ref: https://man.archlinux.org/man/sssd.conf.5#PAM_configuration_options
    pam: {}
      # debug_level: 9
  #
  # --(list)
  # List of volume mounts.
  # Ref: https://kubernetes.io/docs/concepts/storage/volumes/
  extraVolumeMounts:
    - name: fsx-lustre
      mountPath: /fsx
    - name: fsx-openzfs
      mountPath: /home
    # - name: nfs-home
    #   mountPath: /home
    # - name: nfs-data
    #   mountPath: /mnt/data
  #
  # --(list)
  # Define list of pod volumes.
  # Ref: https://kubernetes.io/docs/concepts/storage/volumes/
  extraVolumes: 
     - name: fsx-lustre
       persistentVolumeClaim: 
        claimName: fsx-claim
     - name: fsx-openzfs
       persistentVolumeClaim: 
        claimName: openzfs-claim
    # - name: nfs-home
    #   persistentVolumeClaim:
    #     claimName: nfs-home
    # - name: nfs-data
    #   persistentVolumeClaim:
    # - name: nfs-home
    #   nfs:
    #     server: nfs-server.example.com
    #     path: /exports/home/
    # - name: nfs-data
    #   persistentVolumeClaim:
    #     claimName: nfs-data
  #
  # -- (string)
  # Set the priority class to use.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  #
  # -- (map)
  # Selector which must match a node's labels for the pod to be scheduled on that node.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  #
  # -- (object)
  # Set affinity for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: *commonAffinity
  #
  # -- (list)
  # Configure pod tolerations.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  #
  # -- (object)
  # Set container resource requests and limits for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources: {}
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 4Gi

#
# Slurm compute (slurmd) configurations.
compute:
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Default image for the nodeset pod (slurmd)
  # Each nodeset may override this setting.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/slurmd
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # -- (list)
  # Slurm NodeSets by object list.
  nodesets:
      #
      # -- (string)
      # Name of NodeSet. Must be unique.
    - name: hp-node
      #
      # -- (bool)
      # Enables the NodeSet in Slurm.
      enabled: true
      #
      # -- (integer)
      # Set the number of replicas to deploy.
      replicas: 4
      #
      # -- (string)
      # Set the image pull policy.
      imagePullPolicy: Always
      #
      # Set the image to use.
      image:
        #
        # -- (string)
        # Set the image repository to use.
        repository: "<your-account-id-here>.dkr.ecr.<your-region-here>.amazonaws.com/dlc-slurmd"
        #
        # -- (string)
        # Set the image tag to use.
        tag: "24.11.4-ubuntu24.04"
      #
      # -- (string)
      # Set the priority class to use.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
      priorityClassName: ""
      #
      # -- (object)
      # Set container resource requests and limits for Kubernetes Pod scheduling.
      # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
      resources:
        limits: 
            nvidia.com/gpu: "1"
        requests:
            nvidia.com/gpu: "1"
      #
      # -- (map)
      # Selector which must match a node's labels for the pod to be scheduled on that node.
      nodeSelector:
        kubernetes.io/os: linux
        node.kubernetes.io/instance-type: ml.g5.8xlarge
      #
      # -- (object)
      # Set affinity for Kubernetes Pod scheduling.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
      affinity: {}
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     nodeSelectorTerms:
        #       - matchExpressions:
        #           - key: "kubernetes.io/os"
        #             operator: In
        #             values:
        #               - linux
        # podAntiAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     - topologyKey: "kubernetes.io/hostname"
        #       labelSelector:
        #         matchExpressions:
        #           - key: "app.kubernetes.io/name"
        #             operator: In
        #             values:
        #               - slurmctld
        #               - slurmdbd
        #               - slurmrestd
      #
      # -- (list)
      # Configure pod tolerations.
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      tolerations: []
      #
      # -- (object)
      # Set the update strategy configuration.
      updateStrategy:
        #
        # -- (string)
        # Set the update strategy type.
        # Can be either: "RollingUpdate"; "OnDelete".
        type: RollingUpdate
        #
        # -- (object)
        # Define the rolling update policy.
        # Only used when "updateStrategy.type=RollingUpdate".
        rollingUpdate:
          #
          # -- (string)
          # The maximum number of pods that can be unavailable during the update.
          # Value can be an absolute number (ex: 5) or a percentage of desired
          # pods (ex: 10%). Absolute number is calculated from percentage by
          # rounding up. This can not be 0. Defaults to 1.
          maxUnavailable: 20%
      #
      # -- (object)
      # The policy used for PVCs created from the NodeSet VolumeClaimTemplates.
      persistentVolumeClaimRetentionPolicy:
        #
        # -- (string)
        # WhenDeleted specifies what happens to PVCs created from NodeSet
        # VolumeClaimTemplates when the NodeSet is deleted. The default policy
        # of `Retain` causes PVCs to not be affected by NodeSet deletion. The
        # `Delete` policy causes those PVCs to be deleted.
        whenDeleted: Retain
        #
        # -- (string)
        # WhenScaled specifies what happens to PVCs created from NodeSet
        # VolumeClaimTemplates when the NodeSet is scaled down. The default
        # policy of `Retain` causes PVCs to not be affected by a scale-in. The
        # `Delete` policy causes the associated PVCs for any excess pods to be
        # deleted.
        whenScaled: Retain
      #
      # -- (list)
      # List of PVCs to be created from template and mounted on each NodeSet pod.
      # PVCs are given a unique identity relative to each NodeSet pod.
      # Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#volume-claim-templates
      volumeClaimTemplates: []
        # - metadata:
        #     name: scratch
        #   spec:
        #     mountPath: /mnt/scratch
        #     storageClassName: standard
        #     accessModes:
        #       - ReadWriteOnce
        #     resources:
        #       requests:
        #         storage: 1Gi
      #
      # --(list)
      # List of volume mounts.
      # Ref: https://kubernetes.io/docs/concepts/storage/volumes/
      extraVolumeMounts:
        - name: fsx-lustre
          mountPath: /fsx
        - name: fsx-openzfs
          mountPath: /home
        - name: shmem
          mountPath: /dev/shm
        # - name: nfs-home
        #   mountPath: /home
        # - name: nfs-data
        #   mountPath: /mnt/data
      #
      # --(list)
      # Define list of pod volumes.
      # Ref: https://kubernetes.io/docs/concepts/storage/volumes/
      extraVolumes:
        - name: fsx-lustre
          persistentVolumeClaim: 
            claimName: fsx-claim
        - name: fsx-openzfs
          persistentVolumeClaim: 
            claimName: openzfs-claim
        - name: shmem
          hostPath: 
                path: /dev/shm
        # - name: nfs-home
        #   nfs:
        #     server: nfs-server.example.com
        #     path: /exports/home/
        # - name: nfs-data
        #   persistentVolumeClaim:
        #     claimName: nfs-data
      #
      # -- (object)
      # Partition describes the partition created specifically for this NodeSet to be added.
      partition:
        #
        # -- (bool)
        # Enables this NodeSet's partition line to be added in Slurm.
        enabled: true
        #
        # -- (map[string]string | map[string][]string)
        # Extra Slurm partition configuration appended onto the partition line.
        # Ref: https://slurm.schedmd.com/slurm.conf.html#lbAI
        config:
          State: UP
          MaxTime: UNLIMITED
      #
      # --(map[string]string || map[string][]string)
      # Extra Slurm node configuration appended into the --conf arguments.
      # WARNING: Values can override existing ones.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#lbAE
      nodeConfig: {}
        # Features: []
        # Gres: []
        # Weight: 1
  #
  # -- (list)
  # Slurm Partitions by object list.
  partitions:
      #
      # -- (string)
      # Name of Partition. Must be unique.
    - name: all
      #
      # -- (bool)
      # Enables the partition in Slurm.
      enabled: true
      #
      # -- (list)
      # NodeSets to put into this Partition by name/key.
      # NOTE: 'ALL' is a Slurm meta value to mean all nodes in the system.
      nodesets:
        - ALL
      #
      # -- (map[string]string | map[string][]string)
      # Extra Slurm partition configuration appended onto the partition line.
      # Ref: https://slurm.schedmd.com/slurm.conf.html#lbAI
      config:
        State: UP
        Default: "YES"
        MaxTime: UNLIMITED

#
# Slurm accounting (slurmdbd) configurations.
accounting:
  #
  # -- (bool)
  # Enables accounting services.
  enabled: true
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Set the image to use.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/slurmdbd
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # Configuration for an external database (e.g. mariadb, mysql).
  external:
    #
    # -- (bool)
    # Use an external database instead of creating one.
    # WARNING: `accounting.external.enabled=true` is mutually exclusive with `mariadb.enabled=true`.
    enabled: false
    #
    # -- (string)
    # The database user to use.
    user: slurm
    #
    # -- (string)
    # The database context to use.
    database: slurm_acct_db
    #
    # -- (string)
    # The plaintext user password to the database.
    # NOTE: ignored when secretName is not empty.
    password: ""
    #
    # -- (string)
    # The existing secret containing the user password to the database.
    secretName: ""
    #
    # -- (string)
    # The host or service to communicate with.
    host: ""
    #
    # -- (integer)
    # The database port to use.
    port: 3306
  #
  # -- (map)
  # Selector which must match a node's labels for the pod to be scheduled on that node.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  #
  # -- (object)
  # Set affinity for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: *commonAffinity
  #
  # -- (list)
  # Configure pod tolerations.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  #
  # -- (object)
  # Set container resource requests and limits for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources: {}
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 4Gi

#
# `bitnami/mariadb` subchart configurations.
# Ref: https://github.com/bitnami/charts/blob/main/bitnami/mariadb/values.yaml
mariadb:
  enabled: true
  auth:
    username: slurm
    database: slurm_acct_db
  tls:
    enabled: false
  tde:
    enabled: false
  primary:
    # NOTE: https://slurm.schedmd.com/accounting.html#slurm-accounting-configuration-before-build
    configuration: |-
      [mysqld]
      skip-name-resolve
      explicit_defaults_for_timestamp
      basedir=/opt/bitnami/mariadb
      datadir=/bitnami/mariadb/data
      plugin_dir=/opt/bitnami/mariadb/plugin
      port={{ .Values.primary.containerPorts.mysql }}
      socket=/opt/bitnami/mariadb/tmp/mysql.sock
      tmpdir=/opt/bitnami/mariadb/tmp
      innodb_buffer_pool_size=4096M
      innodb_lock_wait_timeout=900
      innodb_log_file_size=1024M
      max_allowed_packet=16M
      bind-address=*
      pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
      log-error=/opt/bitnami/mariadb/logs/mysqld.log
      character-set-server=UTF8
      collation-server=utf8_general_ci
      slow_query_log=0
      long_query_time=10.0
      binlog_expire_logs_seconds=2592000
      {{- if .Values.tls.enabled }}
      ssl_cert=/opt/bitnami/mariadb/certs/{{ .Values.tls.certFilename }}
      ssl_key=/opt/bitnami/mariadb/certs/{{ .Values.tls.certKeyFilename }}
      {{- if (include "mariadb.tlsCACert" .) }}
      ssl_ca={{ include "mariadb.tlsCACert" . }}
      {{- end }}
      {{- end }}
      {{- if .Values.tde.enabled }}
      plugin_load_add=file_key_management
      file_key_management_filename=/opt/bitnami/mariadb/tde/{{ .Values.tde.encryptedKeyFilename }}
      file_key_management_filekey=FILE:/opt/bitnami/mariadb/tde/{{ .Values.tde.randomKeyFilename }}
      file_key_management_encryption_algorithm={{ .Values.tde.fileKeyManagementEncryptionAlgorithm }}
      innodb_encrypt_tables={{ .Values.tde.innodbEncryptTables }}
      innodb_encrypt_log={{ .Values.tde.innodbEncryptLog }}
      innodb_encrypt_temporary_tables={{ .Values.tde.innodbEncryptTemporaryTables }}
      innodb_encryption_threads={{ .Values.tde.innodbEncryptionThreads }}
      encrypt_tmp_disk_tables={{ .Values.tde.encryptTmpDiskTables }}
      encrypt_tmp_files={{ .Values.tde.encryptTmpTiles }}
      encrypt_binlog={{ .Values.tde.encryptBINLOG }}
      aria_encrypt_tables={{ .Values.tde.ariaEncryptTables }}
      {{- end }}

      [client]
      port=3306
      socket=/opt/bitnami/mariadb/tmp/mysql.sock
      default-character-set=UTF8
      plugin_dir=/opt/bitnami/mariadb/plugin

      [manager]
      port=3306
      socket=/opt/bitnami/mariadb/tmp/mysql.sock
      pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
    persistence:
      enabled: false
      existingClaim: ""
      storageClass: standard
      labels: {}
      annotations: {}
      accessModes:
        - ReadWriteOnce
      size: 8Gi
      selector: {}
    priorityClassName: ""
    tolerations: []
    affinity: *commonAffinity
  metrics:
    enabled: false
    serviceMonitor:
      enabled: false
  nodeSelector:
    kubernetes.io/os: linux
  affinity: {}
  resources: {}

#
# Slurm REST API (slurmrestd) configurations.
restapi:
  #
  # -- (bool)
  # Enables restapi services.
  enabled: true
  #
  # -- (integer)
  # Set the number of replicas to deploy.
  replicas: 1
  #
  # -- (string)
  # Set the image pull policy.
  imagePullPolicy: IfNotPresent
  #
  # Set the image to use.
  image:
    #
    # -- (string)
    # Set the image repository to use.
    repository: ghcr.io/slinkyproject/slurmrestd
    #
    # -- (string)
    # Set the image tag to use.
    tag: 24.11-ubuntu24.04
  #
  # -- (object)
  # The restapi service configuration.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/service/
  service: {}
    # type: LoadBalancer
    # externalIPs: []
    # externalName: my.slurmrestd.example.com
  #
  # -- (integer)
  # The external service port number.
  servicePort: 6820
  #
  # -- (integer)
  # The external service node port number.
  # Ignored unless `service.type == NodePort`.
  serviceNodePort: 36820
  #
  # -- (string)
  # Set the priority class to use.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
  priorityClassName: ""
  #
  # -- (map)
  # Selector which must match a node's labels for the pod to be scheduled on that node.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux
  #
  # -- (object)
  # Set affinity for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: *commonAffinity
  #
  # -- (list)
  # Configure pod tolerations.
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  #
  # -- (object)
  # Set container resource requests and limits for Kubernetes Pod scheduling.
  # Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources: {}
    # requests:
    #   cpu: 1
    #   memory: 1Gi
    # limits:
    #   cpu: 2
    #   memory: 4Gi

#
# `slurm-exporter` subchart configurations.
# Ref: https://github.com/SlinkyProject/slurm-exporter/-/blob/main/helm/slurm-exporter/values.yaml
slurm-exporter:
  enabled: false
  exporter:
    enabled: true
    secretName: "slurm-token-exporter"
    affinity: *commonAffinity