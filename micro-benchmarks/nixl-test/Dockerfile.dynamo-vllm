# syntax=docker/dockerfile:1.10.0

# Built for Amazon Web Services by Anton Alexander and Alex Iankoulski
# Based on NVIDIA components with Apache-2.0 license
# SPDX-License-Identifier: Apache-2.0

ARG NIXL_BASE_IMAGE="nixl-h100-efa:optimized"
ARG DYNAMO_BASE_IMAGE="nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.4.0"
ARG PYTORCH_IMAGE="nvcr.io/nvidia/pytorch"
ARG PYTORCH_IMAGE_TAG="25.06-py3"
ARG RUNTIME_IMAGE="nvcr.io/nvidia/cuda"
ARG RUNTIME_IMAGE_TAG="12.8.1-runtime-ubuntu24.04"

# vLLM configuration (aligned with official Dynamo pyproject.toml)
ARG VLLM_REF="v0.10.2"
ARG DEEPGEMM_REF=""
ARG FLASHINF_REF="v0.1.8"
ARG TORCH_BACKEND="cu128"
ARG CUDA_VERSION="12.8"
ARG MAX_JOBS=16

# Build acceleration (optional)
ARG USE_SCCACHE=false
ARG SCCACHE_BUCKET=""
ARG SCCACHE_REGION=""

ARG ARCH=amd64
ARG ARCH_ALT=x86_64
ARG PYTHON_VERSION=3.12
ARG ENABLE_KVBM=false

# GPU Architecture (SM compute capability)
ARG CUDA_ARCH=90
ARG CUDA_ARCH_NAME=H100

# ============================================================================
# Stage 0: NIXL base (alias for COPY --from)
# ============================================================================
FROM ${NIXL_BASE_IMAGE} AS nixl_base

# ============================================================================
# Stage 1: Dynamo artifacts (optional)
# ============================================================================
FROM ${DYNAMO_BASE_IMAGE} AS dynamo_base
RUN mkdir -p /opt/dynamo/wheelhouse

# ============================================================================
# Stage 2: PyTorch from NGC
# ============================================================================
FROM ${PYTORCH_IMAGE}:${PYTORCH_IMAGE_TAG} AS framework

# ============================================================================
# Stage 3: vLLM Installation (simplified - use pip wheel)
# ============================================================================
FROM nixl_base AS vllm_builder

ARG VLLM_REF
ARG TORCH_BACKEND
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG ARCH_ALT
ARG USE_SOURCE_BUILD

WORKDIR /workspace

# Install minimal dependencies
RUN apt-get update -y && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python${PYTHON_VERSION}-dev \
        git \
        wget \
        ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Create virtual environment
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
RUN mkdir -p /opt/dynamo/venv && \
    uv venv /opt/dynamo/venv --python $PYTHON_VERSION

# Activate virtual environment
ENV VIRTUAL_ENV=/opt/dynamo/venv \
    PATH="/opt/dynamo/venv/bin:${PATH}" \
    CUDA_HOME=/usr/local/cuda \
    UV_LINK_MODE=copy

# Install PyTorch
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install \
        --index-url https://download.pytorch.org/whl/${TORCH_BACKEND} \
        torch torchvision && \
    echo "‚úÖ PyTorch installed"

# Install vLLM from pip (MUCH FASTER - no compilation needed!)
RUN --mount=type=cache,target=/root/.cache/uv \
    if [ "${USE_SOURCE_BUILD}" = "true" ]; then \
        echo "‚ö†Ô∏è  Building vLLM from source (slow - 30-60 min)..." && \
        apt-get update && apt-get install -y build-essential cmake ninja-build g++ && \
        cd /tmp && \
        git clone https://github.com/vllm-project/vllm.git && \
        cd vllm && git checkout ${VLLM_REF} && \
        sed -i 's/^license = "Apache-2.0"$/license = {text = "Apache-2.0"}/' pyproject.toml && \
        uv pip install packaging wheel setuptools ninja cmake pybind11 Cython && \
        uv pip install --no-build-isolation -e . && \
        echo "‚úÖ vLLM built from source"; \
    else \
        echo "üöÄ Installing vLLM from pip (fast - pre-built wheel)..." && \
        uv pip install "vllm[flashinfer]==${VLLM_REF#v}" && \
        echo "‚úÖ vLLM ${VLLM_REF} installed from pip with flashinfer"; \
    fi


# ============================================================================
# Stage 4: Runtime Image
# ============================================================================
FROM ${RUNTIME_IMAGE}:${RUNTIME_IMAGE_TAG} AS runtime

WORKDIR /workspace

ARG ARCH_ALT
ARG PYTHON_VERSION
ARG ENABLE_KVBM
ARG CUDA_ARCH
ARG CUDA_ARCH_NAME
ENV NIXL_PREFIX=/opt/nvidia/nvda_nixl
ENV NIXL_LIB_DIR=$NIXL_PREFIX/lib/${ARCH_ALT}-linux-gnu
ENV NIXL_PLUGIN_DIR=$NIXL_LIB_DIR/plugins

# Install runtime dependencies
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        build-essential \
        g++ \
        ninja-build \
        git \
        git-lfs \
        python${PYTHON_VERSION}-dev \
        python3-pip \
        sed \
        findutils \
        coreutils \
        libcudnn9-cuda-12 \
        libzmq3-dev \
        ibverbs-providers \
        ibverbs-utils \
        libibumad3 \
        libibverbs1 \
        libnuma1 \
        librdmacm1 \
        rdma-core \
        openssh-client \
        openssh-server \
        ca-certificates \
        curl \
        jq \
        wget && \
    rm -rf /var/lib/apt/lists/*

# Copy CUDA development tools from framework
COPY --from=framework /usr/local/cuda/bin/nvcc /usr/local/cuda/bin/nvcc
COPY --from=framework /usr/local/cuda/bin/cudafe++ /usr/local/cuda/bin/cudafe++
COPY --from=framework /usr/local/cuda/bin/ptxas /usr/local/cuda/bin/ptxas
COPY --from=framework /usr/local/cuda/bin/fatbinary /usr/local/cuda/bin/fatbinary
COPY --from=framework /usr/local/cuda/include/ /usr/local/cuda/include/
COPY --from=framework /usr/local/cuda/nvvm /usr/local/cuda/nvvm
COPY --from=framework /usr/local/cuda/lib64/libcudart.so* /usr/local/cuda/lib64/
COPY --from=framework /usr/local/lib/lib* /usr/local/lib/

# Copy optional CUDA components
RUN --mount=type=bind,from=framework,source=/usr/local/cuda/lib64,target=/tmp/cuda_lib \
    if [ -f /tmp/cuda_lib/libcupti.so ]; then \
        cp -a /tmp/cuda_lib/libcupti* /usr/local/cuda/lib64/ && \
        echo "‚úÖ libcupti copied"; \
    fi && \
    if [ -f /tmp/cuda_lib/libcusparseLt.so ]; then \
        cp -a /tmp/cuda_lib/libcusparseLt* /usr/local/cuda/lib64/ && \
        echo "‚úÖ libcusparseLt copied"; \
    fi

# Copy NATS & ETCD from dynamo_base or nixl_base
RUN --mount=type=bind,from=dynamo_base,source=/usr/bin,target=/tmp/dyn_bin \
    --mount=type=bind,from=dynamo_base,source=/usr/local/bin,target=/tmp/dyn_local_bin \
    --mount=type=bind,from=nixl_base,source=/usr/bin,target=/tmp/nixl_bin \
    --mount=type=bind,from=nixl_base,source=/usr/local/bin,target=/tmp/nixl_local_bin \
    mkdir -p /usr/bin /usr/local/bin/etcd && \
    ([ -f /tmp/dyn_bin/nats-server ] && cp /tmp/dyn_bin/nats-server /usr/bin/ || \
     [ -f /tmp/nixl_bin/nats-server ] && cp /tmp/nixl_bin/nats-server /usr/bin/ || \
     echo "‚ö†Ô∏è  nats-server not found") && \
    ([ -d /tmp/dyn_local_bin/etcd ] && cp -r /tmp/dyn_local_bin/etcd/* /usr/local/bin/etcd/ || \
     [ -f /tmp/dyn_local_bin/etcd ] && cp /tmp/dyn_local_bin/etcd* /usr/local/bin/etcd/ || \
     [ -f /tmp/nixl_local_bin/etcd ] && cp /tmp/nixl_local_bin/etcd* /usr/local/bin/etcd/ || \
     echo "‚ö†Ô∏è  etcd not found") && \
    echo "‚úÖ Optional binaries copied"

ENV PATH=/usr/local/bin/etcd/:/usr/local/cuda/nvvm/bin:$PATH

# ============================================================================
# CRITICAL: Copy and register ALL communication libraries BEFORE Python setup
# ============================================================================

# Copy UCX from NIXL base
COPY --from=nixl_base /usr/local/ucx /usr/local/ucx
RUN echo "/usr/local/ucx/lib" > /etc/ld.so.conf.d/ucx.conf && \
    echo "/usr/local/ucx/lib/ucx" >> /etc/ld.so.conf.d/ucx.conf && \
    ldconfig && \
    ldconfig -p | grep -i libucs && \
    echo "‚úÖ UCX registered"

ENV PATH=/usr/local/ucx/bin:$PATH

# Copy libfabric from NIXL base
COPY --from=nixl_base /usr/local/libfabric /usr/local/libfabric
RUN echo "/usr/local/libfabric/lib" > /etc/ld.so.conf.d/libfabric.conf && \
    ln -sf /usr/local/libfabric/bin/* /usr/local/bin/ && \
    ldconfig && \
    echo "‚úÖ libfabric registered"

# Copy GDRCopy from NIXL base
COPY --from=nixl_base /opt/gdrcopy /opt/gdrcopy
RUN echo "/opt/gdrcopy/lib" > /etc/ld.so.conf.d/gdrcopy.conf && \
    ldconfig && \
    echo "‚úÖ GDRCopy registered"

# Copy NCCL from NIXL base (or framework base image)
RUN --mount=type=bind,from=nixl_base,source=/usr/lib/x86_64-linux-gnu,target=/tmp/libs \
    --mount=type=bind,from=nixl_base,source=/usr/local/lib,target=/tmp/nixl_libs \
    if [ -f /tmp/nixl_libs/libnccl.so ]; then \
        cp -a /tmp/nixl_libs/libnccl.so* /usr/local/lib/ && \
        echo "‚úÖ NCCL copied from nixl_base"; \
    elif [ -f /tmp/libs/libnccl.so ]; then \
        cp -a /tmp/libs/libnccl.so* /usr/local/lib/ && \
        echo "‚úÖ NCCL copied from framework base image"; \
    else \
        echo "‚ö†Ô∏è  NCCL not found"; \
    fi && \
    ldconfig && ldconfig -p | grep libnccl || true

# Copy aws-ofi-nccl from nixl_base (if NCCL was installed)
RUN --mount=type=bind,from=nixl_base,source=/opt,target=/tmp/nixl_opt \
    if [ -d /tmp/nixl_opt/aws-ofi-nccl ]; then \
        cp -r /tmp/nixl_opt/aws-ofi-nccl /opt/ && \
        echo "/opt/aws-ofi-nccl/lib" > /etc/ld.so.conf.d/aws-ofi-nccl.conf && \
        ldconfig && \
        echo "‚úÖ aws-ofi-nccl copied from nixl_base"; \
    else \
        echo "‚ö†Ô∏è  aws-ofi-nccl not found (NCCL may not have been installed in base)"; \
    fi

# Create libfabric symlinks in /usr/local/lib for easier discovery
RUN ln -sf /usr/local/libfabric/lib/libfabric.so* /usr/local/lib/ && \
    ldconfig && \
    echo "‚úÖ libfabric symlinked to /usr/local/lib"

# Copy NIXL
COPY --from=nixl_base /opt/nvidia/nvda_nixl /opt/nvidia/nvda_nixl
RUN echo "${NIXL_LIB_DIR}" > /etc/ld.so.conf.d/nixl.conf && \
    echo "${NIXL_PLUGIN_DIR}" >> /etc/ld.so.conf.d/nixl.conf && \
    ldconfig && \
    echo "‚úÖ NIXL registered"

# Copy OpenMPI and UCC from PyTorch NGC (remove UCX to avoid contamination)
COPY --from=framework /opt/hpcx/ompi /opt/hpcx/ompi
COPY --from=framework /opt/hpcx/ucc /opt/hpcx/ucc
COPY --from=framework /usr/lib/${ARCH_ALT}-linux-gnu/libnuma.so* /usr/lib/${ARCH_ALT}-linux-gnu/

# CRITICAL: Remove HPC-X UCX if it exists
RUN rm -rf /opt/hpcx/ucx /opt/hpcx/sharp /opt/hpcx/hcoll 2>/dev/null || true && \
    echo "‚úÖ HPC-X UCX removed (keeping only OpenMPI/UCC)"

# Register HPC-X libraries
RUN echo "/opt/hpcx/ucc/lib" > /etc/ld.so.conf.d/hpcx.conf && \
    echo "/opt/hpcx/ompi/lib" >> /etc/ld.so.conf.d/hpcx.conf && \
    ldconfig && \
    echo "‚úÖ HPC-X OpenMPI/UCC registered"

# FINAL ldconfig update before Python
RUN ldconfig && \
    echo "=== Final library registration ===" && \
    ldconfig -p | grep -E "libucs|libfabric|libnccl|libnixl" && \
    echo "‚úÖ All libraries ready for Python/PyTorch"

# Set comprehensive environment variables
ENV DYNAMO_HOME=/workspace
ENV LD_LIBRARY_PATH=\
/usr/local/lib:\
/usr/local/libfabric/lib:\
/usr/local/ucx/lib:\
/usr/local/ucx/lib/ucx:\
/opt/gdrcopy/lib64:\
$NIXL_LIB_DIR:\
$NIXL_PLUGIN_DIR:\
/opt/hpcx/ompi/lib:\
/opt/hpcx/ucc/lib:\
$LD_LIBRARY_PATH

ENV PATH="${VIRTUAL_ENV}/bin:/opt/hpcx/ompi/bin:/usr/local/ucx/bin:/usr/local/libfabric/bin:/usr/local/cuda/bin:/usr/local/cuda/nvvm/bin:$PATH"
ENV OPAL_PREFIX=/opt/hpcx/ompi

# ============================================================================
# Python and PyTorch Setup (AFTER all libraries are registered)
# ============================================================================

# Copy uv and entire virtual environment from vllm_builder
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
COPY --from=vllm_builder /opt/dynamo/venv /opt/dynamo/venv

# Set virtual environment variables
ENV VIRTUAL_ENV=/opt/dynamo/venv \
    PATH="/opt/dynamo/venv/bin:${PATH}" \
    UV_LINK_MODE=copy

# Copy PyTorch and dependencies from NGC (complete copy like NVIDIA does)
ARG TORCH_VER=2.8.0a0+5228986c39.nv25.6
ARG TORCHVISION_VER=0.22.0a0+95f10a4e
ARG PYTORCH_TRITON_VER=3.3.0+git96316ce52.nvinternal
ARG JINJA2_VER=3.1.6
ARG SYMPY_VER=1.14.0
ARG FLASH_ATTN_VER=2.7.4.post1

COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch
COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torch-${TORCH_VER}.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torch-${TORCH_VER}.dist-info
COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchgen ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchgen
COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision
COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/torchvision-${TORCHVISION_VER}.dist-info ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/torchvision-${TORCHVISION_VER}.dist-info
COPY --from=framework /usr/local/lib/python${PYTHON_VERSION}/dist-packages/functorch ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/functorch

# Copy additional PyTorch dependencies
RUN --mount=type=bind,from=framework,source=/usr/local/lib/python${PYTHON_VERSION}/dist-packages,target=/tmp/pydist \
    for pkg in jinja2 sympy flash_attn triton torchvision.libs; do \
        if [ -d /tmp/pydist/$pkg ]; then \
            cp -r /tmp/pydist/$pkg ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/ && \
            find /tmp/pydist -maxdepth 1 -name "${pkg}-*.dist-info" -exec cp -r {} ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/ \; 2>/dev/null || true && \
            echo "‚úÖ Copied $pkg"; \
        fi; \
    done && \
    if compgen -G "/tmp/pydist/flash_attn_2_cuda.cpython-*-*-linux-gnu.so" > /dev/null; then \
        cp /tmp/pydist/flash_attn_2_cuda.cpython-*-*-linux-gnu.so ${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/ && \
        echo "‚úÖ Copied flash_attn CUDA module"; \
    fi

# ============================================================================
# vLLM is already installed in the venv copied from vllm_builder stage
# (either from pip wheel or source build depending on USE_SOURCE_BUILD flag)
# ============================================================================

# ============================================================================
# Dynamo Packages and Dependencies
# ============================================================================

# Copy NVIDIA entrypoint
COPY container/nvidia_entrypoint.sh /opt/nvidia/nvidia_entrypoint.sh
RUN chmod +x /opt/nvidia/nvidia_entrypoint.sh

# Copy benchmarks
COPY benchmarks/ /opt/dynamo/benchmarks/

# Copy and install Dynamo wheelhouse (optional)
RUN --mount=type=bind,from=dynamo_base,source=/opt/dynamo/wheelhouse,target=/tmp/wheels \
    mkdir -p /opt/dynamo/wheelhouse && \
    if [ "$(ls -A /tmp/wheels 2>/dev/null)" ]; then \
        cp -r /tmp/wheels/* /opt/dynamo/wheelhouse/ && \
        uv pip install \
            /opt/dynamo/wheelhouse/ai_dynamo_runtime*.whl \
            /opt/dynamo/wheelhouse/ai_dynamo*any.whl \
            /opt/dynamo/wheelhouse/nixl/nixl*.whl && \
        if [ "${ENABLE_KVBM}" = "true" ]; then \
            uv pip install /opt/dynamo/wheelhouse/kvbm*.whl; \
        fi && \
        echo "‚úÖ Dynamo packages installed"; \
    else \
        echo "‚ö†Ô∏è  No Dynamo wheelhouse (OK for custom build)"; \
    fi

# Install benchmarks
RUN cd /opt/dynamo/benchmarks && \
    UV_GIT_LFS=1 uv pip install --no-cache . && \
    cd - && \
    rm -rf /opt/dynamo/benchmarks

# Install common and test dependencies
RUN --mount=type=bind,source=./container/deps/requirements.txt,target=/tmp/requirements.txt \
    --mount=type=bind,source=./container/deps/requirements.test.txt,target=/tmp/requirements.test.txt \
    UV_GIT_LFS=1 uv pip install \
        --no-cache \
        -r /tmp/requirements.txt \
        -r /tmp/requirements.test.txt

# Copy validation scripts from NIXL base
RUN --mount=type=bind,from=nixl_base,source=/usr/local/bin,target=/tmp/nixl_bin \
    for script in nixl-validate efa-test nixlbench-test env-info; do \
        if [ -f /tmp/nixl_bin/$script ]; then \
            cp /tmp/nixl_bin/$script /usr/local/bin/ && \
            chmod +x /usr/local/bin/$script && \
            echo "‚úÖ $script"; \
        fi; \
    done

# Copy workspace content
COPY . /workspace/

# Setup launch message
COPY ATTRIBUTION* LICENSE /workspace/
RUN --mount=type=bind,source=./container/launch_message.txt,target=/tmp/launch.txt \
    sed '/^#\s/d' /tmp/launch.txt > ~/.launch_screen && \
    echo "cat ~/.launch_screen" >> ~/.bashrc && \
    echo "source $VIRTUAL_ENV/bin/activate" >> ~/.bashrc

# GPU+EFA optimization environment variables
ENV NCCL_NET="AWS Libfabric" \
    NCCL_PROTO="simple" \
    NCCL_ALGO="Ring,Tree" \
    FI_PROVIDER="efa" \
    FI_EFA_USE_DEVICE_RDMA="1" \
    FI_EFA_FORK_SAFE="1" \
    UCX_TLS="tcp,cuda_copy,cuda_ipc" \
    UCX_NET_DEVICES="all" \
    CUDAARCHS="${CUDA_ARCH}" \
    CUDA_ARCH_NAME="${CUDA_ARCH_NAME}" \
    NCCL_DEBUG="INFO"

# Copy and run build validation
COPY scripts/validate-build.sh /usr/local/bin/validate-build
RUN chmod +x /usr/local/bin/validate-build && validate-build

ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh"]
CMD ["/bin/bash"]

# ============================================================================
# Stage 5: Slim Image (Debloated for Production)
# ============================================================================
FROM runtime AS slim

# Copy debloat script
COPY scripts/debloat-container.sh /tmp/debloat-container.sh
RUN chmod +x /tmp/debloat-container.sh

# Run debloat (script removes itself as part of /tmp/* cleanup)
RUN /tmp/debloat-container.sh

# Final cleanup
RUN apt-get autoremove -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /var/cache/apt/* /tmp/* /var/tmp/*

# Run build validation for slim image
RUN validate-build

# ============================================================================
# Stage 6: Development Image
# ============================================================================
FROM runtime AS dev

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        nvtop wget tmux vim git iproute2 rsync zip unzip htop \
        autoconf automake cmake libtool meson net-tools pybind11-dev \
        clang libclang-dev protobuf-compiler && \
    rm -rf /var/lib/apt/lists/*

ENV WORKSPACE_DIR=/workspace \
    DYNAMO_HOME=/workspace \
    RUSTUP_HOME=/usr/local/rustup \
    CARGO_HOME=/usr/local/cargo \
    CARGO_TARGET_DIR=/workspace/target

# Copy Rust toolchain from dynamo_base or nixl_base
RUN --mount=type=bind,from=dynamo_base,source=/usr/local,target=/tmp/dyn_local \
    --mount=type=bind,from=nixl_base,source=/usr/local,target=/tmp/nixl_local \
    if [ -d /tmp/dyn_local/rustup ]; then \
        cp -r /tmp/dyn_local/rustup /usr/local/ && echo "‚úÖ Rust from dynamo_base"; \
    elif [ -d /tmp/nixl_local/rustup ]; then \
        cp -r /tmp/nixl_local/rustup /usr/local/ && echo "‚úÖ Rust from nixl_base"; \
    fi && \
    if [ -d /tmp/dyn_local/cargo ]; then \
        cp -r /tmp/dyn_local/cargo /usr/local/ && echo "‚úÖ Cargo from dynamo_base"; \
    elif [ -d /tmp/nixl_local/cargo ]; then \
        cp -r /tmp/nixl_local/cargo /usr/local/ && echo "‚úÖ Cargo from nixl_base"; \
    fi

ENV PATH=/usr/local/cargo/bin:$PATH

# Install maturin for Rust development
RUN uv pip install maturin[patchelf]

# Optional: Install editable Dynamo (only if source files exist)
RUN --mount=type=bind,source=.,target=/tmp/src \
    if [ -f /tmp/src/pyproject.toml ]; then \
        cp /tmp/src/pyproject.toml /workspace/ && \
        cp /tmp/src/README.md /workspace/ 2>/dev/null || touch /workspace/README.md && \
        cp /tmp/src/hatch_build.py /workspace/ 2>/dev/null || true && \
        cd /workspace && \
        uv pip install --no-deps -e . && \
        echo "‚úÖ Editable install complete"; \
    else \
        echo "‚ö†Ô∏è  No pyproject.toml - skipping editable install"; \
        echo "This is expected if not building from AI Dynamo source repo"; \
    fi

# Run build validation for dev image
RUN validate-build

CMD ["/bin/bash"]