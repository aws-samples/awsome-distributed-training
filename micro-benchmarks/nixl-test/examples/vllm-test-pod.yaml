---
apiVersion: v1
kind: Pod
metadata:
  name: vllm-test
  namespace: default
  labels:
    app: vllm-test
spec:
  restartPolicy: Never
  containers:
  - name: vllm-test
    # Use ECR image
    image: 058264135704.dkr.ecr.us-east-2.amazonaws.com/dynamo-vllm:slim
    # Alternative: Use local image tag (requires image on node)
    # image: dynamo-vllm:slim
    imagePullPolicy: IfNotPresent
    command: ["/bin/bash", "-c"]
    args:
      - |
        echo "===== vLLM Test Environment ====="
        echo "Container: dynamo-vllm:slim"
        echo "vLLM: $(source /opt/venv/bin/activate && python -c 'import vllm; print(vllm.__version__)')"
        echo "PyTorch: $(source /opt/venv/bin/activate && python -c 'import torch; print(torch.__version__)')"
        echo "CUDA Available: $(source /opt/venv/bin/activate && python -c 'import torch; print(torch.cuda.is_available())')"
        echo "GPU Count: $(source /opt/venv/bin/activate && python -c 'import torch; print(torch.cuda.device_count())')"
        echo ""
        echo "To run interactive test, use:"
        echo "  kubectl exec -it vllm-test -- bash"
        echo "  source /opt/venv/bin/activate"
        echo "  python /workspace/test-vllm-local.py"
        echo ""
        echo "Keeping pod alive for testing..."
        sleep infinity
    resources:
      requests:
        nvidia.com/gpu: 1
        memory: "16Gi"
        cpu: "4"
      limits:
        nvidia.com/gpu: 1
        memory: "32Gi"
        cpu: "8"
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]
    volumeMounts:
    - name: workspace
      mountPath: /workspace
    - name: dshm
      mountPath: /dev/shm
  volumes:
  - name: workspace
    emptyDir: {}
  - name: dshm
    emptyDir:
      medium: Memory
      sizeLimit: 8Gi
