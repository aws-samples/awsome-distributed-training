# Config for running the InferenceRecipe in generate.py to generate output from an LLM
#
# To launch, run the following command from root torchtune directory:
#    tune run generate --config generation

# Model arguments
model:
  _component_: torchtune.models.llama3.llama3_70b

checkpointer:
  _component_: torchtune.utils.FullModelHFCheckpointer
  checkpoint_dir:  ${MODEL_PATH}/${HF_MODEL}
  checkpoint_files: [
      model-00001-of-00030.safetensors,
      model-00002-of-00030.safetensors,
      model-00003-of-00030.safetensors,
      model-00004-of-00030.safetensors,
      model-00005-of-00030.safetensors,
      model-00006-of-00030.safetensors,
      model-00007-of-00030.safetensors,
      model-00008-of-00030.safetensors,
      model-00009-of-00030.safetensors,
      model-00010-of-00030.safetensors,
      model-00011-of-00030.safetensors,
      model-00012-of-00030.safetensors,
      model-00013-of-00030.safetensors,
      model-00014-of-00030.safetensors,
      model-00015-of-00030.safetensors,
      model-00016-of-00030.safetensors,
      model-00017-of-00030.safetensors,
      model-00018-of-00030.safetensors,
      model-00019-of-00030.safetensors,
      model-00020-of-00030.safetensors,
      model-00021-of-00030.safetensors,
      model-00022-of-00030.safetensors,
      model-00023-of-00030.safetensors,
      model-00024-of-00030.safetensors,
      model-00025-of-00030.safetensors,
      model-00026-of-00030.safetensors,
      model-00027-of-00030.safetensors,
      model-00028-of-00030.safetensors,
      model-00029-of-00030.safetensors,
      model-00030-of-00030.safetensors,
  ]
  recipe_checkpoint: null
  output_dir: ${MODEL_PATH}/${HF_MODEL}-quantized
  model_type: LLAMA3

device: cuda
dtype: bf16

seed: 1234

# Tokenizer arguments
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  path: ${MODEL_PATH}/${HF_MODEL}/original/tokenizer.model


# Generation arguments; defaults taken from gpt-fast
prompt: "Hello, my name is"
max_new_tokens: 300
temperature: 0.6 # 0.8 and 0.6 are popular values to try
top_k: 300

quantizer: null
