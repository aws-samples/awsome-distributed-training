apiVersion: v1
kind: Service
metadata:
  name: etcd
spec:
  ports:
    - name: etcd-client-port
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    app: etcd

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: etcd
  name: etcd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      labels:
        app: etcd
    spec:
      containers:
        - name: etcd
          command: ["/usr/local/bin/etcd"]
          args:
            - "--data-dir"
            - "/var/lib/etcd"
            - "--enable-v2"
            - "--listen-client-urls"
            - "http://0.0.0.0:2379"
            - "--advertise-client-urls"
            - "http://0.0.0.0:2379"
            - "--initial-cluster-state"
            - "new"
          image: quay.io/coreos/etcd:latest
          ports:
            - containerPort: 2379
              name: client
              protocol: TCP
            - containerPort: 2380
              name: server
              protocol: TCP
      restartPolicy: Always
---
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: megatron
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 1
    maxReplicas: 64
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 90
  pytorchReplicaSpecs:
    Worker:
      replicas: ${NUM_NODES}
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: megatron
        spec:
          volumes:
            - name: shmem
              hostPath: 
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
            - name: fsx-pv
              persistentVolumeClaim:
                claimName: fsx-pvc
          #nodeSelector:
          #  node.kubernetes.io/instance-type: "${INSTANCE_TYPE}"
          containers:
            - name: pytorch
              image: ${IMAGE_URI}
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: ${GPU_PER_NODE}
                  vpc.amazonaws.com/efa: ${EFA_PER_NODE}
                limits:
                  nvidia.com/gpu: ${GPU_PER_NODE}
                  vpc.amazonaws.com/efa: ${EFA_PER_NODE}
              env:
              # for P5 FI_* should be commented out
              - name: LOGLEVEL
                value: "DEBUG"
              #- name: FI_PROVIDER
              #  value: ${FI_PROVIDER}
              #- name: FI_EFA_USE_DEVICE_RDMA
              #  value: "1"
              #- name: FI_EFA_FORK_SAFE
              #  value: "1"
              #- name: FI_LOG_LEVEL
              #  value: "1"
              #- name: FI_EFA_ENABLE_SHM_TRANSFER
              #  value: "1"
              - name: TORCH_DISTRIBUTED_DEBUG
                value: "DETAIL"
              - name: TORCH_NCCL_ENABLE_MONITORING
                value: "1"
              - name: TORCH_NCCL_TRACE_BUFFER_SIZE
                value: "20000"
              - name: TORCH_NCCL_DUMP_ON_TIMEOUT
                value: "1"
              - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
                value: "/local/nccl_trace_rank_"
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: "expandable_segments:True"
              - name: NCCL_DEBUG
                value: "INFO"
              - name: NCCL_SOCKET_IFNAME
                value: "^lo"
              - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
                value: "1"
              - name: CUDA_DEVICE_MAX_CONNECTIONS
                value: "1"
              #- name: TORCH_DIST_INIT_BARRIER
              #  value: "1"
              #- name: NCCL_IGNORE_DISABLED_P2P
              #  value: "1"
              #- name: NCCL_NVLS_ENABLE
              #  value: "0"
              command: [ "/usr/local/bin/torchrun", "--nproc_per_node=${GPU_PER_NODE}", "--nnodes=${NUM_NODES}"]
              args:
                - /workspace/Megatron-LM/pretrain_gpt.py
                - --tensor-model-parallel-size=${TENSOR_PARALLEL}
                - --pipeline-model-parallel-size=${PIPELINE_PARALLEL}
                - --num-layers=${NUM_LAYERS}
                - --hidden-size=${HIDDEN_SIZE}
                - --num-attention-heads=${NUM_ATTENTION_HEADS}
                - --seq-length=${SEQ_LENGTH}
                - --max-position-embeddings=${MAX_POSITION_EMBEDDINGS}
                - --micro-batch-size=${MICRO_BATCH_SIZE}
                - --global-batch-size=${GLOBAL_BATCH_SIZE}
                - --train-samples=146484375
                - --lr-decay-samples=126953125
                - --lr-warmup-samples=183105
                - --lr=6.0e-5
                - --min-lr=6.0e-6
                - --lr-decay-style=cosine
                - --log-interval=1
                - --eval-iters=40
                - --eval-interval=1000
                - --data-path=${DATA_PATH}/gpt2/my-gpt2_text_document
                - --vocab-file=${DATA_PATH}/gpt2/gpt2-vocab.json
                - --merge-file=${DATA_PATH}/gpt2/gpt2-merges.txt
                - --split=98,2,0
                - --clip-grad=1.0
                - --weight-decay=0.1
                - --adam-beta1=0.9
                - --adam-beta2=0.95
                - --init-method-std=0.006
                - --fp16
                - --recompute-activations
              volumeMounts:
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local
                - name: fsx-pv
                  mountPath: /fsx
