apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: stable-diffusion
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 1
    maxReplicas: 96
    maxRestarts: 100
    #metrics:
    #  - type: Resource
    #    resource:
    #      name: cpu
    #      target:
    #        type: Utilization
    #        averageUtilization: 80
  pytorchReplicaSpecs:
    Worker:
      replicas: ${NUM_NODES}
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: stable-diffusion
        spec:
          volumes:
            - name: shmem
              #emptyDir:
              #  medium: Memory
              hostPath:
                path: /dev/shm
          #nodeSelector:
          #  node.kubernetes.io/instance-type: "p5.48xlarge"
          containers:
            - name: pytorch
              image: ${REGISTRY}${DOCKER_IMAGE_NAME}:{TAG}
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu:
                  vpc.amazonaws.com/efa: 32
                limits:
                  nvidia.com/gpu:
                  vpc.amazonaws.com/efa: 32
              env:
              # for P5 FI_* should be commented out
              - name: LOGLEVEL
                value: "DEBUG"
              #- name: FI_PROVIDER
              #  value: efa
              #- name: FI_EFA_USE_DEVICE_RDMA
              #  value: "1"
              #- name: FI_EFA_FORK_SAFE
              #  value: "1"
              #- name: FI_LOG_LEVEL
              #  value: "1"
              #- name: FI_EFA_ENABLE_SHM_TRANSFER
              #  value: "1"
              - name: NCCL_DEBUG
                value: "INFO"
              - name: NCCL_ASYNC_ERROR_HANDLING
                value: "1"
              #- name: NCCL_IGNORE_DISABLED_P2P
              #  value: "1"
              - name: WANDB_MODE
                value: "offline"
              command:
                - bash
                - -c
                - "composer -n ${NUM_GPUS_PER_NODE} --world_size ${WORLD_SIZE} --node_rank $(hostname | cut -d- -f4-) --master_addr stable-diffusion-worker-0 --master_port ${MASTER_PORT} benchmark.py --use_ema --use_synth_data --device_train_microbatch_size 4"
              volumeMounts:
                - name: shmem
                  mountPath: /dev/shm