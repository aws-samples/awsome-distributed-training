# Stable Diffusion Test Case

We will follow MosaicML's stable diffusion benchmarking scripts provided [here](https://github.com/mosaicml/diffusion-benchmark/tree/main). It uses the `'stabilityai/stable-diffusion-2-base'` model. You can check the number of parameters by executing:

```bash
python3 calculate_number_of_parameters.py
Model has 1289.952427 M parameters and 865.910724 M trainable_params
``` 

To simplify testing, we have separate scripts for Single node and Multi node Distributed Training. We will also present a comparison of throughput (images/second) achieved with P4de (A100 80GB) and P5 (H100 80GB) instances.

You can export the following environment variables:

```
export PYTHON_VERSION=3.10
# We are using Python version 3.10 in this work. For a different Python version select the right Miniconda file from https://repo.anaconda.com/miniconda/
export MINICONDA_INSTALLER=Miniconda3-py310_23.5.2-0-Linux-x86_64
export CUDA_VERSION=12.1
export MOSAICML_VERSION=0.15.0
export PYTORCH_INDEX_URL=https://download.pytorch.org/whl/nightly/cu121
export PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:23.08-py3
export DOCKER_IMAGE_NAME=mosaicml-stable-diffusion
export TAG=$MOSAICML_VERSION
```

## 1 Single Node Training

For the single node training use case, we provide both a Conda environment and a Dockerfile for your convenience and preference. However, you do not need both for setting up training.


## 1.1. Create Conda Environment


To install Miniconda, please follow the steps below:

```bash
# install in the shared directory so compute nodes can source the environment:
cd /apps

# Get the appropriate Miniconda_version from https://repo.anaconda.com/miniconda/
wget -O miniconda.sh "https://repo.anaconda.com/miniconda/${MINICONDA_INSTALLER}.sh" \
    && bash miniconda.sh -b -p ./.conda \
    &&  ./.conda/bin/conda init bash

# Detect the OS based on /etc/os-release
os=$(grep '^ID=' /etc/os-release | awk -F= '{print $2}' | tr -d '"')

if [[ "$os" == "amzn" ]]; then
  source /home/ec2-user/.bashrc
elif [[ "$os" == "ubuntu" ]]; then
  source /home/ubuntu/.bashrc
else
  echo "Unknown OS: $os"
fi

conda create -n pt-nightlies python=${PYTHON_VERSION}

conda activate pt-nightlies

# Install PyTorch Nightly distribution with specified Cuda version
pip3 install --pre torch torchvision torchaudio --index-url ${PYTORCH_INDEX_URL}

# Install Diffusers and Transformers
pip3 install diffusers["torch"] transformers

# Install Weights and Biases
pip3 install wandb

# We will install Composer from source. First clone the Repo
git clone https://github.com/mosaicml/composer.git


```


### 1.2 Single Node Setup with Conda

When testing the latest version of MosaicML's Composer, we found that different ways to set up the environment with a PyTorch Nightly conda environment or a Nvidia PyTorch Docker container. For single or multi-node testing, you could use either to run distributed training. Next, we present both approaches.

The conda environment installs the PyTorch nightly distribution for Cuda 12.1 and the `diffusers, transformers` and weights and biases libraries and also clones the Composer repository. Before installing Composer, we need to avoid some torch and torchvision version checks by commenting out those lines in [`composer/setup.py`](https://github.com/mosaicml/composer/blob/dev/setup.py) so it looks like:


```bash
install_requires = [
    'pyyaml>=6.0,<7',
    'tqdm>=4.62.3,<5',
    'torchmetrics>=0.10.0,<1.1',
    'torch_optimizer>=0.3.0,<0.4',
    'requests>=2.26.0,<3',
    'numpy>=1.21.5,<1.27.0',
    'psutil>=5.8.0,<6',
    'coolname>=1.1.0,<3',
    'tabulate==0.9.0',  # for auto-generating tables
    'py-cpuinfo>=8.0.0,<10',
    'packaging>=21.3.0,<23',
    'importlib-metadata>=5.0.0,<7',
    'mosaicml-cli>=0.5.8,<0.6',
]
```
Once this change is done, you can install composer as `pip3 install -e .`

### 1.3 OPTIONAL - Single Node Setup with Docker

The `single-node` folder also has the Dockerfile with commands to build the image and run the container. If you are opting to setup training with a Conda environment, then this setup is not needed. Run this setup only if you need to run MosaicML Composer from within a Nvidia PyTorch container.

```bash
cd awsome-distributed-training/3.test_cases/6.stable-diffusion/single-node
# build the image
docker build --build-arg MOSAICML_VERSION=${MOSAICML_VERSION} --build-arg PYTORCH_IMAGE=${PYTORCH_IMAGE} --build-arg PYTORCH_INDEX_URL=${PYTORCH_INDEX_URL} -t ${DOCKER_IMAGE_NAME}:${TAG} -f 0.Dockerfile .

# run container
docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it ${DOCKER_IMAGE_NAME} /bin/bash

```

### 1.4 Single Node Training

Once you are in the conda environment or the container, run the following to kickstart training. In all these tests, we are using synthetic data generated by `diffusion-benchmark/data.py`

```bash
composer benchmark.py --use_ema --use_synth_data --device_train_microbatch_size 4
```

And you should see training starts as:

```bash
Namespace(batch_size=2048, image_size=512, remote=None, local='/tmp/mds-cache/mds-laion-2/', use_synth_data=True, model_name='stabilityai/stable-diffusion-2-base', use_ema=True, wandb_name=None, wandb_project=None, device_train_microbatch_size=4)
wandb: Tracking run with wandb version 0.13.11
wandb: W&B syncing is set to `offline` in this directory.
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: WARNING URL not available in offline run
******************************
Config:
enabled_algorithms/EMA: true
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 8
num_nodes: 1
rank_zero_seed: 3179589898

******************************
train          Epoch   0:   38%|█████████▍               | 18/48 [03:28<05:29, 10.99s/ba, loss/train/total=0.1463]                      
```

To see GPU utilization, start another shell on the EC2 instance and run either `watch nvidia-smi` or `nvidia-smi -l 2` if you get a segmentation error which happens frequently if you launched the EC2 instance with a DLAMI. You can also run nvidia-smi in docker as:

```bash
docker run --rm -it --gpus all nvidia/cuda:12.2.0-devel-ubuntu20.04 watch nvidia-smi
```

### 1.5 Single Node Training Benchmarks

We ran tests on P4de (A100 80GB) and P5 (H100 80GB) machines and here is a comparison.

<center><img src="single-node/sd_p4de_p5.png" width="80%"/> </br>
</center>

|Micro BS|num_workers| EMA | P4de Throughput| P5 Throughput | Ratio  |
|:------:|:---------:|:---:| :------------: | :-----------: |:------:|
|    4   |     8     | Yes |      142.69    |     189.28    |  1.32x |
|    8   |     8     | Yes |      173.89    |     289.19    |  1.66x |
|   12   |     8     | Yes |      185.18    |     329.55    |  1.78x |
|   16   |     8     | Yes |      192.85    |     353.81    |  1.83x |
|   20   |     8     | Yes |      197.47    |     358.68    |  1.82x |
|   20   |     8     | No  |      197.90    |     361.15    |  1.83x |
|   32   |     8     | Yes |      204.22    |     385.31    |  1.89x |

### 1.6 Scaled Dot Product Attention

HuggingFace Diffusers has a set_attn_processor method that you can use to plug and play different attention processors. A list of attention processors can be found [here](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)

You can try a different Attention Processor like below:

```
from diffusers.models.attention_processor import AttnProcessor

self.unet = UNet2DConditionModel.from_pretrained(''stabilityai/stable-diffusion-2-base'', subfolder='unet')
self.unet.set_attn_processor(AttnProcessor())
```
`AttnProcessor2_0` which is a Processor for implementing scaled dot-product attention is enabled by default if you're using PyTorch 2.0.

The older `self.unet.set_attn_processor(AttnProcessor())` gives [Cuda OOM](https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2) error with a batch size of 32 while with `AttnProcessor2_0()` is able to run with a batch size of 32 and yield 385 images/sec throughput 

More details on this can be found here: https://pytorch.org/blog/accelerated-diffusers-pt-20/

## 2. Multi Node Tests

### 2.1 Multi-Node Training

For the multi-node training we've created a [Dockerfile](https://github.com/aws-samples/awsome-distributed-training/blob/multi-node/3.test_cases/6.stable-diffusion/multi-node/1.Dockerfile), and Slurm submit script to submit the training job. To get started please follow the guide [AWS ParallelCluster Distributed Training](../../1.architectures/2.aws-parallelcluster). Before starting this section make sure you have the following setup:

* AWS ParallelCluster >= 3.7.0
* Pyxis
* Enroot
* FSx Lustre Filesystem

1. To get started, clone this repo and cd into the multi-node directory:

```
git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/6.stable-diffusion/multi-node
```

#### 2.1.1 Next build the docker image:

```
docker build --build-arg MOSAICML_VERSION=${MOSAICML_VERSION} --build-arg PYTORCH_IMAGE=${PYTORCH_IMAGE} -t ${DOCKER_IMAGE_NAME}:${TAG} -f 1.Dockerfile .
```

#### 2.1.2 Convert Docker image to Squash file

Convert the Docker container image to an [Enroot](https://github.com/NVIDIA/enroot) squash file that will be stored in /apps. This step takes a few minutes.

```
enroot import -o /apps/${DOCKER_IMAGE_NAME}.sqsh dockerd://${DOCKER_IMAGE_NAME}:${TAG}
```

#### 2.1.3 Now we can start training

```
sbatch 2.train.sbatch
```
```bash
Namespace(batch_size=2048, image_size=512, remote=None, local='/tmp/mds-cache/mds-laion-2/', use_synth_data=True, model_name='stabilityai/stable-diffusion-2-base', use_ema=True, wandb_name=None, wandb_project=None, device_train_microbatch_size=4)
wandb: Tracking run with wandb version 0.13.11
wandb: W&B syncing is set to `offline` in this directory.
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: WARNING URL not available in offline run
******************************
Config:
enabled_algorithms/EMA: true
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 8
num_nodes: 2
rank_zero_seed: 3179589898

******************************
rain          Epoch   0:  100%|█████████████████████████| 48/48 [09:57<00:00, 12.45s/ba, loss/train/total=0.1521]
                      
```

