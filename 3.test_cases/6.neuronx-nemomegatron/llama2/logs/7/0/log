[NeMo W 2023-09-01 22:58:21 optimizers:67] Could not import distributed_fused_adam optimizer from Apex
[NeMo W 2023-09-01 22:58:22 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-09-01 22:58:22 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-09-01 22:58:23 nemo_logging:349] /home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2023-09-01 22:58:23 megatron_gpt_pretraining:57] 
    
    ************** Experiment configuration ***********
[NeMo I 2023-09-01 22:58:23 megatron_gpt_pretraining:58] 
    name: megatron_llama
    restore_from_path: null
    trainer:
      devices: 32
      num_nodes: 2
      accelerator: tpu
      precision: 32
      logger: false
      enable_checkpointing: false
      replace_sampler_ddp: false
      max_epochs: null
      max_steps: 3
      log_every_n_steps: 1
      val_check_interval: 3
      limit_val_batches: 1
      limit_test_batches: 1
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      benchmark: false
      enable_model_summary: false
    exp_manager:
      create_tensorboard_logger: false
      explicit_log_dir: null
      exp_dir: null
      name: megatron_llama
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: null
        name: null
      resume_if_exists: false
      resume_ignore_no_checkpoint: false
      create_checkpoint_callback: false
      checkpoint_callback_params:
        monitor: step
        save_top_k: -1
        mode: max
        save_last: false
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_llama--{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
        train_time_interval: 36000
    model:
      micro_batch_size: 1
      global_batch_size: 256
      tensor_model_parallel_size: 8
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      encoder_seq_length: 4096
      max_position_embeddings: 4096
      num_layers: 32
      hidden_size: 4096
      ffn_hidden_size: 11008
      num_attention_heads: 32
      init_method_std: 0.021
      use_scaled_init_method: true
      hidden_dropout: 0
      attention_dropout: 0
      ffn_dropout: 0
      kv_channels: null
      apply_query_key_layer_scaling: true
      normalization: rmsnorm
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 8
      pre_process: true
      post_process: true
      persist_layer_norm: true
      share_embeddings_and_output_weights: false
      position_embedding_type: rope
      rotary_percentage: 1
      activation: swiglu
      transformer_block_type: pre_ln
      has_bias: false
      tokenizer:
        library: huggingface
        type: /fsx/Llama2-7b-hf
        model: null
        vocab_file: null
        merge_file: null
        delimiter: null
        sentencepiece_legacy: false
        use_fast: false
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: false
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: false
      gradient_accumulation_fusion: false
      bias_activation_fusion: false
      bias_dropout_add_fusion: false
      masked_softmax_fusion: false
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: true
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: true
      sync_batch_comm: false
      log_parameter_norm: true
      log_gradient_norm: true
      activations_checkpoint_granularity: full
      activations_checkpoint_method: uniform
      activations_checkpoint_num_layers: 1
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: true
      wrap_with_zero: false
      transformer_engine: false
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: false
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1
      fp8_amax_compute_algo: most_recent
      use_emha: false
      data:
        data_prefix:
        - 1.0
        - /fsx/data/llama2/book/book-tokenized_text_document.idx
        index_mapping_dir: null
        data_impl: mmap
        splits_string: 980,10,10
        seq_length: 4096
        skip_warmup: true
        num_workers: 1
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        validation_drop_last: true
        no_seqlen_plus_one_input_tokens: false
        pad_samples_to_global_batch_size: false
      nsys_profile:
        enabled: false
        start_step: 10
        end_step: 10
        ranks:
        - 0
        gen_shape: false
      optim:
        name: adamw
        lr: 0.0003
        weight_decay: 0.1
        capturable: true
        betas:
        - 0.9
        - 0.95
        sched:
          name: CosineAnnealing
          warmup_steps: 10
          constant_steps: 0
          min_lr: 3.0e-05
      save_xser: false
    
[NeMo I 2023-09-01 22:58:23 nlp_overrides:436] NLPTrainer: Initializing trainer with parameters: {'self': <nemo.collections.nlp.parts.nlp_overrides.NLPTrainer object at 0x7f36efa71490>, 'logger': False, 'enable_checkpointing': False, 'callbacks': None, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 2, 'num_processes': None, 'devices': 32, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': 1, 'max_epochs': None, 'min_epochs': None, 'max_steps': 3, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': 1, 'limit_test_batches': 1, 'limit_predict_batches': None, 'val_check_interval': 3, 'log_every_n_steps': 1, 'accelerator': 'tpu', 'strategy': <nemo.collections.nlp.parts.nlp_overrides.NLPDDPStrategy object at 0x7f36efa93430>, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': False, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': [], 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True}
GPU available: True (cuda), used: False
TPU available: True, using: 32 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[NeMo W 2023-09-01 22:58:23 nemo_logging:349] /home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
      rank_zero_warn(
    
`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.
`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.
[NeMo E 2023-09-01 22:58:23 exp_manager:460] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
[NeMo E 2023-09-01 22:58:23 exp_manager:465] You are running multi-gpu without ddp.Please note that this is not tested in NeMo and could result in errors.
[NeMo I 2023-09-01 22:58:23 exp_manager:350] Experiments will be logged at /home/ec2-user/reference-architectures/3.test_cases/6.neuronx-nemomegatron/llama2/nemo_experiments/megatron_llama/7
[NeMo W 2023-09-01 22:58:23 nemo_logging:349] /home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/nemo/utils/exp_manager.py:1035: UserWarning: Detected custom epoch loop. Skipping no validation on restart support.
      warnings.warn("Detected custom epoch loop. Skipping no validation on restart support.", UserWarning)
    
[NeMo I 2023-09-01 22:58:24 megatron_init:252] Rank 0 has data parallel group: [0, 8, 16, 24, 32, 40, 48, 56]
[NeMo I 2023-09-01 22:58:24 megatron_init:255] All data parallel group ranks: [[0, 8, 16, 24, 32, 40, 48, 56], [1, 9, 17, 25, 33, 41, 49, 57], [2, 10, 18, 26, 34, 42, 50, 58], [3, 11, 19, 27, 35, 43, 51, 59], [4, 12, 20, 28, 36, 44, 52, 60], [5, 13, 21, 29, 37, 45, 53, 61], [6, 14, 22, 30, 38, 46, 54, 62], [7, 15, 23, 31, 39, 47, 55, 63]]
[NeMo I 2023-09-01 22:58:24 megatron_init:256] Ranks 0 has data parallel rank: 0
[NeMo I 2023-09-01 22:58:24 megatron_init:264] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2023-09-01 22:58:24 megatron_init:265] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63]]
[NeMo I 2023-09-01 22:58:24 megatron_init:275] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2023-09-01 22:58:24 megatron_init:279] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63]]
[NeMo I 2023-09-01 22:58:24 megatron_init:280] Rank 0 has tensor model parallel rank: 0
[NeMo I 2023-09-01 22:58:24 megatron_init:294] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2023-09-01 22:58:24 megatron_init:306] Rank 0 has embedding group: [0]
[NeMo I 2023-09-01 22:58:24 megatron_init:312] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[NeMo I 2023-09-01 22:58:24 megatron_init:313] Rank 0 has pipeline model parallel rank 0
[NeMo I 2023-09-01 22:58:24 megatron_init:314] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[NeMo I 2023-09-01 22:58:24 megatron_init:315] Rank 0 has embedding rank: 0
23-09-01 22:58:24 - PID:73473 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32
setup_microbatch_calculator 0 None 256 1 8
[NeMo I 2023-09-01 22:58:24 tokenizer_utils:182] Getting HuggingFace AutoTokenizer with pretrained_model_name: /fsx/Llama2-7b-hf
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Using sep_token, but it is not set yet.
Using cls_token, but it is not set yet.
Using pad_token, but it is not set yet.
Using mask_token, but it is not set yet.
[NeMo I 2023-09-01 22:58:27 megatron_base_model:275] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.
Error executing job with overrides: ['trainer.devices=32', 'trainer.num_nodes=2', 'trainer.max_epochs=null', 'trainer.max_steps=3', 'trainer.val_check_interval=3', 'trainer.log_every_n_steps=1', 'trainer.limit_val_batches=1', 'trainer.limit_test_batches=1', 'trainer.accumulate_grad_batches=1', 'trainer.precision=32', 'model.tokenizer.type=/fsx/Llama2-7b-hf', 'model.micro_batch_size=1', 'model.global_batch_size=256', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.max_position_embeddings=4096', 'model.encoder_seq_length=4096', 'model.hidden_size=4096', 'model.ffn_hidden_size=11008', 'model.num_layers=32', 'model.num_attention_heads=32', 'model.init_method_std=0.021', 'model.hidden_dropout=0', 'model.layernorm_epsilon=1e-5', 'model.data.data_prefix=[1.0,/fsx/data/llama2/book/book-tokenized_text_document.idx]', 'model.data.num_workers=1', 'model.data.seq_length=4096', 'model.optim.name=adamw', 'model.optim.lr=3.0e-4', 'model.optim.betas=[0.9,0.95]', 'model.optim.weight_decay=0.1', 'model.optim.sched.name=CosineAnnealing', 'model.optim.sched.warmup_steps=10', 'model.optim.sched.constant_steps=0', 'model.optim.sched.min_lr=3.0e-5', 'model.optim.capturable=True', 'model.sequence_parallel=True', 'model.activations_checkpoint_granularity=full', 'model.activations_checkpoint_method=uniform', 'model.activations_checkpoint_num_layers=1', '+model.save_xser=False', 'exp_manager.create_tensorboard_logger=False', 'exp_manager.resume_if_exists=False', 'exp_manager.resume_ignore_no_checkpoint=False', 'exp_manager.create_checkpoint_callback=False', '+exp_manager.checkpoint_callback_params.train_time_interval=36000', 'exp_manager.checkpoint_callback_params.save_last=False', 'model.use_cpu_initialization=True']
Traceback (most recent call last):
  File "/home/ec2-user/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py", line 117, in <module>
    main()
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/nemo/core/config/hydra_runner.py", line 105, in wrapper
    _run_hydra(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/ec2-user/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py", line 114, in main
    trainer.fit(model)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 603, in fit
    call._call_and_handle_interrupt(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 36, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/nemo/collections/nlp/parts/nlp_overrides.py", line 112, in launch
    xmp.spawn(
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 389, in spawn
    pf_cfg = _pre_fork_setup(nprocs)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/torch_xla/distributed/xla_multiprocessing.py", line 201, in _pre_fork_setup
    raise ValueError(
ValueError: The number of devices must be either 1 or 8, got 32 instead
