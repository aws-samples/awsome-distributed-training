[NeMo W 2023-09-01 23:04:11 optimizers:67] Could not import distributed_fused_adam optimizer from Apex
[NeMo W 2023-09-01 23:04:13 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-09-01 23:04:14 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-09-01 23:04:15 nemo_logging:349] /home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2023-09-01 23:04:15 megatron_gpt_pretraining:57] 
    
    ************** Experiment configuration ***********
[NeMo I 2023-09-01 23:04:15 megatron_gpt_pretraining:58] 
    name: megatron_llama
    restore_from_path: null
    trainer:
      devices: 32
      num_nodes: 2
      accelerator: tpu
      precision: 32
      logger: false
      enable_checkpointing: false
      replace_sampler_ddp: false
      max_epochs: null
      max_steps: 3
      log_every_n_steps: 1
      val_check_interval: 3
      limit_val_batches: 1
      limit_test_batches: 1
      accumulate_grad_batches: 1
      gradient_clip_val: 1.0
      benchmark: false
      enable_model_summary: false
    exp_manager:
      create_tensorboard_logger: false
      explicit_log_dir: null
      exp_dir: null
      name: megatron_llama
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: null
        name: null
      resume_if_exists: false
      resume_ignore_no_checkpoint: false
      create_checkpoint_callback: false
      checkpoint_callback_params:
        monitor: step
        save_top_k: -1
        mode: max
        save_last: false
        always_save_nemo: false
        save_nemo_on_train_end: false
        filename: megatron_llama--{step}-{consumed_samples}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
        train_time_interval: 36000
    model:
      micro_batch_size: 1
      global_batch_size: 256
      tensor_model_parallel_size: 8
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      encoder_seq_length: 4096
      max_position_embeddings: 4096
      num_layers: 32
      hidden_size: 4096
      ffn_hidden_size: 11008
      num_attention_heads: 32
      init_method_std: 0.021
      use_scaled_init_method: true
      hidden_dropout: 0
      attention_dropout: 0
      ffn_dropout: 0
      kv_channels: null
      apply_query_key_layer_scaling: true
      normalization: rmsnorm
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 8
      pre_process: true
      post_process: true
      persist_layer_norm: true
      share_embeddings_and_output_weights: false
      position_embedding_type: rope
      rotary_percentage: 1
      activation: swiglu
      transformer_block_type: pre_ln
      has_bias: false
      tokenizer:
        library: huggingface
        type: /fsx/Llama2-7b-hf
        model: null
        vocab_file: null
        merge_file: null
        delimiter: null
        sentencepiece_legacy: false
        use_fast: false
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: false
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: false
      gradient_accumulation_fusion: false
      bias_activation_fusion: false
      bias_dropout_add_fusion: false
      masked_softmax_fusion: false
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: true
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: true
      sync_batch_comm: false
      log_parameter_norm: true
      log_gradient_norm: true
      activations_checkpoint_granularity: full
      activations_checkpoint_method: uniform
      activations_checkpoint_num_layers: 1
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: true
      wrap_with_zero: false
      transformer_engine: false
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: false
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1
      fp8_amax_compute_algo: most_recent
      use_emha: false
      data:
        data_prefix:
        - 1.0
        - /fsx/data/llama2/book/book-tokenized_text_document.idx
        index_mapping_dir: null
        data_impl: mmap
        splits_string: 980,10,10
        seq_length: 4096
        skip_warmup: true
        num_workers: 1
        dataloader_type: single
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        validation_drop_last: true
        no_seqlen_plus_one_input_tokens: false
        pad_samples_to_global_batch_size: false
      nsys_profile:
        enabled: false
        start_step: 10
        end_step: 10
        ranks:
        - 0
        gen_shape: false
      optim:
        name: adamw
        lr: 0.0003
        weight_decay: 0.1
        capturable: true
        betas:
        - 0.9
        - 0.95
        sched:
          name: CosineAnnealing
          warmup_steps: 10
          constant_steps: 0
          min_lr: 3.0e-05
      save_xser: false
    
[NeMo I 2023-09-01 23:04:15 nlp_overrides:436] NLPTrainer: Initializing trainer with parameters: {'self': <nemo.collections.nlp.parts.nlp_overrides.NLPTrainer object at 0x7efb9d4ee0d0>, 'logger': False, 'enable_checkpointing': False, 'callbacks': None, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 2, 'num_processes': None, 'devices': 32, 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': 1, 'max_epochs': None, 'min_epochs': None, 'max_steps': 3, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': 1, 'limit_test_batches': 1, 'limit_predict_batches': None, 'val_check_interval': 3, 'log_every_n_steps': 1, 'accelerator': 'tpu', 'strategy': <nemo.collections.nlp.parts.nlp_overrides.NLPDDPStrategy object at 0x7efb9d4c4910>, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': False, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': False, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': [], 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True}
Error executing job with overrides: ['trainer.devices=32', 'trainer.num_nodes=2', 'trainer.max_epochs=null', 'trainer.max_steps=3', 'trainer.val_check_interval=3', 'trainer.log_every_n_steps=1', 'trainer.limit_val_batches=1', 'trainer.limit_test_batches=1', 'trainer.accumulate_grad_batches=1', 'trainer.precision=32', 'model.tokenizer.type=/fsx/Llama2-7b-hf', 'model.micro_batch_size=1', 'model.global_batch_size=256', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.max_position_embeddings=4096', 'model.encoder_seq_length=4096', 'model.hidden_size=4096', 'model.ffn_hidden_size=11008', 'model.num_layers=32', 'model.num_attention_heads=32', 'model.init_method_std=0.021', 'model.hidden_dropout=0', 'model.layernorm_epsilon=1e-5', 'model.data.data_prefix=[1.0,/fsx/data/llama2/book/book-tokenized_text_document.idx]', 'model.data.num_workers=1', 'model.data.seq_length=4096', 'model.optim.name=adamw', 'model.optim.lr=3.0e-4', 'model.optim.betas=[0.9,0.95]', 'model.optim.weight_decay=0.1', 'model.optim.sched.name=CosineAnnealing', 'model.optim.sched.warmup_steps=10', 'model.optim.sched.constant_steps=0', 'model.optim.sched.min_lr=3.0e-5', 'model.optim.capturable=True', 'model.sequence_parallel=True', 'model.activations_checkpoint_granularity=full', 'model.activations_checkpoint_method=uniform', 'model.activations_checkpoint_num_layers=1', '+model.save_xser=False', 'exp_manager.create_tensorboard_logger=False', 'exp_manager.resume_if_exists=False', 'exp_manager.resume_ignore_no_checkpoint=False', 'exp_manager.create_checkpoint_callback=False', '+exp_manager.checkpoint_callback_params.train_time_interval=36000', 'exp_manager.checkpoint_callback_params.save_last=False', 'model.use_cpu_initialization=True']
Traceback (most recent call last):
  File "/home/ec2-user/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py", line 97, in main
    trainer = NLPTrainer(plugins=plugins, strategy=strategy, num_sanity_val_steps=0, **cfg.trainer)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/utilities/argparse.py", line 340, in insert_env_defaults
    return fn(self, **kwargs)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/nemo/collections/nlp/parts/nlp_overrides.py", line 538, in __init__
    self._setup_on_init()
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 540, in _setup_on_init
    setup._log_device_info(self)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/trainer/setup.py", line 151, in _log_device_info
    if CUDAAccelerator.is_available():
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py", line 95, in is_available
    return num_cuda_devices() > 0
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/lightning_lite/accelerators/cuda.py", line 107, in num_cuda_devices
    return torch.cuda.device_count()
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/nemo/collections/nlp/modules/common/megatron/megatron_init.py", line 55, in <lambda>
    torch.cuda.device_count = lambda : xm.xrt_world_size()
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/torch_xla/core/xla_model.py", line 166, in xrt_world_size
    return xu.getenv_as(xenv.WORLD_SIZE, int, defval=defval)
  File "/home/ec2-user/aws_neuron_venv_pytorch/lib64/python3.8/site-packages/torch_xla/utils/utils.py", line 139, in getenv_as
    return defval if env is None else type(env)
ValueError: invalid literal for int() with base 10: ''

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
