diff -urN neuronx-nemo-megatron/.git/HEAD nemo-updates/.git/HEAD
--- neuronx-nemo-megatron/.git/HEAD	2023-08-24 16:55:40
+++ nemo-updates/.git/HEAD	2023-08-16 13:00:47
@@ -1 +1 @@
-ref: refs/heads/main
+ref: refs/heads/head
diff -urN neuronx-nemo-megatron/.git/config nemo-updates/.git/config
--- neuronx-nemo-megatron/.git/config	2023-08-24 16:55:40
+++ nemo-updates/.git/config	2023-08-16 13:00:47
@@ -6,8 +6,8 @@
 	ignorecase = true
 	precomposeunicode = true
 [remote "origin"]
-	url = https://github.com/aws-neuron/neuronx-nemo-megatron.git
+	url = ssh://git.amazon.com/pkg/Neuron-Nemo-Megatron/snapshot/aroparas/2023-08-15T18-05-45
 	fetch = +refs/heads/*:refs/remotes/origin/*
-[branch "main"]
+[branch "head"]
 	remote = origin
-	merge = refs/heads/main
+	merge = refs/heads/head
Binary files neuronx-nemo-megatron/.git/index and nemo-updates/.git/index differ
diff -urN neuronx-nemo-megatron/.git/logs/HEAD nemo-updates/.git/logs/HEAD
--- neuronx-nemo-megatron/.git/logs/HEAD	2023-08-24 16:55:40
+++ nemo-updates/.git/logs/HEAD	2023-08-16 13:00:47
@@ -1 +1 @@
-0000000000000000000000000000000000000000 48da8795a42b0355ad0c3fc8a360349053a53768 Hiroshi Tokoyo <htokoyo@amazon.com> 1692863740 +0900	clone: from https://github.com/aws-neuron/neuronx-nemo-megatron.git
+0000000000000000000000000000000000000000 ad952f23646fb526ed771801af92763a97e36f89 Hiroshi Tokoyo <htokoyo@amazon.com> 1692158447 +0900	clone: from ssh://git.amazon.com/pkg/Neuron-Nemo-Megatron/snapshot/aroparas/2023-08-15T18-05-45
diff -urN neuronx-nemo-megatron/.git/logs/refs/heads/head nemo-updates/.git/logs/refs/heads/head
--- neuronx-nemo-megatron/.git/logs/refs/heads/head	1970-01-01 09:00:00
+++ nemo-updates/.git/logs/refs/heads/head	2023-08-16 13:00:47
@@ -0,0 +1 @@
+0000000000000000000000000000000000000000 ad952f23646fb526ed771801af92763a97e36f89 Hiroshi Tokoyo <htokoyo@amazon.com> 1692158447 +0900	clone: from ssh://git.amazon.com/pkg/Neuron-Nemo-Megatron/snapshot/aroparas/2023-08-15T18-05-45
diff -urN neuronx-nemo-megatron/.git/logs/refs/heads/main nemo-updates/.git/logs/refs/heads/main
--- neuronx-nemo-megatron/.git/logs/refs/heads/main	2023-08-24 16:55:40
+++ nemo-updates/.git/logs/refs/heads/main	1970-01-01 09:00:00
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 48da8795a42b0355ad0c3fc8a360349053a53768 Hiroshi Tokoyo <htokoyo@amazon.com> 1692863740 +0900	clone: from https://github.com/aws-neuron/neuronx-nemo-megatron.git
diff -urN neuronx-nemo-megatron/.git/logs/refs/remotes/origin/HEAD nemo-updates/.git/logs/refs/remotes/origin/HEAD
--- neuronx-nemo-megatron/.git/logs/refs/remotes/origin/HEAD	2023-08-24 16:55:40
+++ nemo-updates/.git/logs/refs/remotes/origin/HEAD	1970-01-01 09:00:00
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 48da8795a42b0355ad0c3fc8a360349053a53768 Hiroshi Tokoyo <htokoyo@amazon.com> 1692863740 +0900	clone: from https://github.com/aws-neuron/neuronx-nemo-megatron.git
Binary files neuronx-nemo-megatron/.git/objects/pack/pack-6d4d9acaa46fc060b91fdab68f623d951fd02ca1.idx and nemo-updates/.git/objects/pack/pack-6d4d9acaa46fc060b91fdab68f623d951fd02ca1.idx differ
Binary files neuronx-nemo-megatron/.git/objects/pack/pack-6d4d9acaa46fc060b91fdab68f623d951fd02ca1.pack and nemo-updates/.git/objects/pack/pack-6d4d9acaa46fc060b91fdab68f623d951fd02ca1.pack differ
Binary files neuronx-nemo-megatron/.git/objects/pack/pack-daadcb950221d96c06289823a378343ca1a676c8.idx and nemo-updates/.git/objects/pack/pack-daadcb950221d96c06289823a378343ca1a676c8.idx differ
Binary files neuronx-nemo-megatron/.git/objects/pack/pack-daadcb950221d96c06289823a378343ca1a676c8.pack and nemo-updates/.git/objects/pack/pack-daadcb950221d96c06289823a378343ca1a676c8.pack differ
diff -urN neuronx-nemo-megatron/.git/packed-refs nemo-updates/.git/packed-refs
--- neuronx-nemo-megatron/.git/packed-refs	2023-08-24 16:55:40
+++ nemo-updates/.git/packed-refs	2023-08-16 13:00:47
@@ -1,5 +1,2 @@
 # pack-refs with: peeled fully-peeled sorted 
-48da8795a42b0355ad0c3fc8a360349053a53768 refs/remotes/origin/main
-c82d4798f652053e19771f735ea56567ce77db59 refs/remotes/origin/nemo_2.13_release
-4813f8b35b9df6153ca440199f899cf370512bb0 refs/remotes/origin/perrysc_initial_release
-d5334b6ffa58a67ec6bea4d255121445aa225e10 refs/remotes/origin/perrysc_readme_update
+ad952f23646fb526ed771801af92763a97e36f89 refs/remotes/origin/head
diff -urN neuronx-nemo-megatron/.git/refs/heads/head nemo-updates/.git/refs/heads/head
--- neuronx-nemo-megatron/.git/refs/heads/head	1970-01-01 09:00:00
+++ nemo-updates/.git/refs/heads/head	2023-08-16 13:00:47
@@ -0,0 +1 @@
+ad952f23646fb526ed771801af92763a97e36f89
diff -urN neuronx-nemo-megatron/.git/refs/heads/main nemo-updates/.git/refs/heads/main
--- neuronx-nemo-megatron/.git/refs/heads/main	2023-08-24 16:55:40
+++ nemo-updates/.git/refs/heads/main	1970-01-01 09:00:00
@@ -1 +0,0 @@
-48da8795a42b0355ad0c3fc8a360349053a53768
diff -urN neuronx-nemo-megatron/.git/refs/remotes/origin/HEAD nemo-updates/.git/refs/remotes/origin/HEAD
--- neuronx-nemo-megatron/.git/refs/remotes/origin/HEAD	2023-08-24 16:55:40
+++ nemo-updates/.git/refs/remotes/origin/HEAD	1970-01-01 09:00:00
@@ -1 +0,0 @@
-ref: refs/remotes/origin/main
diff -urN neuronx-nemo-megatron/CODEOWNERS nemo-updates/CODEOWNERS
--- neuronx-nemo-megatron/CODEOWNERS	2023-08-24 16:55:40
+++ nemo-updates/CODEOWNERS	1970-01-01 09:00:00
@@ -1,11 +0,0 @@
-# This file creates codeowners for the documentation. It will allow setting code reviewers for all Pull requests to merge to the master branch 
-# Each line is a file pattern followed by one or more owners.
-
-# Refernce guide - https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-code-owners#example-[â€¦]ners-file
-# Example - These owners will be the default owners for everything in
-# the repo. Unless a later match takes precedence,
-# @global-owner1 and @global-owner2 will be requested for
-# review when someone opens a pull request.
-# *       @global-owner1 @global-owner2
-
-*       @aws-maens @musunita
diff -urN neuronx-nemo-megatron/CODE_OF_CONDUCT.md nemo-updates/CODE_OF_CONDUCT.md
--- neuronx-nemo-megatron/CODE_OF_CONDUCT.md	2023-08-24 16:55:40
+++ nemo-updates/CODE_OF_CONDUCT.md	2023-08-16 13:00:47
@@ -1,5 +1,4 @@
 ## Code of Conduct
 This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).
 For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact
-opensource-codeofconduct@amazon.com with any additional questions or comments.
-
+opensource-codeofconduct@amazon.com with any additional questions or comments.
\ No newline at end of file
diff -urN neuronx-nemo-megatron/CONTRIBUTING.md nemo-updates/CONTRIBUTING.md
--- neuronx-nemo-megatron/CONTRIBUTING.md	2023-08-24 16:55:40
+++ nemo-updates/CONTRIBUTING.md	2023-08-16 13:00:47
@@ -56,4 +56,4 @@
 
 ## Licensing
 
-See the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.
+See the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.
\ No newline at end of file
diff -urN neuronx-nemo-megatron/README.md nemo-updates/README.md
--- neuronx-nemo-megatron/README.md	2023-08-24 16:55:40
+++ nemo-updates/README.md	2023-08-16 13:00:47
@@ -1,3 +1,3 @@
 This project "AWS Neuron Reference for NeMo Megatron" includes modified versions of the open-source packages [NeMo](https://github.com/NVIDIA/NeMo) and [Apex](https://github.com/NVIDIA/apex) that have been adapted for use with AWS Neuron and AWS EC2 Trn1 instances.
 
-Please refer to the [neuronx-nemo-megatron GPT-3 pretraining tutorial](https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-gpt-job.md) for instructions on how to use the code in this repository.
+Please refer to the [neuronx-nemo-megatron GPT-3 pretraining tutorial](https://github.com/aws-neuron/aws-neuron-parallelcluster-samples/blob/master/examples/jobs/neuronx-nemo-megatron-gpt-job.md) for instructions on how to use the code in this repository.
\ No newline at end of file
diff -urN neuronx-nemo-megatron/apex/apex/amp/_initialize.py nemo-updates/apex/apex/amp/_initialize.py
--- neuronx-nemo-megatron/apex/apex/amp/_initialize.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/amp/_initialize.py	2023-08-16 13:00:47
@@ -1,5 +1,8 @@
 import torch
-from torch._six import string_classes
+if torch.__version__.startswith('2'):
+    string_classes = str
+else:
+    from torch._six import string_classes
 import functools
 import numpy as np
 import sys
diff -urN neuronx-nemo-megatron/apex/apex/contrib/clip_grad/clip_grad.py nemo-updates/apex/apex/contrib/clip_grad/clip_grad.py
--- neuronx-nemo-megatron/apex/apex/contrib/clip_grad/clip_grad.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/contrib/clip_grad/clip_grad.py	2023-08-16 13:00:47
@@ -1,5 +1,9 @@
 import torch
-from torch._six import inf
+
+if torch.__version__.startswith('2'):
+    from torch import inf
+else:
+    from torch._six import inf
 from typing import Union, Iterable
 
 _kernel_import_succeeded = False
diff -urN neuronx-nemo-megatron/apex/apex/normalization/__init__.py nemo-updates/apex/apex/normalization/__init__.py
--- neuronx-nemo-megatron/apex/apex/normalization/__init__.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/normalization/__init__.py	2023-08-16 13:00:47
@@ -1,2 +1,2 @@
-from .fused_layer_norm import FusedLayerNorm, MixedFusedLayerNorm, FusedRMSNorm, MixedFusedRMSNorm
+from .fused_layer_norm import FusedLayerNorm, MixedFusedLayerNorm, FusedRMSNorm, MixedFusedRMSNorm, FusedRMSNormXLA
 from .instance_norm import InstanceNorm3dNVFuser
diff -urN neuronx-nemo-megatron/apex/apex/normalization/fused_layer_norm.py nemo-updates/apex/apex/normalization/fused_layer_norm.py
--- neuronx-nemo-megatron/apex/apex/normalization/fused_layer_norm.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/normalization/fused_layer_norm.py	2023-08-16 13:00:47
@@ -164,7 +164,27 @@
         )
         return grad_input, None, None
 
+class FusedRMSNormFunctionXLA(torch.autograd.Function):
+    @staticmethod
+    def forward(ctx, input, normalized_shape, eps):
+        global fused_layer_norm_cuda
+        ctx.normalized_shape = normalized_shape
+        ctx.eps = eps
+        input_ = input.contiguous()
+        output, invvar = fused_layer_norm_cuda.rms_forward(input_, ctx.normalized_shape, ctx.eps)
+        ctx.save_for_backward(input_, invvar)
+        return output
 
+    @staticmethod
+    def backward(ctx, grad_output):
+        input_, invvar = ctx.saved_tensors
+        grad_input = None
+        grad_input = fused_layer_norm_cuda.rms_backward(
+            grad_output.contiguous(), invvar, input_, ctx.normalized_shape, ctx.eps
+        )
+        return grad_input, None, None
+
+
 def fused_layer_norm_affine(input, weight, bias, normalized_shape, eps=1e-6):
     args = _cast_if_autocast_enabled(input, weight, bias, normalized_shape, eps)
     with torch.cuda.amp.autocast(enabled=False):
@@ -194,6 +214,10 @@
     with torch.cuda.amp.autocast(enabled=False):
         return FusedRMSNormFunction.apply(*args)
 
+def fused_rms_norm_xla(input, normalized_shape, eps=1e-6):
+    args = _cast_if_autocast_enabled(input, normalized_shape, eps)
+    with torch.cuda.amp.autocast(enabled=False):
+        return FusedRMSNormFunctionXLA.apply(*args)
 
 def mixed_dtype_fused_rms_norm_affine(input, weight, normalized_shape, eps=1e-6):
     args = _cast_if_autocast_enabled(input, weight, normalized_shape, eps)
@@ -297,6 +321,40 @@
         return "{normalized_shape}, eps={eps}, " "elementwise_affine={elementwise_affine}".format(**self.__dict__)
 
 
+class FusedRMSNormXLA(torch.nn.Module):
+    r"""Overwrite FusedLayerNorm for XLA
+    """
+
+    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
+        super().__init__()
+
+        if isinstance(normalized_shape, numbers.Integral):
+            normalized_shape = (normalized_shape,)
+        self.normalized_shape = torch.Size(normalized_shape)
+        self.eps = eps
+        self.elementwise_affine = elementwise_affine
+        if self.elementwise_affine:
+            self.weight = Parameter(torch.Tensor(*normalized_shape))
+        else:
+            self.register_parameter("weight", None)
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        if self.elementwise_affine:
+            init.ones_(self.weight)
+
+    def forward(self, input):
+        if not input.is_cuda:
+            return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)
+
+        if self.elementwise_affine:
+            return fused_rms_norm_affine(input, self.weight, self.normalized_shape, self.eps)
+        else:
+            return fused_rms_norm_xla(input, self.normalized_shape, self.eps)
+
+    def extra_repr(self):
+        return "{normalized_shape}, eps={eps}, " "elementwise_affine={elementwise_affine}".format(**self.__dict__)
+    
 class FusedRMSNorm(torch.nn.Module):
     r"""Applies RMS Normalization over a mini-batch of inputs
 
diff -urN neuronx-nemo-megatron/apex/apex/transformer/layers/__init__.py nemo-updates/apex/apex/transformer/layers/__init__.py
--- neuronx-nemo-megatron/apex/apex/transformer/layers/__init__.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/transformer/layers/__init__.py	2023-08-16 13:00:47
@@ -2,10 +2,13 @@
 from apex.transformer.layers.layer_norm import FastLayerNorm
 from apex.transformer.layers.layer_norm import FusedLayerNorm
 from apex.transformer.layers.layer_norm import MixedFusedLayerNorm
+from apex.transformer.layers.layer_norm import FastRMSNorm
 
 
+
 __all__ = [
     "FastLayerNorm",
     "FusedLayerNorm",
     "MixedFusedLayerNorm",
+    "FastRMSNorm"
 ]
diff -urN neuronx-nemo-megatron/apex/apex/transformer/layers/layer_norm.py nemo-updates/apex/apex/transformer/layers/layer_norm.py
--- neuronx-nemo-megatron/apex/apex/transformer/layers/layer_norm.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/transformer/layers/layer_norm.py	2023-08-16 13:00:47
@@ -2,11 +2,12 @@
 # NOTE(mkozuki): This file defines two LayerNorm that are compatible with Megatron-LM.
 # while avoiding introducing the breaking change of `"sequence_parallel_enabled"` attribute into apex.normalization.FusedLayerNorm
 # and apex.contrib.layer_norm.FastLayerNorm.
-import warnings
 
 import torch
 
 from apex.normalization import FusedLayerNorm as OrigFusedLayerNorm
+from apex.normalization import FusedRMSNormXLA as OrigFusedRMSNorm
+
 from apex.normalization import MixedFusedLayerNorm as OrigMixedFusedLayerNorm
 
 HAS_FAST_LAYER_NORM=False
@@ -23,6 +24,7 @@
     "FusedLayerNorm",
     "FastLayerNorm",
     "MixedFusedLayerNorm",
+    "FastRMSNorm"
 ]
 
 
@@ -52,7 +54,26 @@
             _set_sequence_parallel_enabled(self.weight, self.sequence_parallel_enabled)
             _set_sequence_parallel_enabled(self.bias, self.sequence_parallel_enabled)
 
+class FusedRMSNorm(OrigFusedRMSNorm):
+    def __init__(
+        self,
+        normalized_shape,
+        eps: float = 1e-5,
+        elementwise_affine: bool = True,
+        *,
+        sequence_parallel_enabled: bool = False,
+    ):
+        super().__init__(
+            normalized_shape=normalized_shape,
+            eps=eps,
+            elementwise_affine=elementwise_affine,
+        )
+        self.sequence_parallel_enabled = sequence_parallel_enabled
+        if self.elementwise_affine:
+            _set_sequence_parallel_enabled(self.weight, self.sequence_parallel_enabled)
+            # _set_sequence_parallel_enabled(self.bias, self.sequence_parallel_enabled)
 
+
 # note: MixedFusedLayerNorm is no different from FusedLayerNorm if it's used in `torch.cuda.amp`.
 class MixedFusedLayerNorm(OrigMixedFusedLayerNorm):
     def __init__(
@@ -93,10 +114,24 @@
             *,
             sequence_parallel_enabled: bool = False,
         ):
-            warnings.warn("`apex.contrib.layer_norm.FastLayerNorm` isn't available thus falling back to `apex.normalization.FusedLayerNorm`")
+            super().__init__(
+                normalized_shape=hidden_size,
+                eps=eps,
+                elementwise_affine=True,
+                sequence_parallel_enabled=sequence_parallel_enabled,
+            )
+
+class FastRMSNorm(FusedRMSNorm):
+        def __init__(
+            self,
+            hidden_size,
+            eps: float = 1e-5,
+            *,
+            sequence_parallel_enabled: bool = False,
+        ):
             super().__init__(
                 normalized_shape=hidden_size,
                 eps=eps,
                 elementwise_affine=True,
                 sequence_parallel_enabled=sequence_parallel_enabled,
-            )
+            )
\ No newline at end of file
diff -urN neuronx-nemo-megatron/apex/apex/transformer/tensor_parallel/layers.py nemo-updates/apex/apex/transformer/tensor_parallel/layers.py
--- neuronx-nemo-megatron/apex/apex/transformer/tensor_parallel/layers.py	2023-08-24 16:55:40
+++ nemo-updates/apex/apex/transformer/tensor_parallel/layers.py	2023-08-16 13:00:47
@@ -513,6 +513,7 @@
         gradient_accumulation_fusion=False,
         accumulation_in_fp16: bool = False,
         sequence_parallel_enabled: bool = False,
+        transfer_with_static_ring: bool = True,
     ):
         super().__init__()
 
@@ -524,6 +525,7 @@
         world_size = get_tensor_model_parallel_world_size()
         self.output_size_per_partition = divide(output_size, world_size)
         self.skip_bias_add = skip_bias_add
+        self.transfer_with_static_ring = transfer_with_static_ring
 
         # Parameters.
         # Note: torch.nn.functional.linear performs XA^T + b and as a result
@@ -629,7 +631,7 @@
 
         output_parallel = self._forward_impl(
             input=input_parallel,
-            weight=TransferWithStaticRing.apply(self.weight),
+            weight=TransferWithStaticRing.apply(self.weight) if self.transfer_with_static_ring else self.weight,
             bias=bias,
             gradient_accumulation_fusion=self.gradient_accumulation_fusion,
             async_grad_allreduce=self.async_tensor_model_parallel_allreduce,
@@ -702,6 +704,7 @@
         gradient_accumulation_fusion=False,
         accumulation_in_fp16: bool = False,
         sequence_parallel_enabled: bool = False,
+        transfer_with_static_ring: bool = True,
     ):
         super().__init__()
 
@@ -715,6 +718,7 @@
         self.skip_bias_add = skip_bias_add
         self.gradient_accumulation_fusion = gradient_accumulation_fusion
         self.sequence_parallel_enabled = sequence_parallel_enabled
+        self.transfer_with_static_ring = transfer_with_static_ring
         if self.sequence_parallel_enabled and not self.input_is_parallel:
             raise RuntimeError("To enable `sequence_parallel_enabled`, `input_is_parallel` must be `True`")
 
@@ -795,7 +799,7 @@
         # Matrix multiply.
         output_parallel = self._forward_impl(
             input=input_parallel,
-            weight=TransferWithStaticRing.apply(self.weight),
+            weight=TransferWithStaticRing.apply(self.weight) if self.transfer_with_static_ring else self.weight,
             bias=None,
             gradient_accumulation_fusion=self.gradient_accumulation_fusion,
             async_grad_allreduce=False,
diff -urN neuronx-nemo-megatron/k8s/README.md nemo-updates/k8s/README.md
--- neuronx-nemo-megatron/k8s/README.md	1970-01-01 09:00:00
+++ nemo-updates/k8s/README.md	2023-08-16 13:00:47
@@ -0,0 +1,63 @@
+This directory contains a Dockerfile and related assets to enable neuronx-nemo-megatron 
+distributed training jobs on EKS.
+
+## Prereqs
+To use this code, it is assumed that you already have an EKS cluster with a trn1 nodegroup, 
+Neuron/EFA plugins, and FSx storage as outlined in [this tutorial](https://github.com/aws-neuron/aws-neuron-eks-samples/tree/master/dp_bert_hf_pretrain).
+**Note:** torchx and volcano are mentioned in the tutorial, but are not required to run neuronx-nemo-megatron on EKS.
+
+Additionally, you will need to install the [MPI Operator for Kubernetes](https://github.com/kubeflow/mpi-operator) 
+on your cluster.
+
+To run the included GPT pretraining examples, download the preprocessed training dataset
+to the FSx storage attached to your EKS cluster as follows:
+* First, launch and then connect to a temporary pod that has access to the FSx shared volume. Here we assume the volume is located at `/shared` within the pod.
+* Within the temporary pod, run the following commands to download the GPT training data to /shared:
+```
+mkdir -p /shared/examples_datasets/gpt2
+cd /shared/examples_datasets/gpt2
+wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
+wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
+aws s3 cp s3://neuron-s3/training_datasets/gpt/wikipedia/my-gpt2_text_document.bin .  --no-sign-request
+aws s3 cp s3://neuron-s3/training_datasets/gpt/wikipedia/my-gpt2_text_document.idx .  --no-sign-request
+aws s3 cp s3://neuron-s3/training_datasets/gpt/wikipedia/license.txt .  --no-sign-request
+```
+
+## Build the neuronx-nemo-megatron container image
+The following steps can be run on a cloud desktop or vanilla EC2 linux instance (trn1 not required).
+
+First make sure that your Python environment has the wheel and torch packages
+```
+pip3 install wheel torch
+```
+
+Copy/clone the contents of this repository to your cloud desktop / linux instance.
+
+Create a new ECR repository (ex: neuronx_nemo) in the AWS region you intend to use.
+
+Modify `build_docker_image.sh` to specify the correct AWS region and ECR repo.
+
+Ensure that you have configured AWS credentials on your instance with permission to login
+and push images to ECR.
+
+From the root of the cloned repository, run `./k8s/build_docker_image.sh` to build
+the Nemo/Apex packages, build the neuronx-nemo-megatron container image, and then
+push the image to your ECR repo.
+
+## Launch neuronx-nemo-megatron GPT pretraining job on EKS
+Copy the contents of the `example_manifests` directory to the instance you use to 
+manage your EKS cluster.
+
+Modify the contents of the example manifests to reference your ECR repo / image on
+the various lines containing the `image:` definitions.
+
+Launch the ahead-of-time compilation job:
+```
+kubectl apply -f ./mpi_compile.yaml
+```
+
+Once the compilation job is complete, launch the training job:
+```
+kubectl apply -f ./mpi_train.yaml
+```
+
diff -urN neuronx-nemo-megatron/k8s/build_docker_image.sh nemo-updates/k8s/build_docker_image.sh
--- neuronx-nemo-megatron/k8s/build_docker_image.sh	1970-01-01 09:00:00
+++ nemo-updates/k8s/build_docker_image.sh	2023-08-16 13:00:47
@@ -0,0 +1,17 @@
+#!/bin/bash
+# Build a Neuron container for running neuronx-nemo-megatron jobs on EKS
+export DOCKER_BUILDKIT=1
+
+# First build the Nemo and Apex wheels
+./build.sh
+
+# Specify AWS / ECR / repo info
+AWS_ACCT=$(aws sts get-caller-identity | jq -r ".Account")
+REGION=us-west-2
+ECR_REPO=$AWS_ACCT.dkr.ecr.$REGION.amazonaws.com/neuronx_nemo
+
+# Authenticate with ECR, build & push the image
+aws ecr get-login-password --region $REGION | docker login --username AWS \
+    --password-stdin $AWS_ACCT.dkr.ecr.$REGION.amazonaws.com \
+  && docker build . -f ./k8s/docker/Dockerfile -t $ECR_REPO:latest \
+  && docker push $ECR_REPO:latest
diff -urN neuronx-nemo-megatron/k8s/docker/Dockerfile nemo-updates/k8s/docker/Dockerfile
--- neuronx-nemo-megatron/k8s/docker/Dockerfile	1970-01-01 09:00:00
+++ nemo-updates/k8s/docker/Dockerfile	2023-08-16 13:00:47
@@ -0,0 +1,122 @@
+FROM public.ecr.aws/lts/ubuntu:20.04_stable
+ARG PIP="python3.8 -m pip"
+ARG DEBIAN_FRONTEND=noninteractive
+
+# Neuron repos
+ARG APT_REPO=https://apt.repos.neuron.amazonaws.com
+ARG PIP_REPO=https://pip.repos.neuron.amazonaws.com
+
+# Python wonâ€™t try to write .pyc or .pyo files on the import of source modules
+# Force stdin, stdout and stderr to be totally unbuffered. Good for logging
+ENV PYTHONDONTWRITEBYTECODE=1
+ENV PYTHONUNBUFFERED=1
+ENV PYTHONIOENCODING=UTF-8
+ENV LANG=C.UTF-8
+ENV LC_ALL=C.UTF-8
+ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/aws/neuron/lib"
+ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/amazon/efa/lib"
+ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/amazon/efa/lib64"
+ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/amazon/openmpi/lib64"
+ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/usr/local/lib"
+
+RUN apt-get update \
+    && apt-get install -y --no-install-recommends \
+    build-essential \
+    ca-certificates \
+    cmake \
+    curl \
+    git \
+    jq \
+    software-properties-common \
+    wget \
+    unzip \
+    vim \
+    zlib1g-dev \
+    openssl \
+    libssl-dev \
+    libsqlite3-dev \
+    libgdbm-dev \
+    libc6-dev \
+    libbz2-dev \
+    tk-dev \
+    libffi-dev \
+    libcap-dev \
+    gnupg2 \
+    gpg-agent \
+    pciutils \
+    python3.8-full \
+    python3.8-dev \
+    cython3 \
+    inetutils-ping \
+    google-perftools \
+    ffmpeg \
+    && rm -rf /var/lib/apt/lists/* \
+    && apt-get clean
+
+# EFA Installer - required - installs libfabric (but no EFA driver) inside the container
+RUN apt-get update \
+    && cd $HOME \
+    && curl -O https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz \
+    && wget https://efa-installer.amazonaws.com/aws-efa-installer.key && gpg --import aws-efa-installer.key \
+    && cat aws-efa-installer.key | gpg --fingerprint \
+    && wget https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz.sig && gpg --verify ./aws-efa-installer-latest.tar.gz.sig \
+    && tar -xf aws-efa-installer-latest.tar.gz \
+    && cd aws-efa-installer \
+    && ./efa_installer.sh -y -g -d --skip-kmod --skip-limit-conf --no-verify \
+    && rm -fr /root/aws-efa-installer* \
+    && cd $HOME \
+    && rm -rf /var/lib/apt/lists/* \
+    && rm -rf /tmp/tmp* \
+    && apt-get clean
+
+# Neuron system packages (minus driver)
+RUN echo "deb $APT_REPO focal main" > /etc/apt/sources.list.d/neuron.list \
+    && wget -qO - $APT_REPO/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | apt-key add - \
+    && apt-get update \
+    && apt-get install -y aws-neuronx-tools aws-neuronx-collectives aws-neuronx-runtime-lib \
+    && rm -rf /var/lib/apt/lists/* \
+    && rm -rf /tmp/tmp* \
+    && apt-get clean
+
+WORKDIR /
+RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3.8 get-pip.py && python3.8 -m pip install --upgrade pip
+RUN mkdir -p /etc/pki/tls/certs && cp /etc/ssl/certs/ca-certificates.crt /etc/pki/tls/certs/ca-bundle.crt
+
+# PyTorch Neuron packages (2.12)
+RUN ${PIP} config set global.extra-index-url $PIP_REPO \
+    && ${PIP} install --force-reinstall torch-neuronx==1.13.1.* neuronx-cc==2.8.* \
+    && ${PIP} install --no-cache-dir -U python-etcd \
+    && rm -fr /root/.cache/
+
+# Install packages and configure SSH for MPI operator in k8s
+RUN apt-get update && apt-get install -y openmpi-bin openssh-server \
+    && mkdir -p /var/run/sshd \
+    && echo "    UserKnownHostsFile /dev/null" >> /etc/ssh/ssh_config \
+    && echo "    StrictHostKeyChecking no" >> /etc/ssh/ssh_config \
+    && sed -i 's/#\(StrictModes \).*/\1no/g' /etc/ssh/sshd_config \
+    && rm -rf /var/lib/apt/lists/* \
+    && rm -rf /tmp/tmp* \
+    && apt-get clean
+
+# Add Neuron tools to path
+RUN echo "export PATH=/opt/aws/neuron/bin:\$PATH" >> /root/.bashrc \
+    && echo "export TERM=screen" >> /root/.bashrc
+
+# Install Nemo, Apex, and related packages
+COPY ./build/ /nemo_packages
+RUN cd /nemo_packages && ${PIP} install \
+    nemo_toolkit-1.14.0-py3-none-any.whl['all'] \
+    apex-0.1-py3-none-any.whl \
+    torch==1.13.* protobuf==3.20.3 \
+    && rm -fr /root/.cache/
+
+# Add Nemo and allreduce examples
+COPY ./nemo/examples/ /nemo_examples
+RUN cd /usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/ && make -C .
+COPY ./k8s/docker/allreduce* /nemo_examples/nlp/language_modeling
+
+# nodelist_helper.py is required for MPI runs on EKS
+COPY ./k8s/docker/nodelist_helper.py /
+WORKDIR /nemo_examples/nlp/language_modeling
+
+CMD ["/bin/bash"]
diff -urN neuronx-nemo-megatron/k8s/docker/allreduce.py nemo-updates/k8s/docker/allreduce.py
--- neuronx-nemo-megatron/k8s/docker/allreduce.py	1970-01-01 09:00:00
+++ nemo-updates/k8s/docker/allreduce.py	2023-08-16 13:00:47
@@ -0,0 +1,28 @@
+import torch
+import torch_xla.core.xla_model as xm
+import torch.distributed as dist
+import torch_xla.distributed.xla_backend
+import os
+
+def rprint(txt):
+    rank = os.environ.get("RANK", "unk")
+    if int(rank) == 0:
+        print(f"{rank}: {txt}", flush=True)
+
+dist.init_process_group('xla')
+rprint("before 1st rendezvous")
+xm.rendezvous('first')
+
+device = xm.xla_device()
+for c in range(1000000):
+    ones = torch.ones((2, 3))
+    xones = ones.to(device)
+    result = xm.all_reduce('sum', xones)
+    xm.mark_step()
+    result_cpu = result.cpu()
+    expected = torch.ones((2, 3)) * int(os.environ.get("WORLD_SIZE", 0))
+    rprint(f"result: {c}: {result}  result.size(): {result.size()}")
+    assert torch.all(result_cpu == expected), f'ERROR: {result_cpu} != {expected}'
+
+rprint("before final rendezvous")
+xm.rendezvous('last')
diff -urN neuronx-nemo-megatron/k8s/docker/allreduce.sh nemo-updates/k8s/docker/allreduce.sh
--- neuronx-nemo-megatron/k8s/docker/allreduce.sh	1970-01-01 09:00:00
+++ nemo-updates/k8s/docker/allreduce.sh	2023-08-16 13:00:47
@@ -0,0 +1,46 @@
+#!/usr/bin/env bash
+set -o pipefail
+ulimit -n 65535
+sysctl -w net.ipv4.ip_local_reserved_ports=41000
+
+export FI_EFA_USE_DEVICE_RDMA=1
+export FI_PROVIDER=efa
+export FI_EFA_FORK_SAFE=1
+export CCOM_SOCKET_IFNAME=eth0
+
+if [ -v SLURM_NNODES ]
+then
+    # SLURM runs
+    IPS=""
+    for h in $(scontrol show hostname); do
+        IPS="$IPS $(nslookup $h  | awk '/^Address: / { print $2 }')";
+    done
+    HOSTS=(${IPS//\ / })
+    NODEID=$SLURM_NODEID
+    NTASKS=$SLURM_NTASKS
+elif [ -v OMPI_COMM_WORLD_RANK ]
+then
+    # MPI runs
+    NODELIST=`/nodelist_helper.py`
+    HOSTS=(${NODELIST//\ / })
+    NODEID=$OMPI_COMM_WORLD_RANK
+    NTASKS=$OMPI_COMM_WORLD_SIZE
+else
+    # Single-node, non-SLURM, non-MPI runs
+    HOSTS=(localhost)
+    NODEID=0
+    NTASKS=1
+fi
+
+export PROCESSES_PER_NODE=32
+export MASTER_ADDR=${HOSTS[0]}
+export MASTER_PORT=41000
+
+DISTRIBUTED_ARGS="--nproc_per_node $PROCESSES_PER_NODE --nnodes $NTASKS --node_rank $NODEID --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
+echo $DISTRIBUTED_ARGS
+
+export MALLOC_ARENA_MAX=128
+export XLA_USE_BF16=1
+export TF_NUM_INTEROP_THREADS=8192
+
+torchrun $DISTRIBUTED_ARGS allreduce.py 2>&1 | tee /data/$(hostname).log
diff -urN neuronx-nemo-megatron/k8s/docker/nodelist_helper.py nemo-updates/k8s/docker/nodelist_helper.py
--- neuronx-nemo-megatron/k8s/docker/nodelist_helper.py	1970-01-01 09:00:00
+++ nemo-updates/k8s/docker/nodelist_helper.py	2023-08-16 13:00:47
@@ -0,0 +1,20 @@
+#!/usr/bin/env python3
+# Helper script to create a list of worker pods participating in
+#   a distributed training job based on current worker's hostname
+#   and the OMPI world size
+import re
+import os
+##os.environ['PMIX_HOSTNAME'] = "test-mpi-dumpenv-worker-1"
+##os.environ['OMPI_COMM_WORLD_SIZE'] = "2"
+this_host = os.environ.get("PMIX_HOSTNAME", "")
+s = re.search(r"^(.*-worker)-\d+", this_host)
+if not s:
+    raise Exception("Error: This script should be run via mpirun on EKS")
+host_prefix = s.group(1)
+world_size = int(os.environ.get("OMPI_COMM_WORLD_SIZE", "0"))
+
+hosts = []
+for x in range(world_size):
+    hosts.append(f"{host_prefix}-{x}.{host_prefix}.default.svc")
+
+print(" ".join(hosts))
diff -urN neuronx-nemo-megatron/k8s/example_manifests/mpi_allreduce.yaml nemo-updates/k8s/example_manifests/mpi_allreduce.yaml
--- neuronx-nemo-megatron/k8s/example_manifests/mpi_allreduce.yaml	1970-01-01 09:00:00
+++ nemo-updates/k8s/example_manifests/mpi_allreduce.yaml	2023-08-16 13:00:47
@@ -0,0 +1,71 @@
+apiVersion: kubeflow.org/v2beta1
+kind: MPIJob
+metadata:
+  name: test-mpi-allreduce
+spec:
+  slotsPerWorker: 1
+  runPolicy:
+    cleanPodPolicy: Running
+  mpiReplicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+         spec:
+           containers:
+           - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             name: mpitest
+             imagePullPolicy: Always
+             command:
+             - mpirun
+             - --allow-run-as-root
+             - -np
+             - "4"
+             - -bind-to
+             - none
+             - -map-by
+             - slot
+             - -x
+             - LD_LIBRARY_PATH
+             - -x
+             - PATH
+             - ./allreduce.sh
+           initContainers:
+           - name: wait-hostfilename
+             image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             command:
+             - bash
+             - -cx
+             - "[[ $(cat /etc/mpi/discover_hosts.sh | wc -l) != 1 ]] && (date; echo Ready; cat /etc/mpi/discover_hosts.sh) || (date; echo 'not ready ...'; sleep 10; exit 1) && while read host; do while ! ssh $host echo $host ; do date; echo \"Pod $host is not up ...\"; sleep 10; done; date; echo \"Pod $host is ready\"; done <<< \"$(/etc/mpi/discover_hosts.sh)\""
+             volumeMounts:
+             - mountPath: /etc/mpi
+               name: mpi-job-config
+             - mountPath: /root/.ssh
+               name: ssh-auth
+
+    Worker:
+      replicas: 4
+      template:
+        spec:
+          containers:
+          - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+            name: mpitest
+            imagePullPolicy: Always
+            resources:
+              limits:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+              requests:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+            volumeMounts:
+            - name: persistent-storage
+              mountPath: /shared
+            - name: dshm
+              mountPath: /dev/shm
+          volumes:
+          - name: persistent-storage
+            persistentVolumeClaim:
+              claimName: fsx-claim
+          - name: dshm
+            emptyDir:
+              medium: Memory
diff -urN neuronx-nemo-megatron/k8s/example_manifests/mpi_compile.yaml nemo-updates/k8s/example_manifests/mpi_compile.yaml
--- neuronx-nemo-megatron/k8s/example_manifests/mpi_compile.yaml	1970-01-01 09:00:00
+++ nemo-updates/k8s/example_manifests/mpi_compile.yaml	2023-08-16 13:00:47
@@ -0,0 +1,80 @@
+apiVersion: kubeflow.org/v2beta1
+kind: MPIJob
+metadata:
+  name: test-mpi-compile
+spec:
+  slotsPerWorker: 1
+  runPolicy:
+    cleanPodPolicy: Running
+  mpiReplicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+         spec:
+           containers:
+           - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             name: mpitest
+             imagePullPolicy: Always
+             env:
+               - name: POD_UID
+                 valueFrom:
+                  fieldRef:
+                    fieldPath: metadata.uid
+             command:
+             - mpirun
+             - --allow-run-as-root
+             - -np
+             - "4"
+             - -bind-to
+             - none
+             - -map-by
+             - slot
+             - -x
+             - LD_LIBRARY_PATH
+             - -x
+             - PATH
+             - -x
+             - POD_UID
+             - neuron_parallel_compile
+             - ./gpt_23b.sh
+           initContainers:
+           - name: wait-hostfilename
+             image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             command:
+             - bash
+             - -cx
+             - "[[ $(cat /etc/mpi/discover_hosts.sh | wc -l) != 1 ]] && (date; echo Ready; cat /etc/mpi/discover_hosts.sh) || (date; echo 'not ready ...'; sleep 10; exit 1) && while read host; do while ! ssh $host echo $host ; do date; echo \"Pod $host is not up ...\"; sleep 10; done; date; echo \"Pod $host is ready\"; done <<< \"$(/etc/mpi/discover_hosts.sh)\""
+             volumeMounts:
+             - mountPath: /etc/mpi
+               name: mpi-job-config
+             - mountPath: /root/.ssh
+               name: ssh-auth
+
+    Worker:
+      replicas: 4
+      template:
+        spec:
+          containers:
+          - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+            name: mpitest
+            imagePullPolicy: Always
+            resources:
+              limits:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+              requests:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+            volumeMounts:
+            - name: persistent-storage
+              mountPath: /shared
+            - name: dshm
+              mountPath: /dev/shm
+          volumes:
+          - name: persistent-storage
+            persistentVolumeClaim:
+              claimName: fsx-claim
+          - name: dshm
+            emptyDir:
+              medium: Memory
+
diff -urN neuronx-nemo-megatron/k8s/example_manifests/mpi_train.yaml nemo-updates/k8s/example_manifests/mpi_train.yaml
--- neuronx-nemo-megatron/k8s/example_manifests/mpi_train.yaml	1970-01-01 09:00:00
+++ nemo-updates/k8s/example_manifests/mpi_train.yaml	2023-08-16 13:00:47
@@ -0,0 +1,79 @@
+apiVersion: kubeflow.org/v2beta1
+kind: MPIJob
+metadata:
+  name: test-mpi-train
+spec:
+  slotsPerWorker: 1
+  runPolicy:
+    cleanPodPolicy: Running
+  mpiReplicaSpecs:
+    Launcher:
+      replicas: 1
+      template:
+         spec:
+           containers:
+           - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             name: mpitest
+             imagePullPolicy: Always
+             env:
+               - name: POD_UID
+                 valueFrom:
+                  fieldRef:
+                    fieldPath: metadata.uid
+             command:
+             - mpirun
+             - --allow-run-as-root
+             - -np
+             - "4"
+             - -bind-to
+             - none
+             - -map-by
+             - slot
+             - -x
+             - LD_LIBRARY_PATH
+             - -x
+             - PATH
+             - -x
+             - POD_UID
+             - ./gpt_23b.sh
+           initContainers:
+           - name: wait-hostfilename
+             image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+             command:
+             - bash
+             - -cx
+             - "[[ $(cat /etc/mpi/discover_hosts.sh | wc -l) != 1 ]] && (date; echo Ready; cat /etc/mpi/discover_hosts.sh) || (date; echo 'not ready ...'; sleep 10; exit 1) && while read host; do while ! ssh $host echo $host ; do date; echo \"Pod $host is not up ...\"; sleep 10; done; date; echo \"Pod $host is ready\"; done <<< \"$(/etc/mpi/discover_hosts.sh)\""
+             volumeMounts:
+             - mountPath: /etc/mpi
+               name: mpi-job-config
+             - mountPath: /root/.ssh
+               name: ssh-auth
+
+    Worker:
+      replicas: 4
+      template:
+        spec:
+          containers:
+          - image: <aws-account-id>.dkr.ecr.us-west-2.amazonaws.com/neuronx_nemo:latest
+            name: mpitest
+            imagePullPolicy: Always
+            resources:
+              limits:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+              requests:
+                aws.amazon.com/neuron: "16"
+                vpc.amazonaws.com/efa: "8"
+            volumeMounts:
+            - name: persistent-storage
+              mountPath: /shared
+            - name: dshm
+              mountPath: /dev/shm
+          volumes:
+          - name: persistent-storage
+            persistentVolumeClaim:
+              claimName: fsx-claim
+          - name: dshm
+            emptyDir:
+              medium: Memory
+
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml nemo-updates/nemo/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml	2023-08-24 16:55:40
+++ nemo-updates/nemo/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml	2023-08-16 13:00:48
@@ -21,6 +21,7 @@
   enable_model_summary: False # default PTL callback for this does not support model parallelism, instead we log manually
 
 exp_manager:
+  create_tensorboard_logger: True
   explicit_log_dir: null
   exp_dir: null
   name: megatron_gpt
@@ -109,6 +110,10 @@
   apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
   gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
   sync_batch_comm: False # Enable stream synchronization after each p2p communication between pipeline stages
+  log_parameter_norm: False # Logs parameter norm across model parallel ranks
+  log_gradient_norm: False # Logs gradient norm across model parallel ranks
+  save_logits: False # Saves logits in numpy array of dimensions batch x sequence_length x vocab_size
+  save_logits_interval: 0 # Step interval at which to save logits to disk - Note should be Zero Indexed
 
   ## Activation Checkpointing
   # NeMo Megatron supports 'selective' activation checkpointing where only the memory intensive part of attention is checkpointed.
@@ -142,6 +147,11 @@
   # Makes tensor parallelism more memory efficient for LLMs (20B+) by parallelizing layer norms and dropout sequentially
   # See Reducing Activation Recomputation in Large Transformer Models: https://arxiv.org/abs/2205.05198 for more details.
   sequence_parallel: False
+
+  ## Zero Redundancy Optimizer
+  # Wraps your chosen optimizer with a Zero Redundancy Optimizer
+  # Partitions optimizer states across ranks reducing memory consumption
+  wrap_with_zero: False
 
   ## Transformer Engine
   transformer_engine: False
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/conf/megatron_llama_config.yaml nemo-updates/nemo/examples/nlp/language_modeling/conf/megatron_llama_config.yaml
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/conf/megatron_llama_config.yaml	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/conf/megatron_llama_config.yaml	2023-08-16 13:00:48
@@ -0,0 +1,220 @@
+name: megatron_llama
+restore_from_path: null # used when starting from a .nemo file
+
+trainer:
+  devices: 1
+  num_nodes: 1
+  accelerator: tpu
+  precision: 32
+  logger: False # logger provided by exp_manager
+  enable_checkpointing: False
+  replace_sampler_ddp: False
+  max_epochs: -1 # PTL default. In practice, max_steps will be reached first. 
+  max_steps: 100000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
+  log_every_n_steps: 10
+  val_check_interval: 100
+  limit_val_batches: 50
+  limit_test_batches: 500
+  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
+  gradient_clip_val: 1.0
+  benchmark: False
+  enable_model_summary: False # default PTL callback for this does not support model parallelism, instead we log manually
+
+exp_manager:
+  create_tensorboard_logger: True
+  explicit_log_dir: null
+  exp_dir: null
+  name: megatron_llama
+  create_wandb_logger: False
+  wandb_logger_kwargs:
+    project: null
+    name: null
+  resume_if_exists: True
+  resume_ignore_no_checkpoint: True
+  create_checkpoint_callback: True
+  checkpoint_callback_params:
+    monitor: step
+    save_top_k: -1
+    mode: max
+    save_last: True
+    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
+    save_nemo_on_train_end: False # not recommended when training large models on clusters with short time limits
+    filename: 'megatron_llama--{step}-{consumed_samples}'
+    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
+
+
+model:
+  # specify micro_batch_size, global_batch_size, and model parallelism
+  # gradient accumulation will be done automatically based on data_parallel_size
+  micro_batch_size: 4 # limited by GPU memory
+  global_batch_size: 8 # will use more micro batches to reach global batch size
+  tensor_model_parallel_size: 1 # intra-layer model parallelism
+  pipeline_model_parallel_size: 1 # inter-layer model parallelism
+  virtual_pipeline_model_parallel_size: null # interleaved pipeline
+
+  # model architecture
+  encoder_seq_length: 512
+  max_position_embeddings: ${.encoder_seq_length}
+  num_layers: 12
+  hidden_size: 768
+  ffn_hidden_size: 3072 # Transformer FFN hidden size. For Llama it's 8/3*hidden_size
+  num_attention_heads: 12
+  init_method_std: 0.02 # Standard deviation of the zero mean normal distribution used for weight initialization.')
+  use_scaled_init_method: True # use scaled residuals initialization
+  hidden_dropout: 0 # Dropout probability for hidden state transformer.
+  attention_dropout: 0 # Dropout probability in the attention layer.
+  ffn_dropout: 0 # Dropout probability in the feed-forward layer.
+  kv_channels: null # Projection weights dimension in multi-head attention. Set to hidden_size // num_attention_heads if null
+  apply_query_key_layer_scaling: True # scale Q * K^T by 1 / layer-number.
+  normalization: 'rmsnorm' # Type of normalization layers ['rmsnorm', 'layernorm']
+  layernorm_epsilon: 1e-5
+  do_layer_norm_weight_decay: False # True means weight decay on all params
+  make_vocab_size_divisible_by: 8 # Pad the vocab size to be divisible by this value for computation efficiency.
+  pre_process: True # add embedding
+  post_process: True # add pooler
+  persist_layer_norm: True # Use of persistent fused layer norm kernel.
+  share_embeddings_and_output_weights: False # Untie embedding and output layer weights.
+  position_embedding_type: 'rope' # Position embedding type. Options ['learned_absolute', 'rope]
+  rotary_percentage: 1 # If using position_embedding_type=rope, then the per head dim is multiplied by this.
+  activation: 'swiglu' # ['swiglu', 'gelu']
+  transformer_block_type: 'pre_ln' # ['pre_ln', 'post_ln', 'normformer', 'gpt_j'] https://github.com/EleutherAI/gpt-neox/blob/303d7be582ae1c969347c25c54f568cc122445fc/megatron/model/transformer.py#L804-L847
+  has_bias: False
+
+  tokenizer:
+    library: 'huggingface'
+    type: '/root/scripts/data/llama7b-hf'
+    model: null
+    vocab_file: null
+    merge_file: null 
+    delimiter: null # only used for tabular tokenizer
+    sentencepiece_legacy: False # Legacy=True allows you to add special tokens to sentencepiece tokenizers.
+    use_fast: False
+
+  # Mixed precision
+  native_amp_init_scale: 4294967296 # 2 ** 32
+  native_amp_growth_interval: 1000
+  hysteresis: 2 # Gradient scale hysteresis
+  fp32_residual_connection: False # Move residual connections to fp32
+  fp16_lm_cross_entropy: False # Move the cross entropy unreduced loss calculation for lm head to fp16
+
+  # Megatron O2-style half-precision
+  megatron_amp_O2: False # Enable O2-level automatic mixed precision using main parameters
+  grad_allreduce_chunk_size_mb: 125
+
+  # Fusion
+  grad_div_ar_fusion: False # Fuse grad division into torch.distributed.all_reduce. Only used with O2 and no pipeline parallelism..
+  gradient_accumulation_fusion: False # Fuse weight gradient accumulation to GEMMs. Only used with pipeline parallelism and O2.
+  bias_activation_fusion: False # Use a kernel that fuses the bias addition from weight matrices with the subsequent activation function.
+  bias_dropout_add_fusion: False # Use a kernel that fuses the bias addition, dropout and residual connection addition.
+  masked_softmax_fusion: False # Use a kernel that fuses the attention softmax with it's mask.
+
+
+  # Miscellaneous
+  seed: 1234
+  resume_from_checkpoint: null # manually set the checkpoint file to load from
+  use_cpu_initialization: False # Init weights on the CPU (slow for large models)
+  onnx_safe: False # Use work-arounds for known problems with Torch ONNX exporter.
+  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
+  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
+  sync_batch_comm: False # Enable stream synchronization after each p2p communication between pipeline stages
+  log_parameter_norm: True # Logs parameter norm across model parallel ranks
+  log_gradient_norm: True # Logs gradient norm across model parallel ranks
+
+  ## Activation Checkpointing
+  # NeMo Megatron supports 'selective' activation checkpointing where only the memory intensive part of attention is checkpointed.
+  # These memory intensive activations are also less compute intensive which makes activation checkpointing more efficient for LLMs (20B+).
+  # See Reducing Activation Recomputation in Large Transformer Models: https://arxiv.org/abs/2205.05198 for more details.
+  # 'full' will checkpoint the entire transformer layer.
+  activations_checkpoint_granularity: selective # 'selective' or 'full' 
+  activations_checkpoint_method: uniform # 'uniform', 'block'
+  # 'uniform' divides the total number of transformer layers and checkpoints the input activation
+  # of each chunk at the specified granularity. When used with 'selective', 'uniform' checkpoints all attention blocks in the model.
+  # 'block' checkpoints the specified number of layers per pipeline stage at the specified granularity
+  activations_checkpoint_num_layers: 1
+  # when using 'uniform' this creates groups of transformer layers to checkpoint. Usually set to 1. Increase to save more memory.
+  # when using 'block' this this will checkpoint the first activations_checkpoint_num_layers per pipeline stage.
+  num_micro_batches_with_partial_activation_checkpoints: null
+  # This feature is valid only when used with pipeline-model-parallelism.
+  # When an integer value is provided, it sets the number of micro-batches where only a partial number of Transformer layers get checkpointed
+  # and recomputed within a window of micro-batches. The rest of micro-batches in the window checkpoint all Transformer layers. The size of window is
+  # set by the maximum outstanding micro-batch backpropagations, which varies at different pipeline stages. The number of partial layers to checkpoint
+  # per micro-batch is set by 'activations_checkpoint_num_layers' with 'activations_checkpoint_method' of 'block'.
+  # This feature enables using activation checkpoint at a fraction of micro-batches up to the point of full GPU memory usage.
+  activations_checkpoint_layers_per_pipeline: null
+  # This feature is valid only when used with pipeline-model-parallelism.
+  # When an integer value (rounded down when float is given) is provided, it sets the number of Transformer layers to skip checkpointing at later
+  # pipeline stages. For example, 'activations_checkpoint_layers_per_pipeline' of 3 makes pipeline stage 1 to checkpoint 3 layers less than
+  # stage 0 and stage 2 to checkpoint 6 layers less stage 0, and so on. This is possible because later pipeline stage
+  # uses less GPU memory with fewer outstanding micro-batch backpropagations. Used with 'num_micro_batches_with_partial_activation_checkpoints',
+  # this feature removes most of activation checkpoints at the last pipeline stage, which is the critical execution path.
+
+  ## Sequence Parallelism
+  # Makes tensor parallelism more memory efficient for LLMs (20B+) by parallelizing layer norms and dropout sequentially
+  # See Reducing Activation Recomputation in Large Transformer Models: https://arxiv.org/abs/2205.05198 for more details.
+  sequence_parallel: True
+
+  ## Zero Redundancy Optimizer
+  # Wraps your chosen optimizer with a Zero Redundancy Optimizer
+  # Partitions optimizer states across ranks reducing memory consumption
+  wrap_with_zero: False
+
+  ## Transformer Engine
+  transformer_engine: False
+  fp8: False # enables fp8 in TransformerLayer forward
+  fp8_e4m3: False # sets fp8_format = recipe.Format.E4M3 
+  fp8_hybrid: False # sets fp8_format = recipe.Format.HYBRID
+  fp8_margin: 0 # scaling margin 
+  fp8_interval: 1 # scaling update interval
+  fp8_amax_history_len: 1 # Number of steps for which amax history is recorded per tensor
+  fp8_amax_compute_algo: most_recent # 'most_recent' or 'max'. Algorithm for computing amax from history
+  use_emha: False # Use fused multi-head attention for large sequence-length. Note this is not yet supported. Please set to False.
+
+  data:
+   # Path to data must be specified by the user.
+    # Supports List, String and Dictionary
+    # List : can override from the CLI: "model.data.data_prefix=[.5,/raid/data/pile/my-gpt3_00_text_document,.5,/raid/data/pile/my-gpt3_01_text_document]",
+    # Or see example below: 
+    # data_prefix: 
+    #   - .5
+    #   - /raid/data/pile/my-gpt3_00_text_document
+    #   - .5
+    #   - /raid/data/pile/my-gpt3_01_text_document
+    # Dictionary: can override from CLI "model.data.data_prefix"={"train":[1.0, /path/to/data], "validation":/path/to/data, "test":/path/to/test}
+    # Or see example below:
+    # "model.data.data_prefix: {train:[1.0,/path/to/data], validation:[/path/to/data], test:[/path/to/test]}"
+    data_prefix: ???
+    index_mapping_dir: null # path to save index mapping .npy files, by default will save in the same location as data_prefix
+    data_impl: mmap
+    splits_string: 900,50,50
+    seq_length: ${model.encoder_seq_length}
+    skip_warmup: True
+    num_workers: 2
+    dataloader_type: single # cyclic
+    reset_position_ids: False # Reset position ids after end-of-document token
+    reset_attention_mask: False # Reset attention mask after end-of-document token
+    eod_mask_loss: False # Mask loss for the end of document tokens
+    validation_drop_last: True # Set to false if the last partial validation samples is to be consumed
+    no_seqlen_plus_one_input_tokens: False # Set to True to disable fetching (sequence length + 1) input tokens, instead get (sequence length) input tokens and mask the last token
+    pad_samples_to_global_batch_size: False # Set to True if you want to pad the last partial batch with -1's to equal global batch size
+
+  # Nsys profiling options
+  nsys_profile:
+    enabled: False
+    start_step: 10  # Global batch to start profiling
+    end_step: 10 # Global batch to end profiling
+    ranks: [0] # Global rank IDs to profile
+    gen_shape: False # Generate model and kernel details including input shapes
+  
+  optim:
+    name: adamw
+    lr: 2e-4
+    weight_decay: 0.01 
+    capturable: False
+    betas: 
+    - 0.9
+    - 0.98
+    sched:
+      name: CosineAnnealing
+      warmup_steps: 500
+      constant_steps: 50000
+      min_lr: 2e-5
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_175b.sh nemo-updates/nemo/examples/nlp/language_modeling/gpt_175b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_175b.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/gpt_175b.sh	2023-08-16 13:00:48
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+export SEQ_LENGTH=2048
+export HS=12288
+export TP=32
+export PP=8
+export N_LAYERS=96
+export N_AH=96
+
+./test.sh
+
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_23b.sh nemo-updates/nemo/examples/nlp/language_modeling/gpt_23b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_23b.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/gpt_23b.sh	2023-08-16 13:00:48
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+export SEQ_LENGTH=2048
+export HS=8192
+export TP=8
+export PP=4
+export N_LAYERS=28
+export N_AH=64
+
+./test.sh
+
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_24b.sh nemo-updates/nemo/examples/nlp/language_modeling/gpt_24b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_24b.sh	2023-08-24 16:55:40
+++ nemo-updates/nemo/examples/nlp/language_modeling/gpt_24b.sh	1970-01-01 09:00:00
@@ -1,11 +0,0 @@
-#!/usr/bin/env bash
-
-export SEQ_LENGTH=2048
-export HS=8192
-export TP=8
-export PP=4
-export N_LAYERS=28
-export N_AH=64
-
-./test.sh
-
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_32b.sh nemo-updates/nemo/examples/nlp/language_modeling/gpt_32b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_32b.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/gpt_32b.sh	2023-08-16 13:00:48
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+export SEQ_LENGTH=2048
+export HS=8192
+export TP=8
+export PP=4
+export N_LAYERS=40
+export N_AH=64
+
+./test.sh
+
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_64b.sh nemo-updates/nemo/examples/nlp/language_modeling/gpt_64b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/gpt_64b.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/gpt_64b.sh	2023-08-16 13:00:48
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+export SEQ_LENGTH=2048
+export HS=8192
+export TP=8
+export PP=8
+export N_LAYERS=80
+export N_AH=64
+
+./test.sh
+
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/lightning_neuron_patch.py nemo-updates/nemo/examples/nlp/language_modeling/lightning_neuron_patch.py
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/lightning_neuron_patch.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/examples/nlp/language_modeling/lightning_neuron_patch.py	2023-08-16 13:00:48
@@ -1,12 +1,21 @@
 from typing import Any, Callable, Dict, List, Optional, Union
+import functools
 import torch
 import sys
 from lightning_lite.utilities.device_parser import _check_data_type
+from lightning_utilities.core.imports import RequirementCache
 
 def auto_device_count_patched() -> int:
     """Get the devices when set to auto."""
     return 2
 
+_XLA_AVAILABLE = RequirementCache("torch_xla")
+
+@functools.lru_cache(maxsize=1)
+def is_available() -> bool:
+    # check `_XLA_AVAILABLE` again to avoid launching processes
+    return bool(_XLA_AVAILABLE)
+
 def _parse_tpu_cores_str_patched(tpu_cores: str) -> Union[int, List[int]]:
     if tpu_cores in ("1", "2", "8", "32"):
         return int(tpu_cores)
@@ -21,7 +30,7 @@
     # allow picking 1 of 8 indexes
     if isinstance(tpu_cores, (list, tuple, set)):
         has_1_tpu_idx = len(tpu_cores) == 1
-        is_valid_tpu_idx = 1 <= list(tpu_cores)[0] <= 8
+        is_valid_tpu_idx = 1 <= list(tpu_cores)[0] <= 32
 
         is_valid_tpu_core_choice = has_1_tpu_idx and is_valid_tpu_idx
         return is_valid_tpu_core_choice
@@ -61,3 +70,4 @@
 tpu_module._parse_tpu_cores_str = _parse_tpu_cores_str_patched
 tpu_module._tpu_cores_valid = _tpu_cores_valid_patched
 tpu_module.TPUAccelerator.auto_device_count = staticmethod(auto_device_count_patched)
+tpu_module.TPUAccelerator.is_available = staticmethod(is_available)
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/llama_7b.sh nemo-updates/nemo/examples/nlp/language_modeling/llama_7b.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/llama_7b.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/llama_7b.sh	2023-08-16 13:00:48
@@ -0,0 +1,17 @@
+#!/usr/bin/env bash
+
+export SEQ_LENGTH=4096
+export HS=4096
+export TP=8
+export PP=1
+export N_LAYERS=32
+export N_AH=32
+export FFN_HS=11008
+export GBS=256
+
+# pushd .
+# cd /usr/local/lib/python3.8/site-packages/nemo/collections/nlp/data/language_modeling/megatron/
+# make
+# popd
+
+./test_llama.sh
\ No newline at end of file
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py nemo-updates/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/examples/nlp/language_modeling/megatron_gpt_pretraining.py	2023-08-16 13:00:48
@@ -12,6 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import lightning_neuron_patch
+import os
 
 from lightning_lite.plugins.environments import TorchElasticEnvironment
 from omegaconf.omegaconf import OmegaConf, open_dict
@@ -61,11 +62,16 @@
     plugins = []
     
     nlp_xla_checkpoint_io = NLPCheckpointIO()
+    cluster_environment = None
+    if os.environ.get("TORCHELASTIC_RUN_ID") is not None:
+        cluster_environment=TorchElasticEnvironment()
     strategy = NLPDDPStrategy(
         no_ddp_communication_hook=True,  # we don't use DDP for async grad allreduce
         gradient_as_bucket_view=cfg.model.gradient_as_bucket_view,
         find_unused_parameters=False,
-        checkpoint_io=nlp_xla_checkpoint_io
+        cluster_environment=cluster_environment,
+        checkpoint_io=nlp_xla_checkpoint_io,
+        megatron_amp_o2=megatron_amp_o2
     )
     if cfg.trainer.precision in [16, 'bf16']:
         scaler = None
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test.sh nemo-updates/nemo/examples/nlp/language_modeling/test.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test.sh	2023-08-24 16:55:40
+++ nemo-updates/nemo/examples/nlp/language_modeling/test.sh	2023-08-16 13:00:48
@@ -1,21 +1,17 @@
 #!/usr/bin/env bash
 set -o pipefail
 
-sudo sysctl -w net.ipv4.ip_local_reserved_ports=48620
+ulimit -n 65535
 
+sudo sysctl -w net.ipv4.ip_local_reserved_ports=41000
+
 export FI_EFA_USE_DEVICE_RDMA=1
 export FI_PROVIDER=efa
 export FI_EFA_FORK_SAFE=1
-export TPU_PORT=51101
 
-if [ -z "${SLURM_NNODES}" ]
+if [ -v SLURM_NNODES ]
 then
-    # Single-node, non-SLURM runs
-    HOSTS=(localhost)
-    NODEID=0
-    NTASKS=1
-else
-    # SLURM runs, single or multi-node
+    # SLURM runs
     IPS=""
     for h in $(scontrol show hostname); do
         IPS="$IPS $(nslookup $h  | awk '/^Address: / { print $2 }')";
@@ -23,64 +19,63 @@
     HOSTS=(${IPS//\ / })
     NODEID=$SLURM_NODEID
     NTASKS=$SLURM_NTASKS
+    export NEMO_EXPM_VERSION=$SLURM_JOB_ID
+    export EXPLICIT_LOGDIR=null
+    LOG_PATH=logs/$SLURM_JOB_ID/$NODEID/
+    mkdir -p $LOG_PATH
+elif [ -v OMPI_COMM_WORLD_RANK ]
+then
+    # MPI runs on EKS
+    export CCOM_SOCKET_IFNAME=eth0
+    NODELIST=`/nodelist_helper.py`
+    HOSTS=(${NODELIST//\ / })
+    NODEID=$OMPI_COMM_WORLD_RANK
+    NTASKS=$OMPI_COMM_WORLD_SIZE
+    export EXPLICIT_LOGDIR=/shared/nemo_experiments/$POD_UID
+    LOG_PATH=$EXPLICIT_LOGDIR/$(hostname)
+    mkdir -p $LOG_PATH
+else
+    # Single-node, non-SLURM, non-MPI runs
+    HOSTS=(localhost)
+    NODEID=0
+    NTASKS=1
+    export NEMO_EXPM_VERSION=$(date "+%Y-%m-%d_%H-%M-%S")
+    export EXPLICIT_LOGDIR=null
+    LOG_PATH=logs
+    mkdir -p $LOG_PATH
 fi
 
+export HYDRA_FULL_ERROR=1
 export PROCESSES_PER_NODE=32
-export XRT_LOCAL_WORKER="c_localservice:$NODEID"
-export XRT_SHARD_ORDINAL=$NODEID
-export XRT_MESH_SERVICE_ADDRESS=${HOSTS[0]}:8477
-export TPU_MESH_CONTROLLER_ADDRESS=${HOSTS[0]}:8476
-export TPU_MESH_CONTROLLER_PORT=8476
-export NEURON_RT_ROOT_COMM_ID=${HOSTS[0]}:48620
-export TF_GRPC_DEFAULT_OPTIONS="grpc.keepalive_time_ms=60000,grpc.keepalive_timeout_ms=14400000,grpc.http2.max_pings_without_data=0,grpc.http2.min_ping_interval_without_data_ms=300000"
-export XRT_SHARD_WORLD_SIZE=$NTASKS
-export WORLD_SIZE=$((NTASKS*PROCESSES_PER_NODE))
 export MASTER_ADDR=${HOSTS[0]}
 export MASTER_PORT=41000
-export ALLOW_MULTIPLE_LIBTPU_LOAD=1
-export NEURON_USE_LOAD_COLLECTIVES=1
-export NEURON_GLOBAL_DEVICE_COUNT=$WORLD_SIZE
-export NEURON_RT_NUM_CORES=$PROCESSES_PER_NODE
-export NEURON_NUM_DEVICES=$NEURON_RT_NUM_CORES
-export CLOUD_TPU_TASK_ID=$NODEID
-export RANK=$((NODEID*PROCESSES_PER_NODE))
-export NEURON_GLOBAL_DEVICE_ID=$RANK
 
+export NEURON_RT_EXEC_TIMEOUT=10
+DISTRIBUTED_ARGS="--nproc_per_node $PROCESSES_PER_NODE --nnodes $NTASKS --node_rank $NODEID --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
+echo $DISTRIBUTED_ARGS
+
 export NEURON_FUSE_SOFTMAX=1
-export NEURON_RT_STOCHASTIC_ROUNDING_EN=0
+export NEURON_RT_STOCHASTIC_ROUNDING_EN=1
+export NEURON_RT_ENABLE_VERBOSE_NUMERICAL_ERRORS=0
+export NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3
 export NEURON_TRANSFER_WITH_STATIC_RING_OPS=""
-export ALLOC_ARENA_MAX=128
+export MALLOC_ARENA_MAX=128
 
-#### Need to set all the server related env variables before server launch
-export TPU_CHIPS_PER_HOST_BOUNDS=$NEURON_RT_NUM_CORES,$NEURON_RT_NUM_CORES
-
 export XLA_USE_BF16=1
-export NEURON_CC_FLAGS="--model-type=transformer --enable-internal-seeded-rng-dropout --tensorizer-options='--no-keep-remat-dma-transpose' --cache_dir=$HOME/neuron_cache/$NODEID"
-export TF_NUM_INTEROP_THREADS=8192
 
-echo "Starting XRT server"
-if [ "$NODEID" = 0 ]; then
-    idx=0
-    for ip in ${HOSTS[@]}; do
-        tpu_configs+=("c_localservice;$((idx++));$ip:$TPU_PORT")
-    done
-    export XRT_TPU_CONFIG=$(IFS="|"; echo "${tpu_configs[*]}")
-    export TPU_NUM_DEVICES=$PROCESSES_PER_NODE
+export NEURON_CC_FLAGS="--model-type transformer --distribution-strategy=nemo"
+if [ -v OMPI_COMM_WORLD_RANK ]  # put Neuron cache on shared storage for MPI/EKS runs
+then
+    NEURON_CC_FLAGS="$NEURON_CC_FLAGS --cache_dir=/shared/nemo_cache/$NODEID"
 fi
+export TF_NUM_INTEROP_THREADS=8192
 
-echo "NTASKS: $NTASKS"
-if [ $NTASKS = 1 ]; then
-    export XRT_TPU_CONFIG="localservice;0;localhost:$TPU_PORT"
-    export XRT_LOCAL_WORKER="localservice:$NODEID"
-    export TPU_NUM_DEVICES=$PROCESSES_PER_NODE
-fi
-
-export XRT_START_LOCAL_SERVER=0
-
 export TRAIN_ITERS=300000
 export GBS=$((NTASKS*32))
+CREATE_TB_LOGGER=True
 if [ "$NEURON_EXTRACT_GRAPHS_ONLY" = "1" ]; then
     export TRAIN_ITERS=3
+    CREATE_TB_LOGGER=False
 fi
 
 : ${SEQ_LENGTH:=2048}
@@ -93,15 +88,16 @@
 export FFN_HS=$(($HS*4))
 echo "SEQ_LEN=$SEQ_LENGTH, HS=$HS, FFN_HS=$FFN_HS TP=$TP PP=$PP N_LAYERS=$N_LAYERS N_AH=$N_AH GBS=$GBS UBS=$UBS"
 
-python3 megatron_gpt_pretraining.py  \
+
+torchrun $DISTRIBUTED_ARGS megatron_gpt_pretraining.py  \
     --config-path=conf \
     --config-name=megatron_gpt_config \
-    trainer.devices=$NEURON_NUM_DEVICES \
+    trainer.devices=$PROCESSES_PER_NODE \
     trainer.num_nodes=$NTASKS \
     trainer.max_epochs=null \
     trainer.max_steps=$TRAIN_ITERS\
     trainer.val_check_interval=$TRAIN_ITERS \
-    trainer.log_every_n_steps=10 \
+    trainer.log_every_n_steps=1 \
     trainer.limit_val_batches=1 \
     trainer.limit_test_batches=1 \
     trainer.accumulate_grad_batches=1 \
@@ -122,7 +118,7 @@
     model.tokenizer.vocab_file=$HOME/examples_datasets/gpt2/gpt2-vocab.json \
     model.tokenizer.merge_file=$HOME/examples_datasets/gpt2/gpt2-merges.txt \
     model.data.data_prefix=[1.0,$HOME/examples_datasets/gpt2/my-gpt2_text_document] \
-    model.data.num_workers=2 \
+    model.data.num_workers=1 \
     model.data.seq_length=$SEQ_LENGTH \
     model.data.splits_string=\'980,10,10\' \
     model.optim.name=adamw \
@@ -139,10 +135,14 @@
     model.activations_checkpoint_method=uniform \
     model.activations_checkpoint_num_layers=1 \
     +model.save_xser=True \
+    exp_manager.create_tensorboard_logger=$CREATE_TB_LOGGER \
     exp_manager.resume_if_exists=False \
     exp_manager.resume_ignore_no_checkpoint=False \
     exp_manager.create_checkpoint_callback=True \
+    exp_manager.explicit_log_dir=$EXPLICIT_LOGDIR \
     +exp_manager.checkpoint_callback_params.train_time_interval=3600 \
-    model.use_cpu_initialization=True   2>&1  | tee  $(hostname).log  &
+    model.use_cpu_initialization=True   2>&1  | tee  $LOG_PATH/log
 
-python3 -m torch_neuronx.distributed._xrt_run_server --port $TPU_PORT --pid_to_track $!
+# Note: to resume training using a checkpoint, please add the following configuration above, adjusting for your checkpoint path
+#    +model.load_xser=True \
+#    model.resume_from_checkpoint='/efs/checkpoint/megatron_gpt--step\=1085-consumed_samples\=69632.0-last.ckpt' \
diff -urN neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh nemo-updates/nemo/examples/nlp/language_modeling/test_llama.sh
--- neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh	1970-01-01 09:00:00
+++ nemo-updates/nemo/examples/nlp/language_modeling/test_llama.sh	2023-08-16 13:00:48
@@ -0,0 +1,134 @@
+#!/usr/bin/env bash
+set -o pipefail
+
+ulimit -n 65535
+
+sudo sysctl -w net.ipv4.ip_local_reserved_ports=41000
+
+export FI_EFA_USE_DEVICE_RDMA=1
+export FI_PROVIDER=efa
+export FI_EFA_FORK_SAFE=1
+
+if [ -z "${SLURM_NNODES}" ]
+then
+    # Single-node, non-SLURM runs
+    HOSTS=(localhost)
+    NODEID=0
+    NTASKS=1
+    export NEMO_EXPM_VERSION=$(date "+%Y-%m-%d_%H-%M-%S")
+else
+    # SLURM runs, single or multi-node
+    IPS=""
+    for h in $(scontrol show hostname); do
+        IPS="$IPS $(nslookup $h  | awk '/^Address: / { print $2 }')";
+    done
+    HOSTS=(${IPS//\ / })
+    NODEID=$SLURM_NODEID
+    NTASKS=$SLURM_NTASKS
+    export NEMO_EXPM_VERSION=$SLURM_JOB_ID
+fi
+
+export HYDRA_FULL_ERROR=1
+export PROCESSES_PER_NODE=32
+export MASTER_ADDR=${HOSTS[0]}
+export MASTER_PORT=41000
+
+export NEURON_RT_EXEC_TIMEOUT=10
+export TPU_NUM_DEVICES=$NEURON_RT_NUM_CORES
+export TPU_CHIPS_PER_HOST_BOUNDS=$NEURON_RT_NUM_CORES
+export NEURON_RT_DBG_A2A_CC=0
+export NEURON_RT_ASYNC_EXEC_MODE=0
+
+DISTRIBUTED_ARGS="--nproc_per_node $PROCESSES_PER_NODE --nnodes $NTASKS --node_rank $NODEID --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
+echo $DISTRIBUTED_ARGS
+
+export NEURON_FUSE_SOFTMAX=1
+export NEURON_RT_STOCHASTIC_ROUNDING_EN=1
+export NEURON_RT_ENABLE_VERBOSE_NUMERICAL_ERRORS=0
+export NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3
+export NEURON_TRANSFER_WITH_STATIC_RING_OPS=""
+export MALLOC_ARENA_MAX=128
+
+export XLA_USE_BF16=1
+export NEURON_CC_FLAGS="--model-type transformer --enable-mixed-precision-accumulation --enable-experimental-O1 --distribution-strategy=nemo --cache_dir=$HOME/neuron_cache/llama/`hostname`"
+export TF_NUM_INTEROP_THREADS=8192
+
+export TRAIN_ITERS=20000
+CREATE_TB_LOGGER=True
+if [ "$NEURON_EXTRACT_GRAPHS_ONLY" = "1" ]; then
+    export TRAIN_ITERS=3
+    CREATE_TB_LOGGER=False
+fi
+
+: ${SEQ_LENGTH:=2048}
+: ${HS:=4096}
+: ${TP:=8}
+: ${PP:=1}
+: ${N_LAYERS:=32}
+: ${N_AH:=32}
+: ${UBS:=1}
+: ${FFN_HS:=11008}
+: ${GBS:=256}
+echo "SEQ_LEN=$SEQ_LENGTH, HS=$HS, FFN_HS=$FFN_HS TP=$TP PP=$PP N_LAYERS=$N_LAYERS N_AH=$N_AH GBS=$GBS UBS=$UBS"
+
+LOG_PATH=logs/$SLURM_JOB_ID/$NODEID/
+mkdir -p $LOG_PATH
+
+torchrun $DISTRIBUTED_ARGS megatron_gpt_pretraining.py  \
+    --config-path=conf \
+    --config-name=megatron_llama_config \
+    trainer.devices=$PROCESSES_PER_NODE \
+    trainer.num_nodes=$NTASKS \
+    trainer.max_epochs=null \
+    trainer.max_steps=$TRAIN_ITERS\
+    trainer.val_check_interval=0.99 \
+    trainer.log_every_n_steps=1 \
+    trainer.limit_val_batches=1 \
+    trainer.limit_test_batches=1 \
+    trainer.accumulate_grad_batches=1 \
+    trainer.precision=32 \
+    model.tokenizer.type='/root/scripts/example_datasets/llamav2_weights/7b-hf' \
+    model.micro_batch_size=$UBS \
+    model.global_batch_size=$GBS \
+    model.tensor_model_parallel_size=$TP \
+    model.pipeline_model_parallel_size=$PP \
+    model.max_position_embeddings=$SEQ_LENGTH \
+    model.encoder_seq_length=$SEQ_LENGTH \
+    model.hidden_size=$HS \
+    model.ffn_hidden_size=$FFN_HS \
+    model.num_layers=$N_LAYERS \
+    model.num_attention_heads=$N_AH \
+    model.init_method_std=0.021 \
+    model.hidden_dropout=0 \
+    model.layernorm_epsilon=1e-5 \
+    model.data.data_prefix=[1.0,/root/scripts/data/books/book.jsonl-processed_text_document] \
+    model.data.num_workers=1 \
+    model.data.seq_length=$SEQ_LENGTH \
+    model.data.splits_string=\'980,10,10\' \
+    model.optim.name=adamw \
+    model.optim.lr=3.0e-4 \
+    model.optim.betas=[0.9,0.95] \
+    model.optim.weight_decay=0.1 \
+    model.optim.sched.name=CosineAnnealing \
+    model.optim.sched.warmup_steps=10 \
+    model.optim.sched.constant_steps=0 \
+    model.optim.sched.min_lr=3.0e-5 \
+    model.optim.capturable=True \
+    model.sequence_parallel=True  \
+    model.activations_checkpoint_granularity=full \
+    model.activations_checkpoint_method=uniform \
+    model.activations_checkpoint_num_layers=1 \
+    +model.save_xser=True \
+    exp_manager.create_tensorboard_logger=$CREATE_TB_LOGGER \
+    exp_manager.resume_if_exists=False \
+    exp_manager.resume_ignore_no_checkpoint=False \
+    exp_manager.create_checkpoint_callback=True \
+    +exp_manager.checkpoint_callback_params.train_time_interval=36000 \
+    exp_manager.checkpoint_callback_params.save_last=False \
+    model.use_cpu_initialization=True   2>&1  | tee  $LOG_PATH/log
+
+# Note: to resume training using a checkpoint, please add the following configuration above, adjusting for your checkpoint path
+    # +model.load_xser=True \
+    # +model.resume_from_checkpoint='/root/scripts/example_datasets/llamav2_weights/llama7b_hf_converted_nemo_v3//mp_rank_07/model_optim_rng.ckpt' \
+# To use mixed precision optimizer, add
+    # model.megatron_amp_O2=True \
\ No newline at end of file
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py nemo-updates/nemo/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/data/language_modeling/megatron/gpt_dataset.py	2023-08-16 13:00:48
@@ -16,6 +16,7 @@
 
 import os
 import time
+from typing import Optional
 
 import numpy as np
 import torch
@@ -65,6 +66,7 @@
 except:
     print("Unable to override")
 
+USING_TORCH_VERSION2 = torch.__version__.startswith('2')
 
 def build_dataset(cfg, trainer, data_prefix, data_impl, num_samples, seq_length, seed, skip_warmup, tokenizer, name):
     def _build_dataset(current_data_prefix, current_num_samples):
@@ -347,6 +349,10 @@
                     os.makedirs(self.index_mapping_dir)
             torch.distributed.barrier()
 
+        gloo_group = None
+        if USING_TORCH_VERSION2:
+            gloo_group = trainer.strategy.gloo_group
+
         # Build index mappings.
         self.doc_idx, self.sample_idx, self.shuffle_idx = _build_index_mappings(
             self.name,
@@ -359,6 +365,7 @@
             index_mapping_dir=self.index_mapping_dir,
             drop_last=drop_last,
             add_extra_token=self.add_extra_token,
+            gloo_group=gloo_group
         )
         deallocate_indexed_dataset_memory(self.indexed_dataset)
 
@@ -451,8 +458,6 @@
     """
     assert tokens.ndim == 1
     seq_length = tokens.numel()
-    # `attention_mask` has the shape of [1, seq_length, seq_length]
-    attention_mask = torch.tril(torch.ones((seq_length, seq_length))).unsqueeze(0)
     loss_mask = torch.ones(seq_length, dtype=torch.float)
     if eod_mask_loss:
         loss_mask[tokens == eod_token] = 0.0
@@ -461,26 +466,36 @@
     if reset_position_ids:
         position_ids = position_ids.clone()
 
-    if reset_position_ids or reset_attention_mask:
+    if reset_position_ids:
         # Find indices where EOD token is.
-        eod_index = position_ids[tokens[b] == eod_token]
+        eod_index = position_ids[tokens == eod_token]
         # Detach indices from positions if going to modify positions.
         if reset_position_ids:
             eod_index = eod_index.clone()
         prev_index = 0
         for j in range(eod_index.numel()):
             i = eod_index[j]
-            if reset_attention_mask:
-                attention_mask[0, (i + 1) :, : (i + 1)] = 0
             if reset_position_ids:
                 position_ids[(i + 1) :] -= i + 1 - prev_index
                 prev_index = i + 1
-    # Convert attention mask to binary.
-    attention_mask = attention_mask < 0.5
     if is_torch_tpu_available():
+        assert (not reset_attention_mask),"Neuron flow does not support attention mask reset"
         attention_mask = torch.tensor([True])
-        # Needs to be a dummy tensor since a whole bunch of bach things are done under the hood. 
+        # Needs to be a dummy tensor since a whole bunch of batch things are done under the hood. 
         # Size does not matter, it will be replaced by device tensor
+    else:
+        # `attention_mask` has the shape of [1, seq_length, seq_length]
+        attention_mask = torch.tril(torch.ones((seq_length, seq_length))).unsqueeze(0)
+        if reset_attention_mask:
+            # Find indices where EOD token is.
+            eod_index = position_ids[tokens == eod_token]
+            prev_index = 0
+            for j in range(eod_index.numel()):
+                i = eod_index[j]
+                if reset_attention_mask:
+                    attention_mask[0, (i + 1) :, : (i + 1)] = 0
+        # Convert attention mask to binary.
+        attention_mask = attention_mask < 0.5
     return attention_mask, loss_mask, position_ids
 
 
@@ -495,6 +510,7 @@
     index_mapping_dir: str = None,
     drop_last: bool = True,
     add_extra_token: int = 1,
+    gloo_group: Optional[torch.distributed.ProcessGroup] = None
 ):
     """Build doc-idx, sample-idx, and shuffle-idx.
     doc-idx: is an array (ordered) of documents to be used in training.
@@ -621,7 +637,10 @@
 
     # torch.distributed.barrier()
     import torch_xla.core.xla_model as xm
-    xm.rendezvous('shuffle_idx_mapping')
+    if USING_TORCH_VERSION2:
+        torch.distributed.monitored_barrier(group=gloo_group)
+    else:
+        xm.rendezvous('shuffle_idx_mapping')
     #counts = torch.cuda.LongTensor([1])
     #torch.distributed.all_reduce(counts, group=parallel_state.get_data_parallel_group())
     #torch.distributed.all_reduce(counts, group=parallel_state.get_pipeline_model_parallel_group())
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron/gpt_model.py	2023-08-16 13:00:48
@@ -35,7 +35,6 @@
     # fake missing classes with None attributes
     AttnMaskType = ApexGuardDefaults()
 
-
 def post_language_model_processing(
     lm_output,
     labels,
@@ -47,7 +46,24 @@
     return_logits=False,
     sequence_parallel=False,
     gradient_accumulation_fusion=False,
+    share_embeddings_and_output_weights=True,
 ):
+    if share_embeddings_and_output_weights==False:
+        lm_output = lm_output.transpose(0, 1).contiguous()
+        # Output.
+        if labels is None:
+            return lm_output
+        else:
+            logits = None
+            if return_logits:
+                logits = output.clone()
+            if fp16_lm_cross_entropy:
+                assert lm_output.dtype == torch.half
+                loss = tensor_parallel.vocab_parallel_cross_entropy(lm_output, labels)
+            else:
+                loss = tensor_parallel.vocab_parallel_cross_entropy(lm_output.float(), labels)
+
+            return loss, logits
     if get_key_value:
         lm_output, presents = lm_output
 
@@ -65,6 +81,10 @@
         gradient_accumulation_fusion=gradient_accumulation_fusion,
         async_tensor_model_parallel_allreduce=async_tensor_model_parallel_allreduce,
     )
+    logits = None
+    if return_logits:
+        # Logits are [batch x seq_length x vocab_size]
+        logits = output.clone()
 
     if get_key_value:
         output = [output, presents]
@@ -84,13 +104,9 @@
 
         # [s b] -> [b, s]
         loss = loss.transpose(0, 1).contiguous()
+        return loss, logits
 
-        if return_logits:
-            return loss, output
-        else:
-            return loss
 
-
 class GPTModel(MegatronModule):
     """GPT-2 Language model."""
 
@@ -151,6 +167,7 @@
         reduce_amax=True,
         use_emha=False,
         multi_query_attention=False,
+        save_logits=False,
     ):
 
         super(GPTModel, self).__init__(share_token_embeddings=share_embeddings_and_output_weights)
@@ -162,6 +179,7 @@
         self.sequence_parallel = sequence_parallel
         self.gradient_accumulation_fusion = gradient_accumulation_fusion
         self.share_embeddings_and_output_weights = share_embeddings_and_output_weights
+        self.save_logits = save_logits
 
         if kv_channels is None:
             assert (
@@ -272,7 +290,6 @@
             inference_max_sequence_len=inference_max_sequence_len,
             checkpoint_activations_all_layers=checkpoint_activations_all_layers,
         )
-
         if self.post_process:
             return post_language_model_processing(
                 lm_output,
@@ -284,9 +301,10 @@
                 self.parallel_output,
                 forward_method_parallel_output,
                 self.fp16_lm_cross_entropy,
-                return_logits=encoder_input is not None,
+                return_logits=self.save_logits,
                 sequence_parallel=self.sequence_parallel,
                 gradient_accumulation_fusion=self.gradient_accumulation_fusion,
+                share_embeddings_and_output_weights=self.share_embeddings_and_output_weights
             )
         else:
             return lm_output
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py	2023-08-16 13:00:48
@@ -31,7 +31,7 @@
 from nemo.collections.nlp.modules.common.megatron.megatron_init import initialize_model_parallel_for_nemo
 from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer
 from nemo.collections.nlp.parts.nlp_overrides import GradScaler
-from nemo.core.optim import MainParamsOptimizerWrapper, prepare_lr_scheduler
+from nemo.core.optim import MainParamsOptimizerWrapperXLA, MainParamsOptimizerWrapper, prepare_lr_scheduler
 from nemo.utils import AppState, logging
 from nemo.utils.get_rank import is_global_rank_zero
 
@@ -130,11 +130,16 @@
 
         # buffer used during train_step for logging average loss over gradient accumulation steps
         self._reduced_loss_buffer = []
-        # import pdb; pdb.set_trace()
+        if os.environ.get("TORCHELASTIC_RUN_ID") is not None:
+            g_rank=int(os.environ.get("RANK"))
+            l_rank=int(os.environ.get("LOCAL_RANK"))
+        else:
+            g_rank = trainer.global_rank
+            l_rank = trainer.local_rank
         initialize_model_parallel_for_nemo(
             world_size=int(trainer.num_nodes)*int(trainer.num_devices),
-            global_rank=trainer.global_rank,
-            local_rank=trainer.local_rank,
+            global_rank=g_rank,
+            local_rank=l_rank,
             tensor_model_parallel_size=cfg.get('tensor_model_parallel_size', 1),
             pipeline_model_parallel_size=cfg.get('pipeline_model_parallel_size', 1),
             virtual_pipeline_model_parallel_size=cfg.get('virtual_pipeline_model_parallel_size', None),
@@ -149,7 +154,7 @@
         self._validate_and_override_config()
 
         self.grad_clip_pl_default = False  # use pytorch default for gradient clipping. Default False
-
+        self.wrap_with_zero = self.cfg.get("wrap_with_zero", False)
         if hasattr(self._cfg, "tokenizer") or (
             hasattr(self._cfg, "encoder_tokenizer") and hasattr(self._cfg, "decoder_tokenizer")
         ):
@@ -241,6 +246,7 @@
             merges_file=self.register_artifact("tokenizer.merge_file", self._cfg.tokenizer.merge_file),
             delimiter=self.cfg.tokenizer.get('delimiter', None),
             legacy=legacy,
+            use_fast=self.cfg.tokenizer.get('use_fast', False)
         )
 
     def on_train_start(self) -> None:
@@ -286,7 +292,8 @@
            We use gradient clipping implementation from megatron-lm.
         """
         clip_val = self.trainer.gradient_clip_val
-        if clip_val is None:
+        if clip_val is None or self.wrap_with_zero:
+            # Zero1 optimizer handles gradient clipping for us across TP groups
             return
 
         clip_val = float(clip_val)
@@ -412,47 +419,16 @@
                 # Assume FP32 params, so no need to store main params
                 optim_kwargs['store_params'] = False
 
-        return super().setup_optimization(optim_config=optim_config, optim_kwargs=optim_kwargs)
+        return super().setup_optimization(optim_config=optim_config, optim_kwargs=optim_kwargs,
+                                          wrap_with_zero=self.wrap_with_zero)
 
     def configure_optimizers(self):
         self.setup_optimization()
 
         # Wrap the baseline optimizer with the optimizer class with master parameters
         if self.megatron_amp_o2 and not self.with_distributed_adam and self._optimizer is not None:
-            if self.cfg.precision == 'bf16':
-                fp32_grad_accum = True
-                contiguous_grad_bucket = True
-            elif self.cfg.precision == 16:
-                fp32_grad_accum = False
-                # TODO: contiguous grad bucket for fp16 is also planned to be supported
-                contiguous_grad_bucket = False
-                raise ValueError(
-                    "fp16 training is not yet supported with O2. Please set megatron_amp_O2 to False in the model config."
-                )
-
-            # if using tensor parallel only, we automatically use async grad all-reduce
-            # if using pipeline parallel or sequence parallel or gradient accumulation fusion, then we disable it
-            if self.cfg.get('pipeline_model_parallel_size', 1) == 1 and not (
-                self.cfg.get('sequence_parallel', False) or self.cfg.get('gradient_accumulation_fusion', False)
-            ):
-                async_grad_allreduce = True
-            else:
-                async_grad_allreduce = False
-
-            if async_grad_allreduce:
-                # we need this to be configurable until make_nccl_premul_sum is in public PyTorch.
-                # currently cannot be imported in PyTorch 1.12.0
-                grad_div_ar_fusion = self.cfg.get('grad_div_ar_fusion', False)
-            else:
-                grad_div_ar_fusion = False
-
-            self._optimizer = MainParamsOptimizerWrapper(
+            self._optimizer = MainParamsOptimizerWrapperXLA(
                 self._optimizer,
-                fp32_grad_accum=fp32_grad_accum,
-                contiguous_grad_bucket=contiguous_grad_bucket,
-                async_grad_allreduce=async_grad_allreduce,
-                grad_div_ar_fusion=grad_div_ar_fusion,
-                grad_allreduce_chunk_size_mb=self.cfg.get('grad_allreduce_chunk_size_mb', 125),
             )
 
             assert self._trainer.max_steps is not None, "'max_steps' is missing in trainer config."
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py	2023-08-16 13:00:48
@@ -17,6 +17,8 @@
 
 import numpy as np
 import torch
+import time
+import math
 from omegaconf.dictconfig import DictConfig
 from pytorch_lightning.trainer.trainer import Trainer
 
@@ -27,7 +29,7 @@
 )
 from nemo.collections.nlp.models.language_modeling.megatron.gpt_model import GPTModel
 from nemo.collections.nlp.models.language_modeling.megatron_base_model import MegatronBaseModel
-from nemo.collections.nlp.modules.common.megatron.module import Float16Module
+from nemo.collections.nlp.modules.common.megatron.module import Float16Module, param_is_not_shared
 from nemo.collections.nlp.modules.common.megatron.utils import (
     average_losses_across_data_parallel_group,
     get_all_params_for_weight_decay_optimization,
@@ -50,6 +52,7 @@
 from nemo.core.classes.common import PretrainedModelInfo
 from nemo.utils import logging
 from transformers.utils import is_torch_tpu_available
+import queue
 
 try:
     from apex.transformer import parallel_state
@@ -61,6 +64,8 @@
         _forward_backward_pipelining_with_interleaving,
     )
     from apex.transformer.pipeline_parallel.schedules.fwd_bwd_no_pipelining import forward_backward_no_pipelining
+    from apex.transformer.tensor_parallel.layers import param_is_not_tensor_parallel_duplicate
+    from apex.transformer.pipeline_parallel.utils import get_num_microbatches
 
     HAVE_APEX = True
 except (ImportError, ModuleNotFoundError):
@@ -82,6 +87,33 @@
     HAVE_XLA = False
 
 
+class Throughput:
+    def __init__(self, moving_avg_window_size):
+        self.seqs_per_iteration = None #batch_size * world_size * grad_accum_usteps
+        self.moving_avg_window_size = moving_avg_window_size
+        self.moving_avg_window = queue.Queue()
+        self.window_time = 0
+        self.start_time = time.time()
+        self.throughput_peak = 0
+        self.throughput_sum = 0
+        self.throughputs = []
+
+    def set_seqs_per_iteration(self, batch_size, world_size, grad_accum_usteps):
+        self.seqs_per_iteration = batch_size * world_size * grad_accum_usteps
+
+    def get_throughput(self):
+        step_time = time.time() - self.start_time
+        self.start_time += step_time
+        self.window_time += step_time
+        self.moving_avg_window.put(step_time)
+        window_size = self.moving_avg_window.qsize()
+        if window_size > self.moving_avg_window_size:
+            self.window_time -= self.moving_avg_window.get()
+            window_size -= 1
+        throughput = window_size * self.seqs_per_iteration / self.window_time
+        self.throughputs.append(throughput)
+        return throughput
+
 class MegatronGPTModel(MegatronBaseModel, TextGeneration):
     """
     Megatron GPT pretraining
@@ -116,7 +148,14 @@
 
         # configuration used for inference
         self._inference_config = None
+
+        self.wrap_with_zero = cfg.get('wrap_with_zero', False)
+        self.log_parameter_norm = cfg.get('log_parameter_norm', False)
+        self.log_gradient_norm = cfg.get('log_gradient_norm', False)
+        self.save_logits = cfg.get('save_logits', False)
+        self.save_logits_interval = cfg.get('save_logits_interval', 0)
     
+        self.throughput = Throughput(100) 
     def _build_model(self):
         # build_model returns a list of modules which are used for interleaved pipeline parallelism
         self.model = build_model(
@@ -130,7 +169,9 @@
             self.model = self.model[0]
 
         if self.megatron_amp_o2:
-
+            #We use XLA_USE_DOWNCAST=1 to downcast the model weights constructed in fp32
+            #Not doing explicit model casting below
+            return 
             if not self.with_distributed_adam:
                 # Pre-allocate the model on GPU to have master parameters allocated on the same device with matching data type
                 if isinstance(self.model, list):
@@ -190,6 +231,11 @@
             bias_activation_fusion=self.cfg.get('bias_activation_fusion', True),
             bias_dropout_add_fusion=self.cfg.get('bias_dropout_add_fusion', True),
             share_embeddings_and_output_weights=self.cfg.get('share_embeddings_and_output_weights', True),
+            position_embedding_type=self.cfg.get('position_embedding_type', 'learned_absolute'),
+            rotary_percentage=self.cfg.get('rotary_percentage', 1.0),
+            activation=self.cfg.get('activation', 'gelu'),
+            bias=self.cfg.get('has_bias', True),
+            transformer_block_type=self.cfg.get('transformer_block_type','pre_ln'),
             masked_softmax_fusion=self.cfg.get('masked_softmax_fusion', True),
             gradient_accumulation_fusion=self.cfg.get('gradient_accumulation_fusion', False),
             persist_layer_norm=self.cfg.get('persist_layer_norm', False),
@@ -203,6 +249,7 @@
             fp8_amax_history_len=self.cfg.get('fp8_amax_history_len', 1),
             fp8_amax_compute_algo=self.cfg.get('fp8_amax_compute_algo', 'most_recent'),
             use_emha=self.cfg.get('use_emha', False),
+            save_logits=self.cfg.get('save_logits', False),
         )
 
         return model
@@ -277,6 +324,8 @@
             Microbatches are then moved to GPU during the pipeline.
             The list of microbatches is then piped through the pipeline using Apex fwd/bwd functions.
         """
+        # Log start time of the training loop 
+        start_time = time.time()
 
         # we zero grads here because we also call backward in the apex fwd/bwd functions
         self._optimizer.zero_grad()
@@ -297,17 +346,14 @@
                 custom_sync_context_handler = lambda: self._optimizer.no_sync(greedy_grad_copy=False)
             custom_grad_sync_func = self.reduce_overlap_gradients
         else:
-            if self.megatron_amp_o2 and not self.cfg.get('sequence_parallel', False):
-                custom_sync_context_handler = self._optimizer.no_sync
-            else:
-                # TODO: enable async grad all reduce for O1/autocast mixed precision training
-                custom_sync_context_handler = None
+            #XLA doesn;t need an async context handler 
+            custom_sync_context_handler = None
 
         # run forward and backwards passes for an entire global batch
         # we do this inside training_step to support pipeline parallelism
         fwd_bwd_function = self._get_fwd_bwd_function()
 
-        losses_reduced_per_micro_batch = fwd_bwd_function(
+        losses_per_micro_batch = fwd_bwd_function(
             forward_step_func=self.get_forward_output_and_loss_func(),
             batch=batch_for_pipeline,
             model=self.model,
@@ -326,9 +372,9 @@
 
         xm.mark_step()
         # only the last stages of the pipeline return losses
-        if losses_reduced_per_micro_batch:
+        if losses_per_micro_batch:
             # average loss across micro batches
-            loss_tensors_list = [loss_reduced['avg'] for loss_reduced in losses_reduced_per_micro_batch]
+            loss_tensors_list = [average_losses_across_data_parallel_group([loss['mb_loss']]) for loss in losses_per_micro_batch]
             loss_tensor = torch.concat(loss_tensors_list)
             loss_mean = loss_tensor.mean()
         else:
@@ -342,30 +388,33 @@
         if self.cfg.get('tensor_model_parallel_size', 1) > 1 and self.cfg.get('sequence_parallel', False):
             self.allreduce_sequence_parallel_gradients()
 
-        if self.with_distributed_adam:
+        xm.mark_step()
+
+        if self.with_distributed_adam or self.wrap_with_zero:
             # gradients are reduced internally in distributed optimizer
             pass
         elif self.megatron_amp_o2:
             # when using pipeline parallelism grads must be all-reduced after the pipeline (not asynchronously)
-            if self.cfg.get('pipeline_model_parallel_size', 1) > 1 or self.cfg.get('sequence_parallel', False):
-                # main grads are stored in the MainParamsOptimizer wrapper
-                self._optimizer.allreduce_main_grads()
+            self.allreduce_gradients()
         else:
             # async grad allreduce is not currently implemented for O1/autocasting mixed precision training
             # so we all-reduce gradients after the pipeline
             self.allreduce_gradients()  # @sangkug we think this is causing memory to blow up (hurts perf)
 
+        xm.mark_step()
+
         if self.cfg.get('pipeline_model_parallel_size', 1) > 1 and self.cfg.get('share_embeddings_and_output_weights', True):
             # when using pipeline parallelism the first and last stage must keep embeddings in sync
             self.allreduce_first_last_embeddings()
 
         xm.mark_step()
-
         ## logging
         # we can only log on one rank if it is rank zero so we broadcast from last rank
         # we can avoid this broadcast by updating the PTL log function to accept specific ranks
         torch.distributed.all_reduce(loss_mean, group=parallel_state.get_pipeline_model_parallel_group())
 
+        xm.mark_step()
+
         if self.cfg.precision == 16:
             loss_scale = self.trainer.precision_plugin.scaler._scale
             if loss_scale is not None:
@@ -376,15 +425,147 @@
         lr = self._optimizer.param_groups[0]['lr']
         # TODO: make sure compute_consumed_samples works for pipeline parallelism
         consumed_samples = self.compute_consumed_samples(self.trainer.global_step - self.init_global_step)
-        def _log_metrics(log_fn, loss_mean, lr, global_step, consumed_samples):
+        
+        elapsed_time = time.time() - start_time
+        if self.throughput.seqs_per_iteration is None:          
+            self.throughput.set_seqs_per_iteration(self.cfg.get('micro_batch_size'), parallel_state.get_data_parallel_world_size(), get_num_microbatches())
+        throughput = self.throughput.get_throughput()   
+        throughput_peak = self.throughput.throughput_peak
+        if throughput > throughput_peak:
+            self.throughput.throughput_peak = throughput
+        self.throughput.throughput_sum += throughput
+        param_norm = None
+        grad_norm = None
+        if self.log_parameter_norm:
+            param_norm = self.calculate_parameter_norm(self.parameters())
+        if self.log_gradient_norm:
+            grad_norm = self.calculate_gradient_norm(self.parameters())
+        def _log_metrics(log_fn, loss_mean, lr, global_step, consumed_samples, grad_norm, param_norm, throughput, throughput_peak):
             log_fn('reduced_train_loss', loss_mean.cpu(), prog_bar=True, rank_zero_only=True)
             log_fn('lr', lr, rank_zero_only=True)
+            if grad_norm:
+                log_fn('gradient_norm', grad_norm.detach().cpu(), prog_bar=True, rank_zero_only=True)
+            if param_norm:
+                log_fn('parameter_norm', param_norm.detach().cpu(), prog_bar=True, rank_zero_only=True)
             log_fn('global_step', global_step, prog_bar=True, rank_zero_only=True)
             log_fn('consumed_samples', consumed_samples, prog_bar=True, rank_zero_only=True)
-        xm.add_step_closure(_log_metrics, (self.log, loss_mean.detach(), lr, float(self.trainer.global_step), float(consumed_samples),))
+            log_fn('iteration_time', elapsed_time, prog_bar=True, rank_zero_only=True)
+            log_fn('throughput', throughput, prog_bar=False, rank_zero_only=True)
+            log_fn('throughput_peak', throughput_peak, prog_bar=False, rank_zero_only=True)
+        xm.add_step_closure(_log_metrics, (self.log, loss_mean.detach(), lr, float(self.trainer.global_step), float(consumed_samples), grad_norm, param_norm, throughput, throughput_peak))
 
         return loss_mean
 
+    def calculate_gradient_norm(self, parameters, norm_type=2):
+        """Calculate gradient norms across model parallel ranks
+        Arguments:
+            parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
+                single Tensor that will have gradients normalized
+            norm_type (float or int): type of the used p-norm. Can be ``'math.inf'`` for
+                infinity norm.
+        Returns:
+            Total norm of the gradients (viewed as a single vector).
+        """
+        if isinstance(parameters, torch.Tensor):
+            parameters = [parameters]
+
+        # Filter parameters based on:
+        #   - grad should not be none
+        #   - parameter should not be shared
+        #   - should not be a replica due to tensor model parallelism
+        grads_for_norm = []
+        for param in parameters:
+            grad_not_none = param.grad is not None
+            is_not_shared = param_is_not_shared(param)
+            is_not_tp_duplicate = param_is_not_tensor_parallel_duplicate(param)
+            if grad_not_none and is_not_shared and is_not_tp_duplicate:
+                grads_for_norm.append(param.grad.detach())
+
+        if not grads_for_norm:
+            raise ValueError(f"No grads found on {xm.get_ordinal()}")
+
+        # Norm parameters.
+        norm_type = float(norm_type)
+        total_norm = torch.tensor([float(0.0)], device=xm.xla_device())
+
+        # Calculate norm.
+        if norm_type == math.inf:
+            total_norm = max(grad.abs().max() for grad in grads_for_norm)
+            total_norm = torch.tensor([float(total_norm)], device=xm.xla_device())
+            # Take max across all model-parallel TPUs.
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.MAX, group=parallel_state.get_tensor_model_parallel_group()
+            )
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.MAX, group=parallel_state.get_pipeline_model_parallel_group()
+            )
+            total_norm = total_norm[0]
+        else:
+            for grad in grads_for_norm:
+                grad_norm = torch.norm(grad, norm_type)
+                total_norm += grad_norm ** norm_type
+
+            # Sum across all model-parallel TPUs.
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.SUM, group=parallel_state.get_tensor_model_parallel_group()
+            )
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.SUM, group=parallel_state.get_pipeline_model_parallel_group()
+            )
+            total_norm = torch.pow(total_norm, 1.0 / norm_type)
+        return total_norm
+
+    def calculate_parameter_norm(self, parameters, norm_type=2):
+        """Calculate parameter norms across model parallel ranks
+        Arguments:
+            parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
+                single Tensor
+            norm_type (float or int): type of the used p-norm. Can be ``'math.inf'`` for
+                infinity norm.
+            Total norm of the parameters (viewed as a single vector).
+        """
+        if isinstance(parameters, torch.Tensor):
+            parameters = [parameters]
+        # Norm parameters.
+        norm_type = float(norm_type)
+        total_norm = torch.tensor([float(0.0)], device=xm.xla_device())
+        params_to_norm = []
+
+        # Filter parameters based on:
+        #   - parameter should not be shared
+        #   - should not be a replica due to tensor model parallelism
+        for param in parameters:
+            is_not_shared = param_is_not_shared(param)
+            is_not_tp_duplicate = param_is_not_tensor_parallel_duplicate(param)
+            if is_not_shared and is_not_tp_duplicate:
+                params_to_norm.append(param)
+
+        # Calculate norm.
+        if norm_type == math.inf:
+            total_norm = max(torch.abs(param) for param in params_to_norm)
+            total_norm = torch.tensor([float(total_norm)], device=xm.xla_device())
+            # Take max across all model-parallel TPUs.
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.MAX, group=parallel_state.get_tensor_model_parallel_group()
+            )
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.MAX, group=parallel_state.get_pipeline_model_parallel_group()
+            )
+            total_norm = total_norm[0]
+        else:
+            for param in params_to_norm:
+                param_norm = torch.norm(param, norm_type)
+                total_norm += param_norm**norm_type
+            # Sum across all model-parallel TPUs.
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.SUM, group=parallel_state.get_tensor_model_parallel_group()
+            )
+            torch.distributed.all_reduce(
+                total_norm, op=torch.distributed.ReduceOp.SUM, group=parallel_state.get_pipeline_model_parallel_group()
+            )
+            total_norm = torch.pow(total_norm, 1.0 / norm_type)
+        return total_norm
+
     def backward(self, *args, **kwargs):
         """ LightningModule hook to do backward.
             We want this to do nothing since we run backward in the fwd/bwd functions from apex.
@@ -407,10 +588,8 @@
             else:
                 sequence_parallel_param = getattr(param, 'sequence_parallel_enabled', False)
             if sequence_parallel_param:
-                if self.megatron_amp_o2:
-                    grad = param.main_grad
-                else:
-                    grad = param.grad
+                #megatron_amp_o2 also uses model gradients 
+                grad = param.grad
                 grads.append(grad.data)
 
     def allreduce_sequence_parallel_gradients(self):
@@ -454,14 +633,12 @@
                     module = self.model
             if module.share_token_embeddings:
                 word_embeddings_weight = module.word_embeddings_weight()
-                if self.megatron_amp_o2:
-                    # O2 recipe stores a "main" copy of weights and grads
-                    grad = word_embeddings_weight.main_grad
-                else:
-                    grad = word_embeddings_weight.grad
+
+                #megatron_amp_o2 also uses model gradients
+                grad = word_embeddings_weight.grad
                 torch.distributed.all_reduce(grad, group=parallel_state.get_embedding_group())
 
-    def get_forward_output_and_loss_func(self, validation_step=False):
+    def get_forward_output_and_loss_func(self, validation_step=False, all_reduce_losses=False):
         def fwd_output_and_loss_func(batch, model, checkpoint_activations_all_layers=None):
             if parallel_state.get_pipeline_model_parallel_world_size() == 1:
                 # batch = [x.cuda(non_blocking=True) for x in batch]
@@ -489,15 +666,33 @@
             attention_mask = batch[3][0:1]
             if is_torch_tpu_available():
                 attention_mask = None
+            logits = None
+            if parallel_state.is_pipeline_last_stage():
+                # Only the last PP stage has the logits
+                output_tensor, logits = model(
+                    tokens,
+                    position_ids,
+                    attention_mask,
+                    labels,
+                    checkpoint_activations_all_layers=checkpoint_activations_all_layers,
+                )
+            else:
+                output_tensor = model(
+                    tokens,
+                    position_ids,
+                    attention_mask,
+                    labels,
+                    checkpoint_activations_all_layers=checkpoint_activations_all_layers,
+                )
+            if self.save_logits:
+                def save_logits(logits):
+                    # Save logits on tensor parallel rank zero, data parallel rank zero and last pipeline parallel stage
+                    if parallel_state.get_tensor_model_parallel_rank() == 0 and parallel_state.is_pipeline_last_stage()\
+                            and parallel_state.get_data_parallel_rank() == 0:
+                        if self.trainer.global_step % self.save_logits_interval == 0:
+                            np.save(f"logits-{self.trainer.global_step}.npy", logits.detach().cpu().numpy())
+                xm.add_step_closure(save_logits, (logits,))
 
-            output_tensor = model(
-                tokens,
-                position_ids,
-                attention_mask,
-                labels,
-                checkpoint_activations_all_layers=checkpoint_activations_all_layers,
-            )
-
             def loss_func(output_tensor):
                 loss_for_mb = self.loss_func(loss_mask, output_tensor)
                 if validation_step and not self.cfg.data.get('validation_drop_last', True):
@@ -515,8 +710,11 @@
                     )
                     return loss_for_mb, {'loss_sum_and_mb_size': loss_sum_and_mb_size_all_gpu.detach()}
                 else:
-                    reduced_loss = average_losses_across_data_parallel_group([loss_for_mb])
-                    return loss_for_mb, {'avg': reduced_loss.detach()}
+                    if all_reduce_losses:
+                        reduced_loss = average_losses_across_data_parallel_group([loss_for_mb])
+                        return loss_for_mb, {'avg': reduced_loss.detach()}
+                    else:
+                        return loss_for_mb, {'mb_loss': loss_for_mb.detach()}
 
             return output_tensor, loss_func
 
@@ -569,7 +767,7 @@
         # we do this inside validation_step to support pipeline parallelism
         fwd_bwd_function = self._get_fwd_bwd_function()
 
-        losses_reduced_per_micro_batch = fwd_bwd_function(
+        losses_per_micro_batch = fwd_bwd_function(
             forward_step_func=self.get_forward_output_and_loss_func(validation_step=True),
             batch=batch_for_pipeline,
             model=self.model,
@@ -581,16 +779,16 @@
         )
 
         # only the last stage of the pipeline returns losses
-        if losses_reduced_per_micro_batch:
+        if losses_per_micro_batch:
             if self.cfg.data.get('validation_drop_last', True):
                 # average loss across micro batches
-                loss_tensors_list = [loss_reduced['avg'] for loss_reduced in losses_reduced_per_micro_batch]
+                loss_tensors_list = [average_losses_across_data_parallel_group([loss['mb_loss']]) for loss in losses_per_micro_batch]
                 return torch.concat(loss_tensors_list).mean()
             else:
                 # Get the total loss since micro batches sizes are not uniform
                 loss_sum_tensors_list = [
                     loss_sum['loss_sum_and_mb_size']
-                    for loss_sum in losses_reduced_per_micro_batch
+                    for loss_sum in losses_per_micro_batch
                     if loss_sum['loss_sum_and_mb_size'][1] > 0
                 ]
                 loss_sum = (
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/clip_grads.py nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/clip_grads.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/clip_grads.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/clip_grads.py	2023-08-16 13:00:48
@@ -17,7 +17,10 @@
 import itertools
 
 import torch
-from torch._six import inf
+if torch.__version__.startswith('2'):
+    from torch import inf
+else:
+    from torch._six import inf
 
 from nemo.collections.nlp.modules.common.megatron.module import param_is_not_shared
 
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/language_model.py nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/language_model.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/language_model.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/language_model.py	2023-08-16 13:00:48
@@ -27,7 +27,7 @@
 )
 
 try:
-    from apex.transformer import tensor_parallel
+    from apex.transformer import parallel_state, tensor_parallel
     from apex.transformer.enums import AttnMaskType
 
     HAVE_APEX = True
@@ -491,6 +491,7 @@
         self.output_layer_init_method = output_layer_init_method
         self.position_embedding_type = position_embedding_type
         self.share_embeddings_and_output_weights = share_embeddings_and_output_weights
+        self.sequence_parallel = sequence_parallel
 
         if kv_channels is None:
 
@@ -514,12 +515,14 @@
                 fp32_residual_connection=fp32_residual_connection,
             )
             self._embedding_key = 'embedding'
-        if position_embedding_type == 'rope':
-            rotary_dim = self.hidden_size // num_attention_heads if kv_channels is None else kv_channels
-            assert 0 < rotary_percentage <= 1
-            if rotary_percentage < 1:
-                rotary_dim = int(rotary_dim * rotary_percentage)
-            self.rotary_pos_emb = RotaryEmbedding(rotary_dim)
+        # Move Rope to core attention
+        # TODO: check perf penalty vs original Nemo implementation
+        # if position_embedding_type == 'rope':
+        #     rotary_dim = self.hidden_size // num_attention_heads if kv_channels is None else kv_channels
+        #     assert 0 < rotary_percentage <= 1
+        #     if rotary_percentage < 1:
+        #         rotary_dim = int(rotary_dim * rotary_percentage)
+        #     self.rotary_pos_emb = RotaryEmbedding(rotary_dim)
         # Transformer.
         self.encoder = ParallelTransformer(
             init_method=self.init_method,
@@ -570,6 +573,7 @@
             fp8_amax_compute_algo=fp8_amax_compute_algo,
             reduce_amax=reduce_amax,
             use_emha=use_emha,
+            position_embedding_type=self.position_embedding_type
         )
         self._encoder_key = 'encoder'
 
@@ -619,12 +623,21 @@
                 self._pooler_key = 'pooler'
 
             if not self.share_embeddings_and_output_weights:
+                no_async_tensor_model_parallel_allreduce = (
+                    parallel_state.get_tensor_model_parallel_world_size() == 1 or sequence_parallel
+                )
                 self.output_layer = tensor_parallel.ColumnParallelLinear(
                     self.hidden_size,
                     self.vocab_size,
                     bias=False,  # Setting bias to False always to keep it consistent with embedding tying that also does not have a bias.
                     init_method=self.init_method,
-                    use_cpu_initialization=use_cpu_initialization
+                    skip_bias_add=True,
+                    use_cpu_initialization=use_cpu_initialization,
+                    gather_output=False,
+                    sequence_parallel_enabled=sequence_parallel,
+                    no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
+                    gradient_accumulation_fusion=gradient_accumulation_fusion,
+                    transfer_with_static_ring=(not (activations_checkpoint_granularity=="selective")),
                 )
                 self._output_layer_key = 'output_layer'
 
@@ -666,16 +679,25 @@
         # encoder_input: [s, b, h]
 
         # enc_attn_mask: [1, 1, s, s]
-
-        if self.position_embedding_type == 'rope':
-            if inference_max_sequence_len is not None:
-                rotary_pos_emb = self.rotary_pos_emb(inference_max_sequence_len)
-            elif self.encoder.input_tensor is not None:
-                rotary_pos_emb = self.rotary_pos_emb(self.encoder.input_tensor.size(0))
-            else:
-                rotary_pos_emb = self.rotary_pos_emb(encoder_input.size(0))
-        else:
-            rotary_pos_emb = None
+        # Move rope to core attention
+        # if self.position_embedding_type == 'rope':
+        #     if inference_max_sequence_len is not None:
+        #         rotary_pos_emb = self.rotary_pos_emb(inference_max_sequence_len)
+        #     elif self.encoder.input_tensor is not None:
+        #         if self.sequence_parallel:
+        #             rotary_pos_emb = self.rotary_pos_emb(
+        #                 self.encoder.input_tensor.size(0) * parallel_state.get_tensor_model_parallel_world_size()
+        #             )
+        #         else:
+        #             rotary_pos_emb = self.rotary_pos_emb(self.encoder.input_tensor.size(0))
+        #     else:
+        #         if self.sequence_parallel:
+        #             rotary_pos_emb = self.rotary_pos_emb(
+        #                 encoder_input.size(0) * parallel_state.get_tensor_model_parallel_world_size()
+        #             )
+        #         else:
+        #             rotary_pos_emb = self.rotary_pos_emb(encoder_input.size(0))
+        rotary_pos_emb = None
         # encoder.
         if enc_hidden_states is None:
             encoder_output = self.encoder(
@@ -694,6 +716,8 @@
             encoder_output = enc_hidden_states.to(encoder_input.dtype)
 
         if self.post_process:
+            if not self.share_embeddings_and_output_weights:
+                encoder_output, _ = self.output_layer(encoder_output)
             if self.add_pooler:
                 pooled_output = self.pooler(encoder_output, pooling_sequence_index)
 
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/megatron_init.py nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/megatron_init.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/megatron_init.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/megatron_init.py	2023-08-16 13:00:48
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 import random
+import os
 
 import numpy as np
 import torch
@@ -208,13 +209,13 @@
     model_parallel_size = tensor_model_parallel_size * pipeline_model_parallel_size
 
     assert (
-        tensor_model_parallel_size in [1, 8 ,32]
-    ), f'tensor_model_parallel_size: {tensor_model_parallel_size} must be 1, 8, or 32.'
-
-    assert (
         world_size % tensor_model_parallel_size * pipeline_model_parallel_size == 0
     ), f'world_size: {world_size} must be divisible by tensor_model_parallel_size: {tensor_model_parallel_size} times pipeline_model_parallel_size {pipeline_model_parallel_size}'
     data_parallel_size = world_size // (tensor_model_parallel_size * pipeline_model_parallel_size)
+
+    assert (
+        tensor_model_parallel_size in [1, 8 ,32]
+    ), f'tensor_model_parallel_size: {tensor_model_parallel_size} must be 1, 8, or 32.'
 
     if pipeline_model_parallel_size > 1:
         if tensor_model_parallel_size == 1:
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/transformer.py nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/transformer.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/modules/common/megatron/transformer.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/modules/common/megatron/transformer.py	2023-08-16 13:00:48
@@ -42,7 +42,7 @@
 from nemo.collections.nlp.modules.common.megatron.layer_norm_1p import LayerNorm1P
 from nemo.collections.nlp.modules.common.megatron.layer_type import LayerType
 from nemo.collections.nlp.modules.common.megatron.module import MegatronModule
-from nemo.collections.nlp.modules.common.megatron.rotary_pos_embedding import apply_rotary_pos_emb
+# from nemo.collections.nlp.modules.common.megatron.rotary_pos_embedding import apply_rotary_pos_emb
 from nemo.collections.nlp.modules.common.megatron.utils import ApexGuardDefaults, attention_mask_func, erf_gelu
 from nemo.collections.nlp.modules.common.megatron.utils import openai_gelu as openai_gelu_func
 from nemo.core import adapter_mixins
@@ -54,7 +54,8 @@
     from apex.transformer.enums import AttnMaskType, AttnType, ModelType
     from apex.transformer.utils import divide as safe_divide
     from apex.transformer.parallel_state import get_tensor_model_parallel_world_size
-    from apex.normalization import MixedFusedRMSNorm
+    # from apex.normalization import MixedFusedRMSNorm
+    from apex.transformer.layers.layer_norm import FastRMSNorm as MixedFusedRMSNorm
     from apex.transformer.functional.fused_softmax import FusedScaleMaskSoftmax
 
     HAVE_APEX = True
@@ -85,7 +86,52 @@
                 "Transformer Engine was not found. transformer_engine.pytorch.transformer.TransformerLayer will not work. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt."
             )
 
+class RotaryEmbedding(torch.nn.Module):
+    def __init__(self, dim, max_position_embeddings=4096, base=10000, device=None):
+        super().__init__()
+        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
+        self.register_buffer("inv_freq", inv_freq)
 
+        # Build here to make `torch.jit.trace` work.
+        self.max_seq_len_cached = max_position_embeddings
+        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
+        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
+        # Different from paper, but it uses a different permutation in order to obtain the same calculation
+        emb = torch.cat((freqs, freqs), dim=-1)
+        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
+        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)
+
+    def forward(self, x, seq_len=None):
+        # x: [bs, num_attention_heads, seq_len, head_size]
+        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.
+        if seq_len > self.max_seq_len_cached:
+            self.max_seq_len_cached = seq_len
+            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
+            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
+            # Different from paper, but it uses a different permutation in order to obtain the same calculation
+            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
+            self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
+            self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)
+        return (
+            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
+            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
+        )
+
+
+def rotate_half(x):
+    """Rotates half the hidden dims of the input."""
+    x1 = x[..., : x.shape[-1] // 2]
+    x2 = x[..., x.shape[-1] // 2:]
+    return torch.cat((-x2, x1), dim=-1)
+
+
+def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):
+    cos = cos[..., offset: q.shape[-2] + offset, :]
+    sin = sin[..., offset: q.shape[-2] + offset, :]
+    q_embed = (q * cos) + (rotate_half(q) * sin)
+    k_embed = (k * cos) + (rotate_half(k) * sin)
+    return q_embed, k_embed
+
 """ We use the following notation throughout this file:
      h: hidden size
      n: number of attention heads
@@ -129,6 +175,7 @@
         sequence_parallel=False,
         gradient_accumulation_fusion=False,
         dropout=0.0,
+        transfer_with_static_ring=True,
     ):
         super(ParallelMLP, self).__init__()
         self.activation = activation
@@ -159,6 +206,7 @@
             sequence_parallel_enabled=sequence_parallel,
             no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
             gradient_accumulation_fusion=gradient_accumulation_fusion,
+            transfer_with_static_ring=transfer_with_static_ring,
         )
 
         if activation in ['geglu', 'reglu', 'swiglu']:
@@ -175,6 +223,7 @@
                 sequence_parallel_enabled=sequence_parallel,
                 no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
 
         self.glu_activation_family = activation in ['geglu', 'reglu', 'swiglu']
@@ -221,6 +270,7 @@
             bias=bias,
             sequence_parallel_enabled=sequence_parallel,
             gradient_accumulation_fusion=gradient_accumulation_fusion,
+            transfer_with_static_ring=transfer_with_static_ring,
         )
 
         # Normformer normalization
@@ -237,7 +287,8 @@
                 )
             else:
                 self.normalization = MixedFusedRMSNorm(
-                    ffn_hidden_size // get_tensor_model_parallel_world_size(), layernorm_epsilon
+                    ffn_hidden_size // get_tensor_model_parallel_world_size(), layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel
                 )
 
     def forward(self, hidden_states):
@@ -439,6 +490,7 @@
         sequence_parallel=False,
         normalize_attention_scores=True,
         multi_query_attention=False,
+        position_embedding_type='learned_absolute'
     ):
 
         super(CoreAttention, self).__init__()
@@ -459,7 +511,7 @@
         # If True, will scale attention scores by 1 / sqrt(hidden_size_per_attention_head).
         # This arg is been provided mostly to support weight conversion of Huggingface models. (ex: T5v1.1)
         self.normalize_attention_scores = normalize_attention_scores
-
+        self.position_embedding_type = position_embedding_type
         if kv_channels is None:
             assert (
                 hidden_size % num_attention_heads == 0
@@ -493,6 +545,9 @@
             coeff,
         )
 
+        if self.position_embedding_type=='rope':
+            self.rotary_emb = RotaryEmbedding(self.hidden_size_per_attention_head)
+
         # Dropout. Note that for a single iteration, this layer will generate
         # different outputs on different number of parallel partitions but
         # on average it should not be partition dependent.
@@ -520,16 +575,30 @@
 
         # TODO: figure out how to do this
         # apply relative positional encoding (rotary embedding)
-        if rotary_pos_emb is not None:
-            q_pos_emb, k_pos_emb = rotary_pos_emb
+        # if rotary_pos_emb is not None:
+            # q_pos_emb, k_pos_emb = rotary_pos_emb
 
-            query_layer = apply_rotary_pos_emb(query_layer, q_pos_emb)
-            key_layer = apply_rotary_pos_emb(key_layer, k_pos_emb)
+            # query_layer = apply_rotary_pos_emb(query_layer, q_pos_emb)
+            # key_layer = apply_rotary_pos_emb(key_layer, k_pos_emb)
             # TODO, can apply positional embedding to value_layer so it has
             # absolute positional embedding.
             # otherwise, only relative positional embedding takes effect
             # value_layer = apply_rotary_pos_emb(value_layer, k_pos_emb)
 
+        if self.position_embedding_type=='rope':
+            # [sq, b, np, hn] --> [b, np, sq, hn] TODO optimize the permute of dimension back and forth
+            query_layer = query_layer.permute(1, 2, 0, 3)
+            key_layer = key_layer.permute(1, 2, 0, 3)
+            value_layer = value_layer.permute(1, 2, 0, 3)            
+
+            cos, sin = self.rotary_emb(value_layer, seq_len=query_layer.shape[2])
+            query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin, offset=0)
+
+            # [b, np, sq, hn] --> [sq, b, np, hn] TODO optimize the permute of dimension back and forth
+            query_layer = query_layer.permute(2, 0, 1, 3).contiguous()
+            key_layer = key_layer.permute(2, 0, 1, 3).contiguous()
+            value_layer = value_layer.permute(2, 0, 1, 3).contiguous()
+
         if self.multi_query_attention:
             # [sq, b, np, hn] -> [b, np * sq, hn]
             query_layer = query_layer.permute([1, 2, 0, 3]).reshape(
@@ -688,6 +757,7 @@
         sequence_parallel=False,
         gradient_accumulation_fusion=False,
         normalize_attention_scores=True,
+        transfer_with_static_ring=True,
     ):
         super(ParallelAttention, self).__init__()
 
@@ -733,6 +803,7 @@
                 sequence_parallel_enabled=sequence_parallel,
                 no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
         else:
             assert attention_type == AttnType.cross_attn
@@ -745,6 +816,7 @@
                 sequence_parallel_enabled=sequence_parallel,
                 no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
 
             self.key_value = tensor_parallel.ColumnParallelLinear(
@@ -756,6 +828,7 @@
                 sequence_parallel_enabled=sequence_parallel,
                 no_async_tensor_model_parallel_allreduce=no_async_tensor_model_parallel_allreduce,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
 
         self.core_attention = CoreAttention(
@@ -772,6 +845,7 @@
             multi_query_attention=multi_query_attention,
             sequence_parallel=sequence_parallel,
             normalize_attention_scores=normalize_attention_scores,
+            position_embedding_type=self.position_embedding_type
         )
 
         # Output.
@@ -785,6 +859,7 @@
             bias=bias,
             sequence_parallel_enabled=sequence_parallel,
             gradient_accumulation_fusion=gradient_accumulation_fusion,
+            transfer_with_static_ring=transfer_with_static_ring,
         )
 
         self.headscale = headscale
@@ -1327,7 +1402,7 @@
         if normalization not in ['layernorm', 'layernorm1p', 'rmsnorm']:
             raise ValueError(f'normalization must be "layernorm", "layernorm1p" or "rmsnorm", found {normalization}')
 
-        if transformer_block_type not in ['pre_ln', 'post_ln', 'normformer']:
+        if transformer_block_type not in ['pre_ln', 'post_ln', 'normformer', 'gpt_j']:
             raise ValueError(
                 f'transformer_block_type must be either "pre_ln" or "post_ln" or "normformer", found {transformer_block_type}'
             )
@@ -1337,6 +1412,12 @@
         self.attention_dropout = attention_dropout
         self.bias_dropout_add_fusion = bias_dropout_add_fusion  # if true, enable bias dropout fusion
 
+        self.checkpoint_layer_norm = (
+            activations_checkpoint_granularity == 'selective'
+        )  # transformer engine forward allows for more granular selective checkpointing
+        transfer_with_static_ring = not self.checkpoint_layer_norm # For now do not transfer with static ring
+        # when selective enabled to avoid memory pressure
+
         # Self attention.
         # retrieval_decoder_after_self_attn skips the self attention
         if self.layer_type != LayerType.retrieval_decoder_after_self_attn:
@@ -1350,7 +1431,8 @@
                     hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                 )
             else:
-                self.input_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                self.input_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
             self.self_attention = ParallelAttention(
                 init_method=init_method,
@@ -1376,6 +1458,7 @@
                 sequence_parallel=sequence_parallel,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
                 normalize_attention_scores=normalize_attention_scores,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
 
             if transformer_block_type == 'normformer':
@@ -1384,7 +1467,8 @@
                         hidden_size, layernorm_epsilon, persist_layer_norm
                     )
                 else:
-                    self.post_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                    self.post_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
             if self.layer_type != LayerType.decoder_pre_mlp or self.transformer_block_type != 'post_ln':
                 #  the post_attention_layernorm is used for layermorm after mlp
@@ -1398,7 +1482,8 @@
                         hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                     )
                 else:
-                    self.post_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                    self.post_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
         if self.layer_type == LayerType.decoder_pre_mlp:
             # skip MLP and cross attention
@@ -1417,7 +1502,8 @@
                     hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                 )
             else:
-                self.post_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                self.post_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
         if self.layer_type == LayerType.decoder or self.layer_type == LayerType.retrieval_encoder:
             self.inter_attention = ParallelAttention(
@@ -1441,6 +1527,7 @@
                 sequence_parallel=sequence_parallel,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
                 normalize_attention_scores=normalize_attention_scores,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
             # Normformer normalization
             if transformer_block_type == 'normformer':
@@ -1453,7 +1540,8 @@
                         hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                     )
                 else:
-                    self.post_inter_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                    self.post_inter_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
             # Layernorm on the attention output.
             if normalization == 'layernorm':
@@ -1465,7 +1553,8 @@
                     hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                 )
             else:
-                self.post_inter_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                self.post_inter_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
         elif (
             self.layer_type == LayerType.retrieval_decoder
             or self.layer_type == LayerType.retrieval_decoder_after_self_attn
@@ -1499,7 +1588,8 @@
                         hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                     )
                 else:
-                    self.post_inter_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                    self.post_inter_attention_normformer_norm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
             # Layernorm on the attention output.
             if normalization == 'layernorm':
@@ -1511,7 +1601,8 @@
                     hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                 )
             else:
-                self.post_inter_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                self.post_inter_attention_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
         # MLP
         if num_moe_experts > 1 and self.layer_number % moe_frequency == 0:
@@ -1554,6 +1645,7 @@
                 sequence_parallel=sequence_parallel,
                 gradient_accumulation_fusion=gradient_accumulation_fusion,
                 dropout=ffn_dropout,
+                transfer_with_static_ring=transfer_with_static_ring,
             )
 
     def _get_bias_droput_add_func(self, transformer_block_type='pre_ln', position_after='attention'):
@@ -1610,10 +1702,18 @@
             # Pre-LN: x -> LN -> MHA -> Residual -> LN -> MLP -> Residual
             # Post-LN: x -> MHA -> Residual -> LN -> MLP -> Residual -> LN
             # Normformer: x -> LN -> MHA -> LN -> Residual -> MLP (w/LN) -> Residual
+            # gpt_j: https://github.com/EleutherAI/gpt-neox/blob/303d7be582ae1c969347c25c54f568cc122445fc/megatron/model/transformer.py#L804-L847
+            #        x = x + MHA(Input_LN(x)) + MLP(Post_LN(x))
 
             residual = hidden_states
             # Layer norm at the beginning of the transformer layer.
             if self.transformer_block_type in ['pre_ln', 'normformer']:
+                if self.checkpoint_layer_norm:
+                    hidden_states = tensor_parallel.checkpoint(self.input_layernorm, False, hidden_states)
+                else:
+                    hidden_states = self.input_layernorm(hidden_states)
+            elif self.transformer_block_type in ['gpt_j']:
+                normalization_output = self.post_attention_layernorm(hidden_states)
                 hidden_states = self.input_layernorm(hidden_states)
 
             # Materialize attention mask right before use
@@ -1679,7 +1779,10 @@
                 layernorm_input = normalization_output
             elif self.transformer_block_type in ['pre_ln', 'normformer']:
                 # Layer norm post the self attention.
-                normalization_output = self.post_attention_layernorm(layernorm_input)
+                if self.checkpoint_layer_norm:
+                    normalization_output = tensor_parallel.checkpoint(self.post_attention_layernorm, False, layernorm_input)
+                else:
+                    normalization_output = self.post_attention_layernorm(layernorm_input)
         else:
             layernorm_input, normalization_output = hidden_states
 
@@ -1740,15 +1843,23 @@
         # MLP.
         mlp_output, mlp_bias = self.mlp(normalization_output)
 
-        residual = layernorm_input
+        if self.transformer_block_type in ['gpt_j']:
+            bias_dropout_add_func = self._get_bias_droput_add_func(
+                transformer_block_type=self.transformer_block_type, position_after='mlp'
+            )
+            # x = layernorm_input + MLP(Post_LN(x))
+            output = bias_dropout_add_func(mlp_output, mlp_bias, layernorm_input, self.hidden_dropout)
 
-        bias_dropout_add_func = self._get_bias_droput_add_func(
-            transformer_block_type=self.transformer_block_type, position_after='mlp'
-        )
+        else:
+            residual = layernorm_input
 
-        output = bias_dropout_add_func(mlp_output, mlp_bias, residual, self.hidden_dropout)
-        # print(f"Layer: {self.layer_number} MLP + Dropout + Residual checksum {output.sum()}")
+            bias_dropout_add_func = self._get_bias_droput_add_func(
+                transformer_block_type=self.transformer_block_type, position_after='mlp'
+            )
 
+            output = bias_dropout_add_func(mlp_output, mlp_bias, residual, self.hidden_dropout)
+            # print(f"Layer: {self.layer_number} MLP + Dropout + Residual checksum {output.sum()}")
+
         if self.transformer_block_type == 'post_ln':
             output = self.post_attention_layernorm(output)
 
@@ -2248,6 +2359,7 @@
                     num_moe_experts=num_moe_experts,
                     moe_frequency=moe_frequency,
                     moe_dropout=moe_dropout,
+                    position_embedding_type=self.position_embedding_type
                 )
 
         if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:
@@ -2299,7 +2411,8 @@
                     hidden_size, layernorm_epsilon, sequence_parallel_enabled=sequence_parallel
                 )
             else:
-                self.final_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon)
+                self.final_layernorm = MixedFusedRMSNorm(hidden_size, layernorm_epsilon,
+                    sequence_parallel_enabled=sequence_parallel)
 
     def _get_layer(self, layer_number):
         return self.layers[layer_number]
diff -urN neuronx-nemo-megatron/nemo/nemo/collections/nlp/parts/nlp_overrides.py nemo-updates/nemo/nemo/collections/nlp/parts/nlp_overrides.py
--- neuronx-nemo-megatron/nemo/nemo/collections/nlp/parts/nlp_overrides.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/collections/nlp/parts/nlp_overrides.py	2023-08-16 13:00:48
@@ -26,17 +26,22 @@
 import torch
 import torch.multiprocessing as mp
 from torch import Tensor
+from torch.nn import Module
+from torch.optim.optimizer import Optimizer
 from torchmetrics import Metric
 from lightning_lite.plugins import ClusterEnvironment, XLACheckpointIO
+from lightning_lite.plugins.environments import XLAEnvironment
 from lightning_lite.utilities.types import _PATH
 from lightning_lite.strategies.launchers.xla import _rank_teardown
 from lightning_utilities.core.apply_func import apply_to_collection, apply_to_collections
 from lightning_utilities.core.imports import RequirementCache
+from lightning_lite.accelerators.tpu import _XLA_AVAILABLE
 from omegaconf import OmegaConf
 from pytorch_lightning.profilers import Profiler
 from pytorch_lightning.overrides import LightningDistributedModule
 from pytorch_lightning.plugins import PLUGIN_INPUT
 from pytorch_lightning.plugins.io.checkpoint_plugin import CheckpointIO
+from pytorch_lightning.plugins.precision import PrecisionPlugin
 from pytorch_lightning.plugins.precision.native_amp import NativeMixedPrecisionPlugin
 from pytorch_lightning.strategies import DDPStrategy, TPUSpawnStrategy, Strategy
 from pytorch_lightning.strategies.launchers.xla import _XLALauncher
@@ -44,30 +49,24 @@
 from pytorch_lightning.utilities.exceptions import MisconfigurationException
 from pytorch_lightning.utilities.fetching import DataFetcher
 from pytorch_lightning.loops.utilities import _block_parallel_sync_behavior
-from pytorch_lightning.trainer.states import RunningStage, TrainerFn
 from pytorch_lightning.utilities.argparse import _defaults_from_env_vars
 from pytorch_lightning.trainer.connectors.accelerator_connector import AcceleratorConnector, _LITERAL_WARN
 from pytorch_lightning.trainer.connectors.checkpoint_connector import CheckpointConnector
 
 from pytorch_lightning.trainer import call, setup
-from lightning_lite.utilities.types import _PATH
-from pytorch_lightning.accelerators import Accelerator, TPUAccelerator
-from pytorch_lightning.callbacks import Callback, Checkpoint, EarlyStopping, ProgressBarBase
-from pytorch_lightning.callbacks.prediction_writer import BasePredictionWriter
-from pytorch_lightning.core.datamodule import LightningDataModule
+from pytorch_lightning.accelerators import Accelerator
+from pytorch_lightning.callbacks import Callback, Checkpoint
 from pytorch_lightning.loggers import Logger
-from pytorch_lightning.loggers.tensorboard import TensorBoardLogger
-from pytorch_lightning.loops import Loop, PredictionLoop, TrainingEpochLoop, TrainingBatchLoop, OptimizerLoop
+from pytorch_lightning.loops import PredictionLoop, TrainingEpochLoop, TrainingBatchLoop, OptimizerLoop
 from pytorch_lightning.loops.dataloader.evaluation_loop import EvaluationLoop
 from pytorch_lightning.loops.fit_loop import FitLoop
-from pytorch_lightning.loops.utilities import _parse_loop_limits, _reset_progress
-from pytorch_lightning.trainer.states import RunningStage, TrainerFn, TrainerState, TrainerStatus
+from pytorch_lightning.trainer.states import TrainerFn, TrainerState
 from pytorch_lightning.trainer.connectors.data_connector import DataConnector
 from pytorch_lightning.trainer.connectors.logger_connector import LoggerConnector
-from pytorch_lightning.trainer.connectors.logger_connector.result import _OUT_DICT, _PBAR_DICT, _ResultCollection, _ResultMetric, _ResultMetricCollection
+from pytorch_lightning.trainer.connectors.logger_connector.result import _ResultCollection, _ResultMetric, _ResultMetricCollection
 from pytorch_lightning.trainer.connectors.signal_connector import SignalConnector
 from pytorch_lightning.trainer.connectors.callback_connector import CallbackConnector
-from pytorch_lightning.tuner.tuning import _TunerResult, Tuner
+from pytorch_lightning.tuner.tuning import Tuner
 
 from torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks import noop_hook
 from torch.nn.parallel import DistributedDataParallel
@@ -77,6 +76,7 @@
 from nemo.core.optim import MainParamsOptimizerWrapper
 from nemo.utils import AppState, logging
 from nemo.utils.model_utils import inject_model_parallel_rank
+from nemo.utils.cast_utils import cast_all
 
 try:
     from apex.transformer import parallel_state
@@ -104,16 +104,23 @@
             **rning AMI GPU PyTorch 1.13.1 (Ubuntu 20.04) 20230519
             kwargs: Optional keyword arguments to be passed to the given function.
         """
-        context = mp.get_context(self._start_method)
-        return_queue = context.SimpleQueue()
-        import torch_xla.distributed.xla_multiprocessing as xmp
     
-        xmp.spawn(
-            self._wrapping_function,
-            args=(trainer, function, args, kwargs, return_queue),
-            nprocs=self._strategy.num_processes,
-            start_method=self._start_method,
-        )
+        if not self._strategy.cluster_environment.creates_processes_externally:
+            context = mp.get_context(self._start_method)
+            return_queue = context.SimpleQueue()
+            import torch_xla.distributed.xla_multiprocessing as xmp
+            xmp.spawn(
+                    self._wrapping_function,
+                    args=(trainer, function, args, kwargs, return_queue),
+                    nprocs=self._strategy.num_processes,
+                    start_method=self._start_method,
+                    )
+        else:
+            process_idx = int(os.environ.get("LOCAL_RANK"))
+            self._strategy._local_rank = process_idx
+            results = function(*args, **kwargs)
+            _rank_teardown(process_idx)
+
         return None
     
     def _wrapping_function(
@@ -179,7 +186,6 @@
             and self.trainer.fit_loop._should_accumulate()
         ):
             # For gradient accumulation
-    
             # -------------------
             # calculate loss (train step + train step end)
             # -------------------
@@ -330,7 +336,9 @@
         """Lazily set missing attributes on the previously instantiated strategy."""
         self.strategy.accelerator = self.accelerator
         if self.precision_plugin:
-            self.strategy.precision_plugin = self.precision_plugin
+            #self.strategy.precision_plugin = self.precision_plugin
+            self.strategy.precision_plugin = PrecisionPlugin()
+
         if self.checkpoint_io:
             self.strategy.checkpoint_io = self.checkpoint_io
         if hasattr(self.strategy, "cluster_environment"):
@@ -565,27 +573,49 @@
 
     def __init__(
         self,
+        accelerator: Optional["pl.accelerators.Accelerator"] = None,
         parallel_devices: Optional[List[torch.device]] = None,
-        cluster_environment: ClusterEnvironment = None,
+        cluster_environment: Optional[ClusterEnvironment] = None,
         checkpoint_io: Optional[CheckpointIO] = None,
+        precision_plugin: Optional[PrecisionPlugin] = None,
+        debug: bool = False,
         no_ddp_communication_hook: bool = False,
-        **kwargs: Union[Any, Dict[str, Any]],
+        megatron_amp_o2: bool=False,
+        **_: Any,
     ) -> None:
+        if not _XLA_AVAILABLE:
+            raise ModuleNotFoundError(str(_XLA_AVAILABLE))
         if not HAVE_APEX:
             raise ImportError(
                 "Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/NeMo#megatron-gpt."
             )
-        super().__init__(parallel_devices, cluster_environment, checkpoint_io, **kwargs)
-
+        if cluster_environment is None:
+            cluster_environment=XLAEnvironment()
+        super(TPUSpawnStrategy, self).__init__(
+            accelerator=accelerator,
+            parallel_devices=parallel_devices,
+            cluster_environment=cluster_environment,
+            checkpoint_io=checkpoint_io,
+            precision_plugin=precision_plugin,
+            start_method="fork",
+        )
+        self._checkpoint_io: Optional[CheckpointIO]
+        self.debug = debug
+        self._launched = False
         self.no_ddp_communication_hook = no_ddp_communication_hook
+        self.megatron_amp_o2 = megatron_amp_o2
 
     def _configure_launcher(self) -> None:
         self._launcher = _NLPXLALauncher(self)
 
     def setup_distributed(self, global_rank: int = None, world_size: int = None) -> None:
         import torch.distributed as dist
-        import torch_xla.core.xla_model as xm
-        dist.init_process_group('xla', rank=xm.get_ordinal())
+        if self.cluster_environment.creates_processes_externally:
+            global_rank = int(os.environ.get("RANK"))
+        else:
+            import torch_xla.core.xla_model as xm
+            global_rank = xm.get_ordinal()
+        dist.init_process_group('xla', rank=global_rank)
         # call PTL init ddp
         super().setup_distributed()
 
@@ -716,11 +746,17 @@
         master_only = app_state.data_parallel_rank == 0
         if master_only:
             ensure_directory_exists(filepath)
+        try:
+            save_bf16 = self.lightning_module.cfg.save_bf16
+        except:
+            save_bf16 = False
         if self.is_save_type_xser():
             xser.save({k: v for k, v in checkpoint.items() if k != "callbacks"}, filepath, (not master_only), global_master=True)
         else:
             for tp_rank in range(0, parallel_state.get_tensor_model_parallel_world_size()):
                 my_tp_rank = parallel_state.get_tensor_model_parallel_rank()
+                if save_bf16:
+                    checkpoint = cast_all(checkpoint, from_dtype=torch.float32, to_dtype=torch.bfloat16)
                 should_write_data = True if parallel_state.get_data_parallel_rank() == 0 and my_tp_rank == tp_rank else False
 
                 #Staggering save checkpoints
@@ -761,7 +797,7 @@
         checkpoint_path = inject_model_parallel_rank(checkpoint_path)
         import torch_xla.utils.serialization as xser
         
-        if load_xser_mode:
+        if self.is_load_type_xser():
             return xser.load(checkpoint_path)
         return self.checkpoint_io.load_checkpoint(checkpoint_path)
 
diff -urN neuronx-nemo-megatron/nemo/nemo/core/classes/modelPT.py nemo-updates/nemo/nemo/core/classes/modelPT.py
--- neuronx-nemo-megatron/nemo/nemo/core/classes/modelPT.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/core/classes/modelPT.py	2023-08-16 13:00:48
@@ -20,13 +20,13 @@
 from os import path
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Optional, Union
-
 import hydra
 import torch
 from omegaconf import DictConfig, OmegaConf, open_dict
 from pytorch_lightning import LightningModule, Trainer
 from pytorch_lightning.utilities import model_summary, rank_zero_only
-
+import torch_xla.core.xla_model as xm
+from torch_xla.distributed.zero_redundancy_optimizer import ZeroRedundancyOptimizer
 from nemo import package_info
 from nemo.core import optim
 from nemo.core.classes.common import Model
@@ -36,7 +36,6 @@
 from nemo.utils.app_state import AppState
 from nemo.utils.debug_hook import register_debug_hooks
 from nemo.utils.get_rank import get_rank, is_global_rank_zero
-
 __all__ = ['ModelPT']
 
 
@@ -438,6 +437,7 @@
 
     def setup_optimization(
         self, optim_config: Optional[Union[DictConfig, Dict]] = None, optim_kwargs: Optional[Dict[str, Any]] = None,
+            wrap_with_zero=False
     ):
         """Prepares an optimizer from a string name and its optional config parameters.
 
@@ -454,6 +454,7 @@
             optim_kwargs: A dictionary with additional kwargs for the
                 optimizer. Used for non-primitive types that are not
                 compatible with OmegaConf.
+                :param wrap_with_zero: Parameter to wrap optimizer with ZeroRedundancyOptimizer for Zero-1
 
         """
         # Setup the optimizer parameter groups (by default use all parameters that are trainable)
@@ -596,8 +597,19 @@
                     raise e
 
         else:
-            optimizer = optim.get_optimizer(optimizer_name)
-            optimizer = optimizer(self._optimizer_param_groups, **optimizer_args)
+            if wrap_with_zero:
+                optimizer = ZeroRedundancyOptimizer(self._optimizer_param_groups,
+                                                    optim.AVAILABLE_OPTIMIZERS[optimizer_name],
+                                                    pin_layout=False,
+                                                    grad_clipping=self.trainer.gradient_clip_val is not None and self.trainer.gradient_clip_val != 0,
+                                                    max_norm=self.trainer.gradient_clip_val,
+                                                    sharding_groups=self.calculate_data_parallel_groups(),
+                                                    grad_norm_groups=self.calculate_model_parallel_groups(),
+                                                    **optimizer_args,
+                                                    )
+            else:
+                optimizer = optim.get_optimizer(optimizer_name)
+                optimizer = optimizer(self._optimizer_param_groups, **optimizer_args)
 
             logging.info("Optimizer config = %s", str(optimizer))
 
@@ -611,6 +623,62 @@
         # Return the optimizer with/without scheduler
         # This return allows multiple optimizers or schedulers to be created
         return self._optimizer, self._scheduler
+
+    def calculate_data_parallel_groups(self):
+        """
+        Helper method for calculating data parallel groups for Zero-1 Optimizer Sharding
+        Example: World Size 32 with TP Degree 8 and PP Degree 1 returns [[0, 8, 16, 24], [1, 9, 17, 25],
+                                                                      [2, 10, 18, 26], [3, 11, 19, 27],
+                                                                      [4, 12, 20, 28], [5, 13, 21, 29],
+                                                                      [6, 14, 22, 30], [7, 15, 23, 31]]
+        :return: List of lists of data parallel groups
+        """
+        world_size = xm.xrt_world_size()
+        tensor_model_parallel_size = self.cfg.get('tensor_model_parallel_size', 1)
+        pipeline_model_parallel_size = self.cfg.get('pipeline_model_parallel_size', 1)
+        num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size
+
+        # Build the data-parallel groups.
+        all_data_parallel_group_ranks = []
+        for i in range(pipeline_model_parallel_size):
+            start_rank = i * num_pipeline_model_parallel_groups
+            end_rank = (i + 1) * num_pipeline_model_parallel_groups
+            for j in range(tensor_model_parallel_size):
+                ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)
+                all_data_parallel_group_ranks.append(list(ranks))
+        return all_data_parallel_group_ranks
+
+    def calculate_model_parallel_groups(self):
+        """
+        Helper method for calculating model parallel groups for Zero-1 Optimizer Sharding
+        Example: World Size 32 with TP Degree 8 and PP Degree 4 returns dp groups [[0], [1], [2], [3], [4], [5], [6], [7], [8],
+                                                                        [9], [10], [11], [12], [13], [14], [15], [16],
+                                                                        [17], [18], [19], [20], [21], [22], [23], [24],
+                                                                        [25], [26], [27], [28], [29], [30], [31]]
+        with model parallel groups returned as  [[0, 1, 2, 3, 4, 5, 6, 7, 8,
+                            9, 10, 11, 12, 13, 14, 15, 16,
+                            17, 18, 19, 20, 21, 22, 23, 24,
+                             25, 26, 27, 28, 29, 30, 31]]
+        :return: List of lists of model parallel groups
+        """
+        world_size = xm.xrt_world_size()
+        tensor_model_parallel_size = self.cfg.get('tensor_model_parallel_size', 1)
+        pipeline_model_parallel_size = self.cfg.get('pipeline_model_parallel_size', 1)
+        data_parallel_size = world_size // (
+                tensor_model_parallel_size * pipeline_model_parallel_size
+            )
+        all_data_parallel_group_ranks = self.calculate_data_parallel_groups()
+
+        # Build the data-parallel groups.
+        all_model_parallel_group_ranks = []
+        for i in range(data_parallel_size):
+            ranks = [
+                data_parallel_group_ranks[i]
+                for data_parallel_group_ranks in all_data_parallel_group_ranks
+            ]
+            all_model_parallel_group_ranks.append(list(ranks))
+        return all_model_parallel_group_ranks
+
 
     def setup_optimizer_param_groups(self):
         """
diff -urN neuronx-nemo-megatron/nemo/nemo/core/optim/__init__.py nemo-updates/nemo/nemo/core/optim/__init__.py
--- neuronx-nemo-megatron/nemo/nemo/core/optim/__init__.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/core/optim/__init__.py	2023-08-16 13:00:48
@@ -28,5 +28,5 @@
     prepare_lr_scheduler,
 )
 from nemo.core.optim.novograd import Novograd
-from nemo.core.optim.optimizer_with_main_params import MainParamsOptimizerWrapper
-from nemo.core.optim.optimizers import get_optimizer, parse_optimizer_args, register_optimizer
+from nemo.core.optim.optimizer_with_main_params import MainParamsOptimizerWrapper, MainParamsOptimizerWrapperXLA
+from nemo.core.optim.optimizers import get_optimizer, parse_optimizer_args, register_optimizer, AVAILABLE_OPTIMIZERS
diff -urN neuronx-nemo-megatron/nemo/nemo/core/optim/optimizer_with_main_params.py nemo-updates/nemo/nemo/core/optim/optimizer_with_main_params.py
--- neuronx-nemo-megatron/nemo/nemo/core/optim/optimizer_with_main_params.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/nemo/core/optim/optimizer_with_main_params.py	2023-08-16 13:00:48
@@ -515,3 +515,184 @@
         self.optimizer.defaults = value
 
     defaults = property(_get_defaults, _set_defaults)
+
+
+class MainParamsOptimizerWrapperXLA(torch.optim.Optimizer):
+    """
+    Float16 optimizer wrapper for half precision (fp16 and bf16) data types.
+    This optimizer wrapper holds main parameters and gradients in fp32 to support
+    stable convergence.
+
+    Arguments:
+        optimizer: base optimizer such as Adam or SGD.
+        fp32_grad_accum: to enable the use of fp32 in gradient accumulation and allreduce.
+        contiguous_grad_bucket: to enable allocating the master gradients in the 
+            contiguous memory space to reduce memory fragmentation.
+        async_grad_allreduce: enable asynchronous gradient allreduce that is executed
+            along with the training step backprop.
+    """
+
+    def __init__(
+        self,
+        optimizer,
+    ):
+        self.optimizer = optimizer
+        assert self.optimizer, 'no optimizer is provided.'
+
+        self.float32_groups = []  # original float32 parameters
+        self.fp64_from_float32_groups = []  # fp64 copy of float32 parameters
+
+        # For all the groups in the original optimizer:
+        for i, param_group in enumerate(self.optimizer.param_groups):
+            float32_params_this_group = []
+            fp64_from_float32_params_this_group = []
+            # For all the parameters in this group:
+            for j, param in enumerate(param_group['params']):
+                if param.requires_grad:
+                    float32_params_this_group.append(param)
+
+                    # Allocate the main parameter
+                    main_param = param.detach().clone().double()
+
+                    # Copy tensor model parallel attributes.
+                    copy_tensor_model_parallel_attributes(main_param, param)
+                    if hasattr(param, 'shared'):
+                        main_param.shared = param.shared
+
+                    # Replace the optimizer params with the new fp64 copy.
+                    param_group['params'][j] = main_param
+                    fp64_from_float32_params_this_group.append(main_param)
+                    # Reset existing state dict key to the new main param.
+                    if param in self.optimizer.state:
+                        self.optimizer.state[main_param] = self.optimizer.state.pop(param)
+
+            self.float32_groups.append(float32_params_this_group)
+            self.fp64_from_float32_groups.append(fp64_from_float32_params_this_group)
+
+        # Leverage state_dict() and load_state_dict() to
+        # recast preexisting per-param state tensors
+        self.optimizer.load_state_dict(self.optimizer.state_dict())
+
+
+    def zero_grad(self, set_to_none=True):
+        """We only need to zero the model related parameters, i.e.,
+        float32_groups."""
+        for group in self.float32_groups:
+            _zero_grad_group_helper(group, set_to_none)
+
+    def _copy_model_grads_to_main_grads(self):
+        # This only needs to be done for the float16 group.
+        for model_group, main_group in zip(self.float32_groups, self.fp64_from_float32_groups):
+            for model_param, main_param in zip(model_group, main_group):
+                if model_param.grad is not None:
+                    main_param.grad = model_param.grad.double()
+
+    def _get_model_and_main_params_data_float64(self):
+        model_data = []
+        main_data = []
+        for model_group, main_group in zip(self.float32_groups, self.fp64_from_float32_groups):
+            for model_param, main_param in zip(model_group, main_group):
+                model_data.append(model_param.data)
+                main_data.append(main_param.data)
+        return model_data, main_data
+
+    def _copy_main_params_to_model_params(self):
+        model_data, main_data = self._get_model_and_main_params_data_float64()
+        for this_, that_ in zip(main_data, model_data):
+            that_.copy_(this_.to(float))
+
+    def _copy_model_params_to_main_params(self):
+        model_data, main_data = self._get_model_and_main_params_data_float64()
+        for this_, that_ in zip(model_data, main_data):
+            that_.copy_(this_.double())
+
+    def reload_model_params(self):
+        self._copy_model_params_to_main_params()
+
+    def step(self, closure):
+        closure_result = closure()
+ 
+        # Copy gradients from model params to main params.
+        with torch.no_grad():
+            self._copy_model_grads_to_main_grads()
+        # Step the optimizer.
+        self.optimizer.step()
+ 
+        # Update params from main params.
+        with torch.no_grad():
+            self._copy_main_params_to_model_params()
+ 
+        # Successful update.
+        return closure_result
+
+  
+
+    def state_dict(self):
+        state_dict = {}
+        state_dict['optimizer'] = self.optimizer.state_dict()
+        state_dict['fp64_from_fp32_params'] = self.fp64_from_float32_groups
+        return state_dict
+
+    def load_state_dict(self, state_dict):
+        # Optimizer.
+        optimizer_key = 'optimizer'
+        if optimizer_key not in state_dict:
+            optimizer_key = 'optimizer_state_dict'
+            logging.info('***WARNING*** loading optimizer from ' 'an old checkpoint ...')
+        self.optimizer.load_state_dict(state_dict[optimizer_key])
+
+        # Copy data for the main params.
+        fp64_from_float32_params_key = 'fp64_from_fp32_params'
+        if fp64_from_float32_params_key not in state_dict:
+            fp64_from_float32_params_key = 'fp64_from_fp32'
+        for current_group, saved_group in zip(self.fp64_from_float32_groups, state_dict[fp64_from_float32_params_key]):
+            for current_param, saved_param in zip(current_group, saved_group):
+                current_param.data.copy_(saved_param.data)
+
+    def get_parameters(self):
+        params = []
+        #for param_group in self.optimizer.param_groups:
+        for param_group in self.float32_groups:
+            for param in param_group:
+                params.append(param)
+        return params
+
+    # Promote state so it can be retrieved or set via
+    # "optimizer_instance.state"
+    def _get_state(self):
+        if hasattr(self, 'optimizer'):
+            return self.optimizer.state
+        else:
+            return []
+
+    def _set_state(self, value):
+        self.optimizer.state = value
+
+    state = property(_get_state, _set_state)
+
+    # Promote param_groups so it can be retrieved or set via
+    # "optimizer_instance.param_groups"
+    # (for example, to adjust the learning rate)
+    def _get_param_groups(self):
+        if hasattr(self, 'optimizer'):
+            return self.optimizer.param_groups
+        else:
+            return []
+
+    def _set_param_groups(self, value):
+        self.optimizer.param_groups = value
+
+    param_groups = property(_get_param_groups, _set_param_groups)
+
+    # Promote defaults so it can be retrieved or set via
+    # "optimizer_instance.defaults
+    def _get_defaults(self):
+        if hasattr(self, 'optimizer'):
+            return self.optimizer.defaults
+        else:
+            return []
+
+    def _set_defaults(self, value):
+        self.optimizer.defaults = value
+
+    defaults = property(_get_defaults, _set_defaults)
diff -urN neuronx-nemo-megatron/nemo/requirements/requirements.txt nemo-updates/nemo/requirements/requirements.txt
--- neuronx-nemo-megatron/nemo/requirements/requirements.txt	2023-08-24 16:55:40
+++ nemo-updates/nemo/requirements/requirements.txt	2023-08-16 13:00:48
@@ -5,7 +5,7 @@
 python-dateutil
 ruamel.yaml
 scikit-learn
-setuptools==65.5.1
+setuptools==59.5.0
 tensorboard
 text-unidecode
 torch
Binary files neuronx-nemo-megatron/nemo/scripts/.DS_Store and nemo-updates/nemo/scripts/.DS_Store differ
diff -urN neuronx-nemo-megatron/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py nemo-updates/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py
--- neuronx-nemo-megatron/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py	2023-08-24 16:55:40
+++ nemo-updates/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py	2023-08-16 13:00:48
@@ -80,6 +80,21 @@
     --chunk_size=64 \
     --workers=64 
 ```
+
+
+```python
+python nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \
+    --input=/root/scripts/data/llamav2/arxiv/arxiv_1caed86f-5625-4941-bdc1-cc57e4fec1cd.jsonl \
+    --json-keys=text \
+    --tokenizer-library=huggingface \
+    --tokenizer-type=/root/scripts/data/llama7b-hf \
+    --dataset-impl=mmap \
+    --output-prefix=/root/scripts/data/llamav2/arxiv_hf/arxiv \
+    --append-eod \
+    --need-pad-id \
+    --workers=128
+```
+
 """
 
 import argparse
diff -urN neuronx-nemo-megatron/requirements.txt nemo-updates/requirements.txt
--- neuronx-nemo-megatron/requirements.txt	2023-08-24 16:55:41
+++ nemo-updates/requirements.txt	2023-08-16 13:00:48
@@ -2,7 +2,7 @@
 omegaconf>=2.2,<2.3
 pyyaml<6  # Pinned until omegaconf works with pyyaml>=6
 torchmetrics>=0.4.1rc0,<=0.10.3
-transformers<=4.21.2
+transformers==4.31.0
 wandb
 webdataset>=0.1.48,<=0.1.62
 pandas
@@ -31,3 +31,4 @@
 pytorch-lightning==1.8.6
 ipadic
 mecab-python3
+protobuf~=3.20
