apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: mathstral-7b-fsdp
spec:
  elasticPolicy:
    rdzvBackend: c10d
    minReplicas: 1
    maxReplicas: 64
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 90
  pytorchReplicaSpecs:
    Worker:
      replicas: $NUM_NODES
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: mathstral-7b-fsdp
        spec:
          volumes:
            - name: shmem
              hostPath: 
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
          #nodeSelector:
          #  node.kubernetes.io/instance-type: "${INSTANCE_TYPE}"
          containers:
            - name: pytorch
              image: ${IMAGE_URI}
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: $GPU_PER_NODE
                  vpc.amazonaws.com/efa: $EFA_PER_NODE
                limits:
                  nvidia.com/gpu: $GPU_PER_NODE
                  vpc.amazonaws.com/efa: $EFA_PER_NODE
              env:
              # for P5 FI_* should be commented out
              - name: LOGLEVEL
                value: "DEBUG"
              #- name: FI_PROVIDER
              #  value: $FI_PROVIDER
              #- name: FI_EFA_USE_DEVICE_RDMA
              #  value: "1"
              #- name: FI_EFA_FORK_SAFE
              #  value: "1"
              #- name: FI_LOG_LEVEL
              #  value: "1"
              #- name: FI_EFA_ENABLE_SHM_TRANSFER
              #  value: "1"
              - name: TORCH_DISTRIBUTED_DEBUG
                value: "DETAIL"
              - name: TORCH_NCCL_ENABLE_MONITORING
                value: "1"
              - name: TORCH_NCCL_TRACE_BUFFER_SIZE
                value: "20000"
              - name: TORCH_NCCL_DUMP_ON_TIMEOUT
                value: "1"
              - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
                value: "/local/nccl_trace_rank_"
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: "expandable_segments:True"
              - name: NCCL_DEBUG
                value: "INFO"
              - name: NCCL_SOCKET_IFNAME
                value: "^lo"
              - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
                value: "1"
              - name: HF_TOKEN
                value: "${HF_TOKEN}"
              #- name: TORCH_DIST_INIT_BARRIER
              #  value: "1"
              #- name: NCCL_IGNORE_DISABLED_P2P
              #  value: "1"
              #- name: NCCL_NVLS_ENABLE
              #  value: "0"
              command: 
                - /usr/local/bin/torchrun
                - --nproc_per_node=$GPU_PER_NODE
                - --nnodes=$NUM_NODES
                - /fsdp/train.py
                - --train_batch_size=1
                - --val_batch_size=1
                - --seed=42
                - --grad_clip=1.0
                - --weight_decay=0.2
                - --beta1=0.9
                - --beta2=0.95
                - --activation_checkpointing=1
                - --intermediate_size=14336
                - --num_key_value_heads=8
                - --logging_freq=1
                - --max_context_width=32768
                - --vocab_size=32768
                - --hidden_width=4096
                - --num_layers=32
                - --num_heads=32
                - --resid_pdrop=0.1
                - --embd_pdrop=0.1
                - --attn_pdrop=0.1
                - --summary_first_pdrop=0.1
                - --initializer_range=0.02
                - --model_type=mistral
                - --rotary_pct=0.25
                - --rotary_emb_base=10000
                - --lr=0.0001
                - --lr_decay_style=cosine
                - --min_lr=1e-5
                - --warmup=0.0032
                - --plateau=0.0
                - --dataset=allenai/c4
                - --tokenizer=mistralai/mathstral-7B-v0.1
                - --epochs=3
                - --checkpoint_dir=./checkpoints/mathstral-7B
                - --resume_from_checkpoint=./checkpoints/mathstral-7B
                - --max_steps=200
                - --checkpoint_freq=50
                - --validation_freq=100
                - --dataset_config_name=en
                - --limit_all_gathers=1
                - --sharding_strategy=full  # https://pytorch.org/docs/stable/fsdp.html
                - --offload_activations=1
              volumeMounts:
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local