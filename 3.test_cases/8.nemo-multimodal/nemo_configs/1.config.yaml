defaults:
  - _self_
  - cluster: bcm  # Leave it as bcm even if using bcp. It will be ignored for bcp.
  - data_preparation: multimodal/download_multimodal
  - training: stable_diffusion/860m_res_256_pretrain
  - conversion: null
  - fine_tuning: null
  - evaluation: null
  - fw_inference: null
  - export: null
  - external_conversion: null
  - override hydra/job_logging: stdout

stages:
  - data_preparation

cluster_type: bcm  # bcm or bcp. If bcm, it must match - cluster above.
launcher_scripts_path: ${TARGET_PATH}/launcher_scripts  # Path to NeMo Megatron Launch scripts, should ends with /launcher_scripts
data_dir: ${launcher_scripts_path}/data  # Location to store and read the data.
base_results_dir: ${launcher_scripts_path}/results  # Location to store the results, checkpoints and logs.
container_mounts: # List of additional paths to mount to container. They will be mounted to same path.
  - /fsx:/fsx
container: ${ENROOT_IMAGE}.sqsh

wandb_api_key_file: null  # File where the w&B api key is stored. Key must be on the first line.

env_vars:
  NCCL_DEBUG: INFO # Logging level for NCCL. Set to "INFO" for debug information
  TRANSFORMER_OFFLINE: 1
  FI_EFA_USE_DEVICE_RDMA: 1
  FI_PROVIDER: efa
  NCCL_LAUNCH_MODE: parallel
  FI_EFA_FORK_SAFE: 1
  FI_EFA_ENABLE_SHM_TRANSFER: 1
  FI_EFA_USE_HUGE_PAGE: 0

# GPU Mapping
numa_mapping:
  enable: True  # Set to False to disable all mapping (performance will suffer).
  mode: unique_contiguous  # One of: all, single, single_unique, unique_interleaved or unique_contiguous.
  scope: node  # Either node or socket.
  cores: all_logical  # Either all_logical or single_logical.
  balanced: True  # Whether to assing an equal number of physical cores to each process.
  min_cores: 1  # Minimum number of physical cores per process.
  max_cores: 8  # Maximum number of physical cores per process. Can be null to use all available cores.

# hydra settings
hydra:
  run:
    dir: .
  output_subdir: null

# Do not modify below, use the values above instead.
data_preparation_config: ${hydra:runtime.choices.data_preparation}
training_config: ${hydra:runtime.choices.training}
fine_tuning_config: ${hydra:runtime.choices.fine_tuning}
prompt_learning_config: ${hydra:runtime.choices.prompt_learning}
adapter_learning_config: ${hydra:runtime.choices.adapter_learning}
ia3_learning_config: ${hydra:runtime.choices.ia3_learning}
evaluation_config: ${hydra:runtime.choices.evaluation}
conversion_config: ${hydra:runtime.choices.conversion}
export_config: ${hydra:runtime.choices.export}
fw_inference_config: ${hydra:runtime.choices.fw_inference}
external_conversion_config: ${hydra:runtime.choices.external_conversion}
